## htxne
#
`[//]Functional Type Theory in Cognitive and Dependency GrammarsMy discussion so far has focused on 
characterizing sentences' holistic meaning.  On the face of 
it, such holistic analysis is more semantic than syntactic.  
However, syntactic paradigms can be grounded in theories 
of how language elements aggregatetowardholistic meaning.Here I propose a notion ofcognitive transforms`{-} that holistic meanings emerge from a series of 
interpretive and situational modeling modifications 
which progressively refine our understanding of a 
speaker's construal of our environing context and her 
propositional attitudes.  While elucidation of these 
transforms as cognitive phenomena may belong to semantics, 
syntactic structure dictates thesequenceof 
transforms.  Many transforms are expressed by 
individual word-pairs.  Taking the temporal or logical 
order of transforms into consideration, we can derive a 
syntactic model of sentences by introducing an order among 
word-pairs `{-} a methodology akin to using Dependency Grammar 
parse-graphs as an intermediate stage, then ordering the 
graph-edges around an estimation of cognitive aggregation.  
One transform is a successor to a predecessor if the 
modifications induced by the predecessor are 
consequential for the cognitive reorientation pertinent to 
the successor, and/or to the morphosyntactic features which 
trigger it.In this spirit I talk of Cognitive TransformGrammar, because 
while in the general case transforms are semantic and interpretive 
`{-} not the purview of grammar per se `{-} we can theorize 
grammar as governing theorder of precedenceamong transforms.  
More precisely, there is a particular order of precedence germane 
to sentence meaning; sentences have their precise syntax 
in order to compel recipients' reception of the linguistic 
performance according to that same ordering.From this perspective, an essential aspect of grammar theory is that 
whatever units are understood as syntactic constituents `{-} like 
phrase structure or word-pairs `{-} an order of precedence shouldfall outof grammatic reconstructions.  We should be able 
to supplement parse-representations with a listing of salient 
syntactic features in order, retracing thecognitivesteps 
by which localized sense-alterations synthesize into holistic 
meaning.  The details of this precedence-establishment 
will vary across grammatic paradigms, so one way to assess 
grammar theories is to consider how the engender corresponding 
cognitive-transform models.Type theory can be adopted in this context because 
most versions of type theory include a notion offunction-liketypes: types whose instances modify 
instances of some other type (or types).  This 
establishes an order of precedence: anything modified 
by a function is in some sense logically prior to 
that function.  In formal (e.g., programming) languages, 
the procedure whose output becomes input to a different 
procedure must be evaluated before the latter procedure 
begins (or at least before the output-to-input value is 
used by that latter procedure).A common paradigm is to consider natural-language types as generated 
by just two bases `{-} a noun typeand a proposition type, the type of sentences and of sentence-parts which are 
complete ideas `{-} having in themselves a propositional content 
(see e.g.or).    
Different models derive new types on this basis in different ways.
One approach, inspired by mathematicalpregroups, establishes 
derivative types in terms of word pairs `{-} an adjective followed 
by a noun yields another noun (a noun-phrase, butis the phrase'stype) `{-} e.g.,rescued dogs, likedogs, is conceptually 
a noun.  Adjectives themselves then have the type of words which form 
nouns when paired with a following noun, often written as.  Pregroup grammars distinguish left-hand and right-hand 
adjacency `{-}bark loudly, say, demonstrates an adverbaftera
verb, yielding a verb phrase: soloudlyhere has the type of a 
word producing a verb in combination with a verb to itsleft(sometimes written); by contrast adjectives combine with 
nouns to theirright.A related formalization, whose formal 
analogs lie in Typed Lambda Calculus, abstracts from left-or-right 
word order to models derived types as equivalent (at least 
for purposes of type attribution) tofunctions.  
This engenders a fundamental notion of functionalapplicationand operator/operand distinctions:Categorial Grammarsmake the connection between the 
first and the second level of the ACG.  These models are 
typed applicative systems that verify if a linguistic 
expression is syntactically well-formed, and then construct 
its functional semantic interpretation.  They use thefundamental operation of applicationof an operator to an 
operand.  The result is a new operand or a new operator. 
This operation can be repeated as many times as necessary 
to construct the linguistic expressions' functional semantic 
representation.So, e.g., an adverb becomes a function which takes a verb and produces another verb; 
an adjective takes a noun and produces another noun; 
and a verb takes a noun and produces a proposition.  Byfunctionwe can consider 
some kind of conceptual transformation:loudlytransforms the
conceptbarkinto the conceptloud bark.  
Assuming all lexemes are assigned a Part of 
Speech drawn from such a type system, the definition of 
functional types directly yields a precedence order: 
instances of functional types are functionally dependent 
on their inputs, which are therefore precedent to them.  
On this basis, any well-typed functional expression has a 
unique precedence ordering on its terminal elements 
(i.e., itsleaveswhen the expression is viewed as a 
tree, or its nodes when viewed as a graph), which 
can be uncovered via a straightforward algorithm 
(one implementation is part of this paper's data set; 
see theparse\_sxpmethod in file ntxh-udp-sentence.cpp).Functional parts of speech 
that can be formally modeled with oneargument(the most common case),  
having a single input and output type, 
conveniently lend themselves 
to cognitive transforms defined through word 
pairs `{-} an adjective modifies a noun to another 
noun, an adverb maps a verb to a verb, an auxiliary 
likethatorhavingcan map verbs or propositions 
to nouns, and so forth.  The only main complication 
to this picture is that verbs, which typically have subjects 
as well as objects, can take two or threeinputsinstead of just one.  Instead of a transformpairwe 
can then consider a three- or four-part transform structure 
(verb, subject, direct object, indirect object).  
We can still assign a precedence ordering to verb-headed phrases, 
however, perhaps by stipulating that the subject takes 
precedence before the direct object, and the direct object
before the indirect.  This ordering seems cognitively 
motivated: our construal of the significance of a 
direct object appears to intellectually depend on the verb's 
subject; likewise the indirect object depends on the direct 
object to the degree that it is rationally consequential.A secondary complication involves copulae likeand, which 
can connect more than two words or clauses.  Here, though, a 
natural ordering seems to derive from linear position in the 
sentence: givenx, y, and zwe can treatxas 
precedent toy, andytoz, respectively.In total, sentences as a whole can thus be seen as structurally 
akin to nested expressions in lambda calculi
(and notated viaS-Expressions, like code in the 
Lisp programming language).  S-Expressions are occasionally 
recommended as representations for some level of 
linguistic analysis (cf.,,), and if this form by itself may add little 
extra data, it does offer a succinct way to capture the 
functional sequencing attributed to a sentence during analysis.  
Given, say,The city's ambience is colonial and the climate is tropical.the gloss(and (is ((The ('s city)) ambience) colonial) (is (the climate) tropical))summarizes analytic commitments with regard to the root structure of the sentence 
(in my treatment the copula is the overall root word) and to precedence between words 
(which words are seen as modifiers and which are their ground, for instance).  
So even without extra annotations (without, say, the kind of tagging data included 
by treebanks using S-Expression serializations), rewriting sentences as 
nested expressions captures primitive but significant syntactic details.Nested-expression models also give rise directly to two other representations: 
a precedence ordering among lexemes automatically follows by taking 
function inputs as precedent to function (words) themselvesBut 
note that usingfunction-wordsas terminology here generalizes 
this term beyond its conventional meaning in grammar.; 
moreover, S-Expression formats can be rewritten as sets of 
word-pairs, borrowing the representational paradigms (if not 
identical structures) of Dependency Graphs.  This allows Dependency Graphs 
and S-Expressions to be juxtaposed, which I will discuss in the remainder of this section.Double de Bruijn IndicesAssume then that all non-trivial sentences are nested expressions, and that 
all lexemes other than nouns are notionallyfunctions, which take typedinputsand produce typedoutcomes.  Expressionnestingmeans 
that function inputs are often outcomes from other functions (which establishes a 
precedence order among functions).  Since there is an obvious notion ofparent`{-} instances of functional types are parents of the words or 
phrase-heads which are their inputs `{-} nestable expressions are formally 
trees.  Via tree-to-graph embedding, they can also be treated as graphs, 
with edges linking parents to children; since parse-graphs are canonical 
in some grammar theories (like Link and Dependency grammar), it is 
useful to consider the graph-style representation as 
the intrinsic structure of linguistic glosses based on S-Expressions.  
That is, we want to define a Category of labeled graphs 
each of whose objects is isomorphic to an S-Expression (using this 
terminology in the sense of mathematical Category Theory); equivalently, a 
bijective encoding of S-Expressions within labeled graphs given 
a suitable class of edge-labels.Indeed, labels comprised of 
two numbers suffice, generalizing the lambda-calculusde Bruijn Indices.  The de Bruijn notation is an alternative 
presentation of lambda terms using numeric indices in lieu of 
lambda-abstracted symbol (cf., for a linguistic
or discourse-representation context,and).  Thedoubleindices
accommodate the fact that, in the general case, the 
functional component of an expression may be itself a nested 
expression, meaning thatevaluationhas to proceed in several 
stages: a function (potentially with one or more inputs) is 
evaluated, yielding another function, which is then applied 
to inputs, perhaps again yielding a function applied to still 
more inputs, and so forth.  I use the termevaluatewhich 
is proper to the computer-science context, but in linguistics 
we can take this as a suggestive metaphor.  More correctly, 
we can say that a function/input structure represents a
cognitive transform which produces a new function 
(i.e., a phrase with a function-like part of speech), that is
then the modifier to a new transform, and so forth.  
In general, the result of a transform can either be 
thegroundof a subsequent transform, which is akin to 
passing a function-result to another function; or 
it can be themodifierof a subsequent transform, which is akin to 
evaluating a nested expression to produce a new function, then 
applied to other inputs in turn.For a concrete example, considerThe most popular lodging is actually camping on the beaches.with gloss, as I take it,((actually is) ((The (most popular)) lodging) ((on (the beaches)) camping)).  
Here the adverbactuallyis taken as a modifier tois, so we imagine that 
interpreting the sentence involves first refiningisintois actually, 
yielding a new verb (orverb-idea) that then participates in a verb-subject-object 
pattern.  Hence, the parse opens with the evaluation(actually is)in the 
head-position of the sentence's top-level expression.  Similarly, I readon the beachesas 
functionally a kind of adverb, likeoutsideincamping outside.  
In the generic pattern, a verb can be paired with a designation of location 
to construct the idea of the verb happening at such location; the designation-of-location 
is then a modifier to the verb's ground.  When this designation is a locative 
construction, the whole expression becomes a modifier, while it also has its own 
internal structure.  Inon the beaches,onserves as a modifier which 
maps or reinterpretsthe beachesto a designation of place.So here is the 
unfolding of the phrase: inthe beachesthe determinant (the) is a modifier 
to the groundbeaches, signifying thatbeachesare to be circumscribed 
as an aggregate focus.  Thenonmodifies the outcome of that first transform, 
re-inscribing the focus as a place-designation.  Thenthattransform's output 
becomes a modifier forcamping, wherein the locative construction becomes 
a de-facto adverb, adding detail to the verbcamping(camping on the 
beach as a kind of camping, in effect).If it seems better to read camping as anoun`{-} the act or phenomenon 
of camping `{-} then we could treat the locative 
as anadjective, with the stipulation that the operation 
converting verbs to nouns (fromXto the phenomenon, act, or 
state ofX-ing) 
propagates to any modifiers on the verb: modifying constructions that 
refineXas verb are implicitly mapped to be adjectives likewise 
modifyingXin the nominal sense ofthe phenomenon ofX-ing.Notice in this review thattheas modifier inthe beachesyield 
a pair whose outcome is thegroundforon.  If we take the 
modifier as representative for a modifier-ground pair,theis themodifierin its own transform pair but then thegroundin the 
subsequent pair; the pattern is modifier-then-ground.  However,onis the modifiertheand thenalsomodifiercamping; the pattern is modifier-then-modifier.  
This latter case is the scenario where a lexeme will be a modifier 
on two or more differentlevels, giving rise to thedoublingof de Bruijn indices.  The first index, that is, represents 
theleveltieing a modifier to a ground, while the second 
index is thenormalnotation of lambda-position.  
Incamping on the beaches, the indices for the pairon/thewould be1,1(meaningtheis the first 
argument toonon the first transform level); the 
indices foroncampingwould be2,1(campingis the first argument toonon thesecondtransform level).By combining an index fortransform levels`{-} capturing cases 
where a modifier produces an outcome which is a modifier again, not a
ground `{-} with an index for lambda position (e.g. the direct object 
has index 2 relative to the verb, and the indirect object has 
index 3), we can transform any expression-tree into a labeled graph. 
Parse graphs can then be annotated with these double-indices 
via the same presentations employed for Link or Dependency Grammar 
labels.  Sentence () could be visualized as in 
Figure~, with the double-indices juxtaposed 
alongside conventional Dependency labels (the indices below the sentence, 
and relation labels above it); the upper parse is drawn from the 
Universal Dependency corpus (other annotated examples are included 
in this paper's downloadable data set).
%%As a natural corollary to this notation,  
parts of speech can havetype signaturesnotionally similar to the signatures of function types in programming languages: a verb
needing a direct object, for example,transformstwo nouns (Subject and Object)
to a proposition, which could be notated with something like.A note on notation: I adopt the Haskell convention (referring to the Haskell
programming language and other functional languages) of using arrows both between
parameters and before output notation, but for visual cue I add one dot above the
arrow in the former case, and two dots in the latter:.
I will useandfor the broadest designation of nouns and 
propositions/sentences (the broadest
noun type, respectively type of sentences, 
assuming we are using type-theoretic principles).  I will 
use some extra markings (in diagrams below) for more specific versions of 
nouns.The notation is consistent so long as each constituent of a verb 
phrase has a fixed index number.The subject at position one, for instance; 
direct object at position two; and indirect object at position three.Transforms (potentially with two or more arguments) then combine lexemes 
`{-} having the right signatures `{-} with one or more words or phrases. 
Type analysis recognizes criteria on these combinations, insofar 
as the phrases or lexemes at a given index have a type consistent 
with (potentially a subtype of) the corresponding position in the 
head-word's signature.  Ideally, this representation can 
explainungrammaticalconstructions viafailurefor 
these types to align properly.  If the combinationtype-checks, 
then we can assign the phrase as a whole the type indicated 
in the signature's output type `{-}in, say; 
in this case the signature points out howderives from the 
phrase with twos as its components (along with the verb), 
a derivation we could in turn notate like.   
A resulting phrase can then be included, like a nested expression, 
in other phrases; for instance ajoined tothatso as to create a noun-phrase (recall my 
analysis ofquestion whetherlast section); notationally.Numeric indices take the place of left and right adjacency
`{-}looking forwardor backward `{-} in Combinatory Categorial 
Grammar; the type-theoretic perspective abstracts from word order.  
The theory of result typesfalling outfrom a 
type-checked phrase structure, however, carries over to this 
more abstract analysis.Typesignatureslikemay 
seem little more than notational variants of conventional linguistic
wisdom, such as sentences' requiring a noun (-phrase) and a verb ().
Even at this level, however, type-theoretic intuitions
offer techniques for making sense of more complex, layered sentences,
where integrating Dependency Graphs and phrase structures can be complex.
One complication is 
the problem of applying Dependency Grammar where phrases do not seem
to have an obviouslymost significantword for linkage with other phrases.Often phrases are refinements of one 
single crucial component `{-} a phrase
likemany studentsbecomes in some sense collapsible to its semantic
core,students.  In real-world examples, however, lexemes tend 
to be neither wholly subsumed by their surrounding phrase nor wholly 
autonomous:Many students and their parents 
came to complain about the tuition hikes.Many students came by my office to complain about their grades.Student after student complained about the tuition hikes.Student after student came with their parents to complain 
about the tuition hikes.In () and (), we readMany studentsas topicalizing a multitude, 
but we recognize that each student has their own parents, grade, and we assume they came to 
the office at different times (rather than all at once).  Sostudentslinks 
conceptually with other sentence elements, in a way that pulls it partly outside theMany studentsphrase; the phrase itself is a space-builder which leaves open 
the possibility of multiple derived spaces.  This kind of space-building duality is 
reflected in how the singular/plural alternative is underdetermined in a multi-space 
context; consider Langacker's example:(repeating)Three times, students asked an interesting question.Three times, a student asked an interesting question.Meanwhile, in () and () the phraseStudent after studentinvokes a multiplicity akin toMany students, 
but the former phrase has distinct syntactic properties; in particular
we can replacetheir parents(which is ambiguous between a plural and 
a gender-neutral singular reading) with, say,his parents(at a 
boy's school), a valid substitution in ()-()
but not ()-().Cases likeStudent after student(or considertime after time,year after year, and so forth; this is a common idiomatic pattern 
in English) present a further difficulty for Dependency Grammar, 
since it is hard to identify which word of the three is the more 
significant, or thehead.  Arguably Constituency Grammar is 
more intuitive here because then phrases as a whole can get linked 
to other phrases, without needing to nominate one word to proxy 
the enclosing phrase.  As I feel thestudentsexamples illustrated, however, 
it is too simplistic to treat phrases as full-scale replacements for 
semantic units, as if any phrase is an ad-hoc single lexeme 
(of course some phrasesdoget entrenched as de facto lexemes, 
likeMember of Parliament, or my earlier examplesred cardandstolen base).  
In the general case though component words retain some syntactic and 
semantic autonomy (entrenchment diminishes but does not entirely 
eliminate such autonomy).  There is, then, a potential dilemma:
phrases link to other phrases (sometimes via subsumption and 
sometimes more indirectly, as in anaphora resolution), but phrases 
are not undifferentiated units; lexemes, which on this sort of 
analysisareunits, can be designated as proxies for their 
phrase; but then we can have controversy over which word in a 
phrase is the most useful stand-in for the whole 
(has an interesting review of a similar 
controversy in Dependency Grammar).Incorporating 
type theory, we can skirt these issues by modeling phrases through the perspective of
type signatures: given Part of Speech annotations for phrasal units and then for
some of their parts, the signatures of other parts, like verbs or adjectives
linked to nouns, or adverbs linked to verbs, tend to follow automatically.  
A successful analysis yields a formal tree, where if (in an act of semantic
abstraction) words are replaced by their types, theroottype is something likeand the rest of a tree is formally a reducible structure in
Typed Lambda Calculus:collapsesto,collapses
to, and so forth, with the treefolding inwardlike a
fan until only the root remains `{-} though a more subtle analysis would
replace the singletype with variants that recognize different
forms of speech acts, like questions and commands.
In Figure ~,
this can be seen via the type annotations: from right to leftyields theas second argument foris, which in turn yields athat is mapped
(bythat) to, finally becoming the second argument toknow.  This calculation
only considers the most coarse-grained classification (noun, verb, proposition) `{-} as I
have emphasized, a purely formal reduction can introduce finer-grained grammatical or
lexico-semantic classes (likeatneeding anargumentwhich is somehow an expression
of place `{-} or time, as inat noon).  Just as useful, however, may be analyses
which leave the formal type scaffolding at a very basic level and introduce
finer type or type-instance qualifications at a separate stage.In either case, Parts of Speech are modeled as (somehow analogous to) 
functions, but the important
analogy is that they havetype signatureswhich formally resemble functions'.
Words with function-like types proxy their corresponding phrase, not because 
they are necessarily more important or are Dependencyheads, but 
because they supply the pivot in the type resolutions which, 
collectively/sequentially, progress to a propositional culmination.  
This epistemological telos induces a sequencing on the 
type resolutions `{-} there is a fixed way that trees collapse `{-} 
which motivates the selection of function-words to proxy phrases; 
they are not semantically more consequential, necessarily, but 
are landmarks in a dynamic figured syntactically asfolding inwardand semantically as a progressive signifying refinement.   
Phrases are modeled via afunction-likeParts of Speech along with one or more
additional words whose own types match its signature; the type calculationscollapsingthese phrases can mimic semantic simplifications
likemany studentstostudents, but here the theory is explicit
that the simplification is grammatic and not semantic: the collapse
is acknowledged at the level oftypes, notmeanings.  In addition,
tree structures can be modeled purely in terms of inter-word relations 
`{-} as I have proposed here with double-indices `{-} 
so a type-summary of a sentence's phrase structure can be notated and
analyzed without leaving the Link or Dependency Grammar paradigm.In sum, then, function-like words can always be represented as 
theheadof corresponding phrases, but this implies 
neither greater semantic importance nor that phrases are 
conceptual units that fully subsume their parts.  Instead, 
ahead-functionnotation captures the idea that 
sentence-synthesis is bounded by considerations of 
conceptual integrity that we can model, to some coarse 
approximation, via type theory.  Designatingafteras 
theheadinstudent after studentmeans that we have a 
type machinery that models (coarsely but formally) how successive cognitive refinement 
converges onto something with the conceptual profile of a 
proposition, and that we can leverage this formality by 
identifying certain words as functional pivots in the 
synthesis-toward-proposition: these words are not necessarily 
pivotal in term of meaning, but they are the core skeleton of
the schematic type-based model which we attach to a 
sentence while modeling the constraints on its synthesizing 
process.  (There is a further dimension in thestudent after studentphrase `{-} the repetition ofstudent`{-} which I will discuss later.)Or at least, this is one area of analysis where type theory 
is relevant for linguistics.  I will argue that there are 
several different methodologies where linguists have 
turned to type theory, and moreover that they can 
be integrated into a unified picture organized around 
the granularity of types themselves, according to the 
relevant theories.Three tiers of linguistic type theoryWhen explaining grammaticality as type-checking `{-} the 
concordance between function-like signatures and 
word or phrasearguments`{-} types are 
essentially structural artifacts; their 
significance lies in the compositional patterns 
guiding phrases to merge into larger phrases in a 
well-ordered way `{-} specifically, that theoutermostexpression, canonically the whole sentence, is 
type-theoretically a proposition.  I proposed earlier that 
sentence-understanding be read as an accretion of detail 
culminating in a complete idea; type-checking then imposes 
regulatory guidelines on this accretion, with each 
constituent phrase being an intermediate stage.  Assigning 
types to phrases presents a formal means of checking that 
the accretion stays on track to an epistemological 
telos `{-} that the accumulated detail will eventually 
cohere into a propositional whole, a trajectory formally 
captured by the progressive folding-inward of phrase types 
to a propositional root.Types themselves are therefore partly structural fiats 
`{-} they are marks on intermediate processing stages 
embodying the paradigm that type-checkingcapturesthe orderliness of how successive cognitive transforms 
accrue detail toward a propositionally free-standing end-point.  
At the same time, types also have semantic interpretations; 
the/distinction, for example, is motivated by 
the cognitive difference between nominals and states of affairs 
as units of reason.  Type-theoretic semantics allows the 
structural paradigm of type-checked resolutions, the 
treefolding inwardonto its root, to be merged with 
a more semantic or conceptual analysis of types qua 
categories or classifications of meanings (or of 
units comprising meanings).  I have described this merger 
at a coarse level of classification, taking broad 
parts of speech as individual types, but similar 
methods apply to more fine-grained analysis.
By threetiersof linguistic organization, I am thinking of
different levels of granularity, distinguished by relative scales of
resolution, amongst the semantic implications of
putative type representations for linguistic phenomena.From one perspective, grammar is just a
most top-level semantics, the primordial Ontological division of language into designations of
things or substances (nouns), events or processes (verbs), qualities and attributes (adjectives),
and so forth.  Further distinctions like count, mass, and plural nouns add
semantic precision but arguably remain in the orbit of grammar (singular/plural
agreement rules, for example); the question is whether semantic detail gets
increasingly fine-grained and somewhere therein lies aboundarybetween syntax and
semantics.  The mass/count distinction is perhaps a topic in grammar more so than
semantics, because its primary manifestation in language is via agreement
(somewine in a glass;awine that won a prize;manywines
from Bordeaux).  But what about distinctions between natural and constructed objects,
or animate and inanimate kinds, or social institutions and natural
systems, matters more of grammar or of lexicon?For example, the templateI believed
Xgenerally requires thatXbe a noun
(I believed run), but more narrowly a
certaintypeof noun, something that can be interpreted
as an idea or proposition of some kind (I believed flower).
Asher and Pustejovsky point out the anomaly in a sentence
likeBob's idea weighs five pounds, which
possesses a flavor of unacceptability that feels akin to
Part of Speech errors but are not in fact syntactic
errors.  The object ofweighisfive poundsand
its subject isBob's idea, which is admissiblesyntacticallybut fails to honor our semantic convention
that the verbto weighshould be applied to things
with physical mass (at least if the direct object denotes a quantity;
contrast withLet's all weigh Bob's idea, where theideais object rather than subject).  These conventions are
analogous to Part of Speech rules but more fine-grained:
there is a meaning ofweighwhich has (like any transitive
verb) to be paired with a subject and object noun, but beyond
just being nouns the subject must be a physical body
(in effect a sub-type of nouns) and the object a quantitative
expression (another sub-type of nouns).  Potentially, type
restrictions on a coarse scale (e.g. that the subject of a verb
must be a noun) and those on a finer scale (as in this
sense ofto weigh) can be unified into an overarching theory,
which spans both grammar and semantics `{-} for instance,
both Part of Speech rules and usage conventions of the
kind often subtly or cleverly subverted in metaphor and
idioms (seeflowers want sunshine,my computer died,neutrinos are sneaky, as rather elegantly compactified
by assigning sentient states to inert things).  This is one way of
reading the type-theoretic semantic project.CertainlyOntologicalqualities of signifieds engender
agreements and propriety which appear similar to
grammatic rules.The tree wants to run away from the dogsounds wrong `{-} because
the verbwant, suggestive of propositional attitudes, seems incompatible with
the nonsentienttree.  Structurally, the problem with this sentence seems analogous
to the flawedThe trees wants to run away: the latter has incorrect singular/plural linkage,
the former has incorrect sentient/nonsentient linkage, so to speak.  But does this
structural resemblance imply that singular/plural is as much part of semantics as grammar, or
sentient/nonsentient as much part of grammar as semantics?  It is true that there are no
morphological markers forsentienceor its absence, at least in English `{-} except
perhaps foritvs.him/her`{-} but is this an accident of English or revealing
something deeper?In effect, assessments of propriety seem to operate on several levels.  First 
(not to imply an actual temporal priority, though) 
we may consider fine-grained word-sense: which lexical entrant for 
such-and-such word is plausible in the current context?  Then 
we can consider larger-scale Ontological criteria: is the subject 
of this sentence figured as material or immaterial, 
sentient or nonsentient, natural or sociocultural, spatially and/or 
temporally extended or pointwise; and so forth?  And finally, 
what Part of Speech is consistent for various words given syntactic
principles and morphological cues `{-} distinguishing noun/verb/adjective, 
and etc., along with singular/plural, mass/count, verb tense and case, 
and other criteria of morphosyntactic fit?So type-related observations can be grouped (not necessarily
exclusively or exhaustively) into those I will callmacrotypes`{-} relating mostly to Parts of Speech and the functional treatment
of phrases as applicative structures;mesotypes`{-} engaged with
existential/experiential qualities andOntologicalclassifications
like sentient/nonsentient, rigid/nonrigid, and
others I have discussed; andmicrotypes`{-} related to lexemes and word-senses.
This lexical level can includemicroclassification, or
gathering nouns and verbs by the auxiliary prepositions they allow and
constructions they participate in (such as, different cases), and
especially how through this they compel various spatial and
force-dynamic readings; their morphosyntactic resources for describing states
of affairs; and, within semantics, when we look toward even more fine-grained classifications
of particular word-senses, to reason through contrasts in usage.So, conceiving microclasses similar in spirit to Steven Pinker in
Chapter 2 of, though I'm not committing to using the
term only in the way Pinker uses it.  Cf. also, which
combines a microclass theory I find reminiscent ofThe Stuff of Thoughtwith
formal strategies like Unification Grammar.Microclasses can point out similarities
in mentalpicturesthat explain words' similar behaviors, or
study why different senses of one word succeed or fail to be acceptable in particular phrases.
There arestains all over the tableclothandpaint splattered all over the tablecloth,
but not (or not as readily)dishes all over the tablecloth.  Whilestainsis count-plural andpaintis mass-aggregate, they work in similar phrase-structures because both
imply extended but not rigid spatial presence; whereasdishescan work for
this schema only by mentally adjusting to that perspective, spatial construal shifting
from visual/perceptual to practical/operational (we might think of dishesall overthe
tablecloth if we have the chore of clearing them).  Such observations support
microclassification of nouns (and verbs, etc.) via Ontological and
spatial/dynamic/configuration criteria.Type-theoretic semantics can also apply Ontological tropes to unpack the overlapping mesh of word-senses,
likematerial objectorplaceorinstitution.
This mode of analysis is especially well illustrated when competing senses
collide in the same sentence.  Slightly modifying two examples:(former) and(latter).The newspaper you are reading is being sued.Liverpool, an important harbor, built new docks.Both have a mid-sentence shift between senses, which is analyzed
in terms oftype coercions(see alsoand).
The interesting detail of this treatment
is how it correctly predicts that such coercions are not guaranteed to
be accepted:The newspaper fired a reporter and fell off
the table.Liverpool beat Tottenham and built new docks.(again, slightly modifying the counter-examples).  Type coercions arepossiblebut notinevitable.  Some word-sensesblockcertain coercions
`{-} that is, certain sense combinations, or juxtapositions, are disallowed.
These preliminary, motivating analyses carry to more
complex and higher-scale types, like plurals (the plural of a type-coercion
works as a type-coercion of the plural, so to speak).
As it becomes structurally established that type rules at the
simpler levels have correspondents at more complex levels, the use of
type notionsper se(rather than justword sensesor other
classifications) becomes more well-motivated.Clearly, for example,
only certain kinds of agents may have beliefs or desires, so
attributing mental states forces us to conceive of their referents
in those terms:Liverpool wants to sign a left-footed striker.That newspaper plans to fire its editorial staff.Thiscanbe analyzed astype coercions; but the type-theoretic machinery should contribute
more than just obliquely stating linguistic wisdom, such as
maintaining consistent conceptual frames or joining only suitably
related word senses.  The sense ofsignas inemploy to play on
a sports teamcan only be linked to a sense of Liverpool as the
Football Club; orfireas inrelieve from dutyis only compatible with newspapers as
institutions.  These dicta can be expressed in multiple ways.
But the propagation of classifications
(likeinanimate objectscompared tomental agents) through complex type structures lends credence to the
notion that type-theoretic perspectives are more than just an expository tool;
they provide an analytic framework which integrates grammar and semantics, and
various scales of linguistic structuration.
For instance, we are prepared to accept some examples of dual-framing
or frame-switching, like thinking of a newspaper as a physical object and a city government
(but we reject other cases, likeLiverpool voted in a new city government and signed a
new striker`{-} purporting to switch from the city to the Football Club).  The rules for
such juxtapositions appear to reveal a system of types with some parallels to
those in formal settings, like computer languages.In short,Ontologicaltypes likeinstitutionorplaceserve in some
examples to partition senses of one multi-faceted word.  Here they reveal
similar cognitive dynamics to reframing-examples liketo the press, where
Ontological criteria (like reading something as a place) are triggered by
phrase-scale structure.  But there are also interesting contrasts:
thenewspaperandLiverpoolexamples
imply that some words have multiple framings which are well-conventionalized;
newspaper-as-institution feels less idiomatic and metaphorical than
press-as-place.  So these examples suggest twoaxesof variation.
First, whether the proper Ontological framing follows from other word-choices
(likefireinthe newspaper fired the reporter, which has
its own semantic needs), or from morphosyntax
(like the locative into the press); and, second, whether triggered framings work
by selecting from established word senses or by something more metaphorical.
Metaphors liketo the pressdo have an element of standardization;
but apparently not so much so to be distinct senses: note howthe pressas metaphorical place
does not work in general:at the press,near the press(butat the newspaper,near the newspaper`{-} imagine two journalists meeting outside the paper's offices `{-} sound quite reasonable).Thetype coercionanalysis works for mid-sentence frame-shifts; but other
examples suggest a more gradual conceptualblending.  For example, the
place/institution dynamic is particularly significant forrestaurant(whose spatial location is, more so, an intrinsic part of its
identity).  Being aplaceimplies both location and extension; most places are not single
points but have an inside where particular kinds of things happen.  I am not convinced
that restaurant as place and as institution are separate word senses; perhaps, instead,
conversations can emphasize one aspect or another, non-exclusively.  
We need not incorporate all framing effects viasubtypes(restaurant as either
subtype of hypotheticaltypes of allplaces or institutions, respectively).  Butplacehood, the Ontological quality of being a place `{-} or analogously being
a social institution `{-} identify associations that factor into cognitive frames; types
can then be augmented with criteria of tolerating or requiring one association or another.
So ifrestaurantis a type, one of its properties is an institutionality thatmaybe associated with its instances.  In conversation,
a restaurant may be talked about as a business or community, foregrounding this
dimension.  Or (like in asking for directions) its spatial dimension may be foregrounded.
The availability of these foregroundings is a feature of a hypothetical restaurant type,
whether or not these phenomena are modeled by subtyping or something more sophisticated.
Thenewspaperexamples suggest how Ontological considerations
clearly partition distinct senses marked by properties like objecthood or
institutionality (respectively).  Fornewspaperthe dimensions are less available for
foregrounding from a blended construal, thanunblendedby conventional usage; that
is why reframings evince a typecoercionand not a gentler shift of emphasis.
The example ofrestaurant, in contrast, shows that competing routes for
cognitive framing need not solidify into competing senses, though they trace
various paths which dialogs may follow.
But both kinds of examples put into evidence an underlying
cognitive-Ontological dynamic which has potential type-oriented models.At the most general level `{-} what I calledmacrotypemodeling `{-} a type
system recognizes initially only the grammatical backbone of expressions, and
then further type nuances can be seen as shadings and interpretations which add substance
to the syntactic form.  So in type-theoretical analysis at this more grammatic level,
we can still keep the more fine-grained theory in mind:
the relation of syntax to semantics is like the relation of a spine to its flesh,
which is a somewhat different paradigm than treating syntax as a logical or temporal
stage of processing.  Instead of a step-by-step algorithm where grammatical parsing
is followed by semantic interpretation, the syntax/semantics interface can be seen
as more analogous to stimulus-and-response: observation that a certain grammatic
configuration appears to hold, in the present language artifact, triggers a marshaling
of conceptual and cognitive resources so that the syntactic backbone can be filled in.
Perhaps a useful metaphor is grammar as gravitation, or the structure of a gravitational
field, and semantics is like the accretion of matter through the interplay of multiple
gravitational centers and orbits.  For this analogy, imagine typed lambda
reductions liketaking the place of gravitational equations;
and sentences' grammatic spine taking the place of curvature pulling mass into a planetary center.As I have argued, sentences' progression toward complete ideas can be 
assessed more semantically `{-} accretion of conceptual detail `{-} 
or more syntactically, in terms of regulated type resolutions pulling in 
from a tree's leaves to its root.  The latter model is a kind of 
schematic outline of the former, marking signposts in the accretion 
process rather like a meetings' agenda.  Type theory allows points in 
conceptual accretion to be selected `{-} corresponding to nested phrases 
`{-} where type-checking signals that the accretion is progressing 
in an orderly fashion.  Or, more precisely, type-checking acts 
as a window on a cognitive process; phrasal units are like 
periodic gaps in a construction wall allowing us to reconstruct interpretive 
processes, and the possibility of certain linguistic elements being 
assigned types marks the points where such windows are possible.  
So type theory can impose a formal paradigm on our assessment 
of sentence structure, but at the cost of sampling only 
discrete steps of an unfolding completion toward understanding.  
In practice, this discrete analysis should be supplemented 
with a more holistic and interpretive paradigm, which explores 
`{-} perhaps speculatively, without demanding thorough 
formalization `{-} the gaps between the formalizable windows.  
I will transition toward this style of analysis in the next section.At the same time, I feel that the foundations ofcognitivelinguistics deserve a little more attention.  I have usedcognitiverather informally, depending on the intuitive 
picture ofcognitivelinguistics, grammar, or indeedcognitive phenomenology, which emerges from the 
speculative project we associate with linguists/philosophers 
like George Lakoff, Mark Johnson, Leonard Talmy, 
Ronald Langacker, and Peter`{-} along with, aspoints out, phenomenologists like Jean Petitot.mentions Lakoff, Langacker, Talmy, Fauconnier, 
and others alongsidea French semiotic tradition, 
exemplified by [Jean-Pierre] Descl|'es ... and 
[Jean] Petitot-Cocorda ... which shares many features with the 
American (mainly Californian) group.At the same time, Petitot also links with a 
tradition that combines elements of phenomenology and 
Analytic Philosophy, represented by philosophers like 
Barry Smith and David Woodruff Smith and byAnalytic PhenomenologyorNaturalizing Phenomenologyprojects, the latter also being 
a large volumes Petitot co-edited.Petitot's and Barry Smith's formalizing projects were parallel
and collaborative to some extent.  Maxwell James Ramstead
in a 2015 master's thesis reviews the history elegantly:Now, thescience of salienceproposed by Petitot and Smith (1997) illustrates the
kind of formalized analysis made possible through the direct
mathematization of phenomenological descriptions.
Its aim is to account for the invariant descriptive
structures of lived experience (what Husserl calledessences)
through formalization, providing a descriptive geometry of
macroscopic phenomena, amorphological eideticsof the
disclosure of objects in conscious experience (in Husserl's
words, theconstitutionof objects).
Petitot employs differential geometry and morphodynamics
to model phenomenal experience, and Smith uses formal structures from
mereotopology (the theory of parts, wholes, and their boundaries)
to a similar effect.Cognitivein these contexts 
tends to imply desire to ground analyses on holistic
human experience, in its experiential, embodied, pragmatically-oriented, 
first-personal, and intersubjective dimensions.Note that in this senseCognitiveconnotes a very different perspective than this same 
term inresearch, for example; on the one side we have a 
philosophical commitment to the irreducibility of
human reason to computablesymbol processing, whereas 
on the other there is a paradigm where, in effect 
(perhaps simply because mental activity is presumably 
reducible to low-level biological processes), there 
existssomecomputable core of cognition, which scientists 
can unlock to build powerfulArtificial General Intelligence.Cognitive linguistics can be calledspeculativebecause its 
methodology generally relies on linguists' assessments of 
acceptability rather than empirical data from surveys, 
computational or statistical analyses of copora, or 
psycholinguistic studies of language processing or acquisition.  
Analytic Phenomenology is speculative in similar ways; 
although in some cases phenomenological structures are 
related to formal/mathematical theories like 
Mereotopology or Differential Geometry (cf. Barry Smith and 
Petitot, respectively, or Kit Fine'sPart-whole), phenomenologists' assessments of 
common perceptual patterns (and how they are 
situationally or conceptually interpreted and engaged with) 
is principally introspective.  Both phenomenologists 
and cognitive linguists, in short, introspect on 
conscious and linguistic experience to identify patterns 
which they believe are not eccentric to their own cognition,
but have some public disputability and theoretical merit.  
Of course, the overall dialog wherein philosophers debate 
and compare their own introspective reports allows 
this speculative method to have some rigor, because descriptions 
of cognitive processes `{-} in their first-personal 
facticity `{-} which seem both subjectively faithful and
structurally revealing will emerge as analyses of 
general pattern so long as multiple philosophical treatments 
agree on their fidelity to experience.  So 
analyses get theoretically favored if they meet three 
different criteria of structural productivity `{-} in the 
sense that produce new insight onto cognitive processes, 
rather than just describing cognition as an experienced
givenness `{-} plus both faithfulness to each person's 
conscious experience and also generality to many 
people's experience.So for instance, Husserl's examination of 
protention and retention in sensing spatial form during 
perceptual episodes focused on discrete, extended objects 
(e.g. inThing and Space), or his investigation of 
how intersubjectivity contributed to consolidating our 
conceptual integration of perceptual givens 
(e.g. in theCartesian Meditations), can be 
considered classic phenomenological analyses because they 
have been deemed both experientially accurate and 
theoretically insightful by subsequent generations' worth of 
public review.At their best, then, both phenomenology and cognitive linguistics 
combine introspective analysis and public disputation to 
develop theories of cognitive-experiential structures `{-} of 
how the immediate structuraion of perceptual experience as 
primordial conscious content unfolds into the
schematic and rational models of our surrounding environment
and situations, for purpose of goal-directed activity and 
inter-personal, collective reasonableness.  The underlying assumption 
is that raw structures, below the threshold of conscious 
deliberation, are enmeshed in immediate experience, and that from 
there we can identify ambient and situational patterns; 
mental representations of 
the structural and material properties of surrounding 
objects and places and the social/pragmatic rules 
governing interpersonal situations.  We can then 
posit situational prototypes and morphological principles 
apparently operating in these representations, which become 
the basis of systematic theorizing of cognitive activity 
in general, including language.In this paradigm, situational and organizational prototypes 
and patterns lend their structure to language, so 
`{-} in many cases, i.e., with respect to many 
language artifacts `{-} the scenarios influencing 
linguistic structure are these extra- or pre-linguistic 
gestalts rather than semantic to syntactic rulesper se.  But for this belief about the origin of 
(at least some) surface-level linguistic form to be 
leveraged as a diligent semantic or syntactic method, 
we need a systematic account of how phenomenological 
pattern evolve into (by grounding) linguistic 
structures.  A thorough treatment of this problem 
is far beyond the scope of one paper, but I will 
offer a few ideas in the remainder of this section.Types and PhenomenologyIn their proper context, understanding linguistic expressions 
requires binding language to objects orphenomenain 
speakers' collective perceptual (or conceptual) horizon.  
Referents are not always material things; they can even 
be theabsenceof objects, or of substance:There are footprints on the beach.There's a hole in the bucket.There are footprints leading up the hill.There are train tracks leading up the hill.In () our attention is direct not to anyobject, but 
to a certain perceptual pattern which has some factual significance, enough 
to warrant a distinct concept.  Insofar asonefootprint is a 
focus of attention, we notice some pattern of discontinuity which allows 
a foreground to emerge from a background `{-} but there is perceptual 
blend of discontinuity and continuity, since a footprint is literally 
situated in a material expanse with its surroundings.  To focus on the 
footprint `{-} a cognitive act which is both perceptual and conceptual 
`{-} we have to retain awareness both of the footprint materially 
continuous with the surrounding sand (say) and also distinct from it 
via a somewhat different color or composition (e.g., the sand in the footprint 
may be darker or more compact than the sand around it).  When talking 
aboutfootprints, plural, we have to direct attention to a perceptual 
totality, something extending over our visual field and possessing inner 
parts, but also suggesting a conceptual totality, a worthiness of 
being cognized in aggregate.In (), moreover, we conceive the totality in conjunction 
with a sense of direction, and protention.  The implication is that we 
do not seeallof the footprints, but we can discern from their 
pattern a direction which, we anticipate, will reveal more footprints.  
Meanwhile the footprints are presumably disjoint, unlike train tracks.
So the perceptual foreground in () is phenomenologically 
complex, including a totality we perceive that implies a greater 
totality, part of which is experienced anticipatorily rather than 
explicitly, along with a fragmentation which nonetheless permits a 
perceptual unification into a coherent whole.  These kinds of 
perceptual/conceptual complexities in arrangements 
`{-} blending continuity and discontinuity, wholeness and 
fragmentation, sensation and protention `{-} are canonical 
to the phenomenology of attentional foci in any cognition 
engaged with ambient situations, which certainly 
includes language.  We are not robots experiencing the 
world as a tableau of simply discrete, integral object-things.Language serves as a guide to negotiating the complexities of 
perceptual foci in inter-personal environments.  
We therefore have to understand how languagehooksinto conversants' phenomenological faculties. 
So we have, at a basic level, a 
contrast between situationally grounded or conceptually 
generalized references:Salesmen are intelligent.Salesmen are knocking on the door.Should I let them in?The effects of () 
(which with () are from theHandbook, example 44, page 169, 
chapter 6) are manifest in several 
changes to conversants' collective understanding: () 
establishes both a domain of potential reference (the salesmen 
could be subsequently identified asthe salesmenorthose salesmenorthem), and the fact of their being 
at the door established as a basis for further dialog (e.g., 
()).  The more generic () 
does not have any comparable situational effects, though 
it permits further dialog on a more generic plane.Sometimes, of course, people dialog about things that are 
perceptually evident around them collectively.  Probably 
more common, though, is that the structures of 
presentational perception translated to general 
cognitive patterns that are signified through language.  
That is to say, the rules of perceptual gestalts 
`{-} the partial continuity and partial discontinuity 
between background and foreground; and the mixture of 
singular integrity and divisibility characterizing attentional  
foregrounds `{-} are taken for granted as patterns 
typifying perception in general, whether that be 
perceptions presently occurring in the situational 
context or those conceptualized indirectly, abstractly, 
or hypothetically.  How foci of attention figure into 
perceptual continua become part of their extralinguistic 
background, reflected in lexical conventions:There are ink stains on the pages of this book.There is a pretty sunset over the river.In some neighborhoods, cicada insects make loud sounds from sunset to sunrise.The audience rose to give thundering applause.The unifying theme in these examples is that their central concepts are 
experienced in terms of a matrix of phenomenological criteria that can 
be harnessed into a general theory: how an ink stain is (in part) 
materially continuous with the stained paper that extends around it; 
howa pretty sunsetis an imprecisely bounded but conceptually 
integral atmospheric event; how () figures the audience as 
discrete individuals who, at that moment, act in such a way as to present a 
conceptual and perceptual totality; how () likewise proposes 
a totality but in a more complex fashion, because the speaker is 
describing a typicalkindof experience rather than the specific 
sounds of cicadas on one specific occasion.The matrix of continuity/discontinuity, and individual coherence balanced
(within intentionalnoemata) 
against internal structuration and diversity, 
areconceptuallyintrinsic to notions likeink stains(orfootprints),applause,sunsets, and evencicadasandnoises.  
They become part of a language's lexical machinery and 
therefore embed, in lexical conventions, phenomenological 
prototypes that imply a situational background, called 
forth by the lexical potency of the specific words.  
The point is not that the hearer of (), say, 
sees the sunset also, and thusexplicitlyundergoes the 
disclosure-experience of a sunset (in all its sensory 
vividness and individuating vagueness).  Instead, the 
patterns of sensation and (dynamic, imprecise, un-fixed) 
individuation are abstracted to prototypes poised within 
the lexicon itself to be perceived as applicable to the 
talked-about situation.  We have some idea of thekindof experience is involved in a sunset, not just 
perceptually but in the nexus of conceptual and 
rational processes which leads us to identify the sunset 
as such and estimate its phenomenal specificity 
`{-} its spatial form, its temporality `{-} so that it 
has some determinate epistemic content, something 
that can be discussed with others (Did you see the 
sunset;Is it too late to see the sunset; etc.).Phenomenological patterns in construing conceptual and 
referential foci, solicited by lexical and idiomatic 
conventions, give language flexibility and rhetorical 
flair, often bounding expressive possibilities on 
phenomenological rather than narrowly semantic grounds.  
I would dispute some of Nunberg's analyses I mentioned 
in the last section, for example, on the 
dubiousness of () or the exceptionality 
of ().  I do not find
() exceptionally jarring, considering the 
situational construal common to () along 
with () and (): 
the formationI am parkedis notjusta matter of 
a person proxy-referring their car.  Usually a person 
sayingI am parkedisintheir car, so they are 
describing a larger situation `{-} the conceptual foreground 
is the totality of the car and themselves in it.  Were the 
addressee to find them, their attention would be directed 
to the carandthe speaker, as a complex but 
integral (in that episode of time) whole.  
This then carries over to cases where the speaker 
isnotin their car.  SayingI am parked(along with 
a place-description) outlines the steps needed to return to 
the car; it is a way of framing the car's location 
operationally.  Using the first person for where the car is 
parked `{-} even when the speaker is not at that spot 
`{-} indicates that the speaker is conceiving the car's 
location in first-personal terms; that is, in terms 
of the actions the speaker is taking or anticipates 
taking.This analysis then implies that () is 
acceptable on similar operational grounds; first-personalizing 
the cars location, so to speak, foregrounds that location 
only in the context of an operational totality involving 
(presumably) going to the car and then driving it.  
If the speaker is concerned about the car not starting, 
this is relevant to how the overall situation is 
cognized, and therefore the transition from the speaker-centeredI am parked ...to the discordantand may not startis understandable.  I would similarly argue that the 
apparent difference between () and
()-() depends on how 
referential foci are established.  The phrasea beercan designate a specific glass or bottle of 
beer `{-} something perceptually and enactively/kinaesthetically 
individuated `{-} or also a specific preparation of beer,
individuated by the unique flavor of the beverage rather than 
the material identity of the liquid.  These 
differences have phenomenological overtones: if I sayI drank two beersI could mean two glasses, pints, or 
bottles; I could also mean twokindsof beer 
(maybe multiple pints of each).  The phrasing itself 
is ambiguous, but the ambiguity is extra-linguistic: 
it relates to how linguistic content binds to empirical 
givens with their rules of phenomenological 
individuation.  Thetwo pintsreading implies 
one kind of phenomenological architecture governingword-to-world fit, where elements in language 
link to discrete, perceptually/operationally integral
objects like glasses (though of course a glass of 
beer is also an integral complex where the beer and 
the glass are semi-autonomous parts).  Thetwo kindsreading is more subtle, resting conceptual focus on the 
belief that a distinct kind of beer has a distinct 
flavor that is both uniqueother beers and 
also consistent across bottles (or kegs), so it hasataste that people can jointly experience and 
talk about.  Nunberg makes the reasonable claim 
that we are more likely the hear the 
former interpretation when it comes to beers, and 
the latter one for Sauternes, but I can imagine plausible
conversations where these conventions would be reversed.To return to type theory, then, this style of 
formalization is a potential window onto how the 
lexicalized concepts negotiate the phenomenological 
options inbindingwords to phenomena.  
The matrix of continuity/discontinuity and 
individuation (singularity)/complexity (internal 
structure or diversity) forms a tableau which 
different word senses and, in explicit phrase contexts, 
different usageshook intoin different ways.  
So beerqualiquid has one conventional 
pattern inword-to-world fit, while beerquaconsumer product, or a brewer's creative 
endeavor, evinces a different phenomenological 
pattern.  This is modeled, to some approximation, 
by thetype(or as I put itmesotype) 
distinction of a liquid (more generally a substance) 
against a consumer product or social good 
(more generally a socio-cultural artifact).  We 
can refer to beers in both senses, and I have 
mentioned theories of typecoercionsor
juxtapositions (cf. Pustejovsky'sdot producttheories inor) that explore when 
the differentOntologicalconstruals or sentences can be 
combined or alternated.  At this point I would add that 
these Ontological details are not only relevant to 
semantics; they also govern how phenomenological 
patterns can be reified and typified in lexical and 
idiomatic conventions.For successful conversation, participants need to converge 
for each sentence on a common conceptual focus; not surprisingly, 
this process often reciprocates the phenomenology of 
perceptual focus, or enactive/kinaesthetic attention.  
Drawing form from phenomenological structure, referential 
signification often takes shapes that can seem opaque 
on purely logical considerations.  In analysis of 
referring expressions, then `{-} analogously to 
semantic analyses as earlier in this section `{-} we need 
to be sensitive to the possibilities of language form 
molding to perceptual and situational gestalts rather 
than predicate structure.Nunberg, for instance, investigates how the proxying effect inI am in the Whitneycan be generalized to other cases.  
I'll point out that this fits a not-uncommon rhetorical 
pattern, as in:I am in the Hall of Fame.I am on TV.Nunberg argues that reference, in these kinds of cases, willtransferbetween conceptually
linked designata, so I can refer to my car's location as my location, 
or to my painting as myself.  If we analyze this logically, 
the rule appears to be that I can substitute first-person 
reference for reference to something associated with 
myself (that I own or have created, etc.).  
He then finds that the pattern does not generalize to a 
scenario such as a painter `{-} referring to the location of her
work in transit `{-} saying something like (his example 12):I'm in the second crate on the right.In other words, Nunberg's analysis turns on theorizing variousI am in...constructions as a kind of referential transfer 
or (as I would say) proxying, and then seeking the logical rules 
behind how and when such proxying works in a first-person
(morphosyntactic) context.Obviouslyfirst personin this setting concerns
verb tense and other linguistic cues linking to
the speaker/enunciator of a piece of language; the
same term is also encountered (including elsewhere
in this paper) in the phenomenological
(and Philosophy-of-Mind) sense of
conscious, intentional experience (in contrast
to experience, or thoughts and feelings, we
attribute to others based on their behavior).On the other hand, I would argue that the pattern in 
()-() is not reducible to 
referential proxying as a logical maxim.  We can speculate thatIobliquely references, say, the bust and info about an 
athlete who has been elected to a Hall of Fame, or to the 
image of a TV personality which viewers see on screen.  
So analyzing this construction as a case where reference 
is transferred from thesubject of enunciationto 
some subordinate vehicle `{-} her bust, image, and so forth 
`{-} is a plausible gloss on the pattern.  But looking for a 
single maxim to accommodate these different cases overlooks 
the contextual particulars, in particular the backstory 
which is lexically embedded in expressions likeHall of Fameandon TV.  There is a long process 
leading to retired athletes being recognized as worthy of 
a Hall; there is also a long process whereby personalities 
get a chance to appear on television.  Actually, () 
has two interpretations `{-} someone could be on TV just momentarily, 
e.g. as a bystander during on-location news coverage `{-} or 
as someone like a policy expert who is interviewed on occasion.  
But the more interesting reading of () involves 
a person being described ason TVnot on the occasion of 
their appearing on-screen in that moment, but as a recurring 
event.  In this casebeing on TVencapsulates a backstory 
reflecting a measure of personal success and esteem, not unlike 
an athlete in a Hall of Fame or an artist in a prestigious museum.The relevant backstories govern interpretations for first-person 
reference:I am in the Halloron TVindirectly reports 
that the speaker has undergone some process which the addressee, 
assumed competent with the relevant lexicon, at least
minimally understands.
So a pattern like () packages up that backstoryin the guiseof a referring expression; it is not so much the 
athlete's bust as a target of referential proxying, but more of the 
act of referring to that bust (i.e., to the referentiable 
object manifesting the speaker's being elected) entering
the Hall of Fame backstory into the discursive ledger.  This 
carries over to the Whitney case: calling oneselfina museum does not just proxy the self for one's art, 
but uses patterns of reference to the art work as a 
vehicle for introducing the backstory of one's 
rise through the art world (or whatever is the relevant 
autobiographical context) into the conversation.I would argue, in short, that referential proxying is not a 
primarily logical operation, but relies on backstory context to 
regulate how referring patterns are received.  To establish conceptual 
foci, addressees have to negotiate the language given to them, in 
search of the focal element or foreground, often an explicit or 
hypothetical perceptual nexus.  So a painterin the Whitneymeans 
that a visitor can, in the right room and orientation, perceptually 
encounter her work; a personalityon TVmeans that the 
addressee can, at times, see her on-screen.  Those potential perceptual 
givens ground the semantics of expressions like () or 
().  This does not mean that the sentences are only 
meaningful to an addressee who wants to go find the speaker at the 
museum or on the television.  But it does present the perceptual 
ground as a foundation for the conceptual implications which 
lead from that perceptual situation: from the perceptual presence 
of an art work in a museum to the backstory of its provenance 
and implications for the artist's career or place in history; 
or from the on-screen image to the context of how TV shows 
are produced and the careers and reputations of the figures that 
show up.We approach referring expressions in these contexts at two levels: 
we identify the canonical perceptually-oriented references which 
supply a conceptual ground (the painting as perceptual 
object, the on-screen image as perceptual simulacrum); but then 
we recognize the backstory and situational context which 
determines how the perceptual ground should be understood.  
Usually the discourse isaboutthat backstory, not about 
the precise situations where explicit perceptual grounds would 
be relevant `{-} an artist probably would not say 
() only to someone looking to visit a museum to 
see her art work.  TheI am in...pattern 
turns on how these two different levels are played off 
in interpretation.
In particular, the addressee needs to ascertain why the
speaker's information is being proferred: is boasting
of their art being hung in an elite museum mostly a
way of conveying the speaker's status, so the main theme is
essentially autobiographical; or is the actual location
of the painting directly relevant to the present conversation?A statement like () (the speaker beingin the crate on the right) does not have the moreautobiographicalreading, since (absent some bizarry
imaginary scene) there is no status or reputation attached
to which crate is carrying your paintings.  But the second kind of reading,
where specific location is operationally relevant,
is plausible in some contexts; perhaps several different
artists' works are in several different crates and
the speaker wants to direct focus onto the one holding
her own works.  So, in the proper context,
I have no objection to
() orI am in that crate(forms which Nunberg rejects).
The gist here, as in (), is to establish conceptual
focus onto a painting or art work, and to do so via a
first-person referential lead-in.  So the situation determines 
firstwhythe focus is thus singled out and secondhowthe first-person reference can proxy that focus.  
In (), we single out a painting insofar as it 
is a product of the artist's creativity, so appreciation of the 
work is manifestly appreciation of the artist.  The artist is 
then referentially linked to the work because the backstory of 
how museums acquire art works overlaps with the story of 
artists' careers and reputations.In () the situation is different: presumably 
the focus is on one or several art works because of some concern 
about transporting or accessing them, so we're attending to the 
works in their guise as (fragile) physical objects.  Moreover 
the link between the artist and the work, which guides the 
referential proxying, is presumably that the artist is 
concerned about accessing and protecting those objects.  
The situational elements are different, but in both these 
cases `{-} whether paintings are in the crate or in the 
Whitney `{-} there is some structural resonance in how
situational, backstory, and hypothetical-perceptual 
gestalts are all integrated into patterns of grounding 
referential interpretation in ambient conceptual contexts.I think that linguists sometimes underestimate the 
multiplicity of layers `{-} situational, contextual, 
phenomenological (explicit and hypothetical), 
referential, contextual `{-} that all converge on 
meaning and reference.  Perhaps this tendency can
be empirically examined by testing judgments of
acceptability: the more that we incorporate
multiple layers of posssible context, the wider becomes
the circle of sentences that feel reasonable
(cf. the reasonableness, in my estimation, of
() and ()).Granted that, perhaps, with a lot of imagination
almost any construction could be deemed
plausible insomecontext.  Accordingly,
one could argue that it is analytically reasonable
to distinguish sentences that exemplify proper
language in a wide range of contexts, as compared
to sentences which could only be meaningful
in very select circumstances.  Even on this perspective,
however, we have to unpack the distinction betweengenericfromselectcircumstances `{-} what
qualities of these situations, together with
relevant word-meanings, make these examples
of broad usage patterns as against unexpected
usages that are nonsensical without a very
specific background?  Rather than being a neutral
arbiter of acceptability, such genericity is a
phenomenon that needs to be explained.  I would
argue that assessments of unacceptability will in
many cases overrate the distinction betweennormalandexceptionalcircumstances.
For example, I dispute that the beer/wine
contrast, or painting in a museum or in a crate
contrast, are such a divergence between normal and
unusual contexts that () has
essentially different plausibility than
(), or ()
compared to () and ().Failure to identify these
layers' workings can lead analysis toward searches 
for reproducible logical rules governing which constructions  
are accepted by a language-community as recurring patterns, 
and logical explanations for the limits on that 
generalizability.  This is one example of
overestimating the logicality of language in 
general, an issue I have approached in this section 
from both semantic and reference-theoretic angles.  
I will focus on this issue of logicality `{-} both its applicability 
and its limits `{-} in the next section.`[/]

`[//]sectionlabelpiqNPropciteNOverNVUnderVquotesentenceListsentenceItemswludrefglfootnotesubsectionvisavisinputrefNNtoPropargsToReturnNNtoPropYieldsPropPropToNYieldsNSeqNPplVPnewsaveboxLangackerboxilrboxuseboxProptoNNtoNspsubsectiontwolineqmarkdubiousthindecolineGardenforsAI`[/]

`[//]s2BarkerShanTGKubotaLevinep. 1RossiSivaReddyKiyoshiSudoChoeCharniakitm:ambiencesemen_gum-ud-trainGUM_voyage_merida-20JanVanEijckRikVanNoorditm:actuallysemen_gum-ud-trainGUM_voyage_socotra-16figactuallydblitm:actuallyfig:actuallydblfigactuallyfigambiencestudentsscoofficescoafterscoparentsscostudentsofficeitm:threetimes`{ }sco`{ }scoafterparentsafterparentsstudentsofficeOsborneMaxwellfigure.texfig:Iknowexample 2, p. 5AsherPustejovskyPinkerAnneVilnatp. 40ChatzikyriakidisLuop. 4MeryMootRetore`{ }lexitm:LiverpoollexZhaohuiLuoZhaohuiLuoSignatures`{ }(?)lex`{ }(?)lex`{ }ont`{ }ontp. 4Gardenforsp. 38RamsteadKitFineitm:footprintsrefitm:holerefitm:footprintsleadingrefitm:traintracksrefitm:footprintsitm:footprintsleadingitm:footprintsleadingitm:Salesmenontitm:knockingontitm:Shouldontitm:knockingitm:Salesmenitm:knockingitm:Shoulditm:Salesmenitm:inkrefitm:sunsetrefitm:cicadarefen_gum-ud-trainGUM_voyage_phoenix-30itm:applauserefitm:applauseitm:cicadaitm:sunsetitm:startitm:Sauternesitm:startitm:startitm:parkeditm:waitingitm:startitm:Sauternesitm:beersitm:MichelobsJamesPustejovskyAsherPustejovskyitm:hallrefitm:ontvrefitm:craterefitm:hallitm:ontvitm:ontvitm:ontvitm:hallitm:Whitneyitm:ontvitm:Whitneyitm:crateitm:crateitm:Whitneyitm:Whitneyitm:crateitm:crateitm:Sauternesitm:crateitm:Whitneyitm:Sauternesitm:beersitm:Michelobs`[/]

@@ ranges
3
=1
2<1>60;1,
61<2>0;2,335;4,
313<1>318;5,
336<1>1380;6,
362<1>381;7,
737<1>744;8,
1381<1>1857;9,
1425<1>1431;10,
1590<1>1608;11,
1858<1>2502;12,
2054<1>2061;13,
2217<1>2225;14,
2503<1>3066;15,
2603<1>2615;16,
3067<1>4083;17,
3168<1>0;18,
3190<1>0;19,
3319<1>0;20,
3321<1>0;22,
3433<1>3441;24,
3571<1>0;25,
3586<1>3589;26,
3599<1>3610;27,
3617<1>3620;28,
3768<1>0;29,
3838<1>3848;30,
3878<1>3882;31,
3917<1>3922;32,
4002<1>4005;33,
4024<1>0;34,
4078<1>4082;35,
4084<1>5891;36,
4287<1>4295;37,
4349<1>4359;38,
4394<2>4924;39,4412;40,
4673<1>4708;41,
4925<1>0;42,
5120<1>5127;45,
5184<1>5189;46,
5212<1>5215;47,
5232<1>5240;48,
5639<1>5644;49,
5844<1>5853;50,
5892<1>7049;51,
5957<1>5964;52,
6213<1>6216;53,
6219<1>6224;54,
6412<1>6417;55,
6462<1>6465;56,
7050<1>7311;57,
7096<1>7098;58,
7247<1>7257;59,
7270<1>7270;60,
7287<1>7287;61,
7293<1>7293;62,
7296<1>7296;63,
7312<1>8409;64,
7438<1>7450;65,
7606<1>0;66,
7607<1>0;68,
7608<1>0;70,
7788<3>7847;72,0;73,0;74,
7848<1>0;78,
7857<1>7930;81,
8410<1>9028;82,
8618<1>8737;83,
8638<1>8651;84,
9029<1>9052;85,
9053<1>10191;86,
9173<1>9181;87,
9200<1>9205;88,
9223<1>9230;89,
9244<1>9250;90,
9412<1>9417;91,
10192<1>11591;92,
10274<1>10290;93,
10483<1>0;94,
10486<1>0;96,
10493<1>10498;98,
10646<1>10655;99,
10910<1>10917;100,
11328<1>11333;101,
11443<1>11450;102,
11592<1>12691;103,
11624<3>11683;104,0;105,0;106,
11684<1>0;110,
11709<1>11783;113,
11803<1>11810;114,
11836<1>11837;115,
11909<1>11910;116,
11915<1>11925;117,
11952<1>11960;118,
12064<1>12076;119,
12157<1>12170;120,
12210<1>12216;121,
12219<1>12233;122,
12590<1>12603;123,
12605<1>12606;124,
12655<1>12665;125,
12692<1>13625;126,
12735<1>12745;127,
12763<1>12765;128,
12796<1>12802;129,
12820<1>12826;130,
12880<1>12881;131,
12982<1>12985;132,
13028<1>13034;133,
13124<1>13130;134,
13187<1>13625;135,
13226<1>13229;136,
13307<1>13315;137,
13390<1>13390;138,
13427<1>13427;139,
13512<1>13512;140,
13579<1>13579;141,
13603<1>13624;142,
13620<1>13620;143,
13626<1>14547;144,
13652<1>13654;145,
13669<1>13679;146,
13714<1>13719;147,
13723<1>13724;148,
13798<1>13800;149,
13807<1>13814;150,
13853<1>13858;151,
13930<1>13931;152,
13947<2>0;153,13949;154,
13958<1>13961;155,
13970<2>0;156,13976;157,
14112<1>14117;158,
14138<1>14145;159,
14210<1>14214;160,
14276<1>14281;161,
14315<1>14336;162,
14363<1>14364;163,
14366<1>14368;164,
14377<1>14379;165,
14388<1>14390;166,
14416<1>14417;167,
14465<1>14466;168,
14467<1>14473;169,
14482<1>14484;170,
14486<1>14492;171,
14517<1>14518;172,
14525<1>14530;173,
14548<2>0;174,15352;176,
14573<1>14588;177,
15028<1>0;178,
15064<1>0;180,
15352<1>0;182,
15353<2>0;184,17750;186,
15420<1>15434;187,
15560<1>15569;188,
15661<1>0;189,
15662<1>16234;190,
15956<1>0;191,
15968<1>0;192,
15971<1>0;193,
16333<1>16445;194,
16853<1>16865;195,
16883<1>16889;196,
16945<1>16955;197,
17052<1>0;198,
17054<1>0;199,
17103<1>0;200,
17136<1>0;201,
17221<1>0;202,
17326<1>0;203,
17335<1>17338;204,
17392<1>17407;205,
17435<1>0;206,
17436<1>17750;207,
17496<1>17510;208,
17647<1>17657;209,
17751<1>18272;210,
17755<1>17764;211,
17769<1>0;212,
17909<1>0;213,
18221<1>18236;214,
18273<1>19915;215,
18351<1>18363;216,
18419<1>18426;217,
18558<3>18834;218,0;219,0;220,
18632<2>0;224,0;225,
18695<2>0;229,0;230,
18752<2>0;234,0;235,
18839<1>0;239,
18846<1>0;241,
18856<1>18868;243,
19052<1>19059;244,
19151<1>19163;245,
19437<2>0;246,19447;249,
19447<1>0;252,
19448<3>19552;255,0;256,0;257,
19500<2>0;265,0;266,
19568<1>0;270,
19575<1>0;272,
19587<1>19607;274,
19638<1>19650;275,
19739<1>19751;276,
19839<1>19849;277,
19897<1>0;278,
19900<1>0;280,
19911<1>0;282,
19914<1>0;284,
19916<1>21351;286,
19926<1>19946;287,
19959<1>19973;288,
19975<1>19989;289,
20193<1>20196;290,
20403<1>20410;291,
20597<1>20598;292,
20640<1>20659;293,
20684<1>20691;294,
20695<1>20705;295,
21113<1>21115;296,
21277<1>0;297,
21352<2>0;299,22871;301,
21815<1>21818;302,
21841<1>0;303,
21923<2>0;304,21931;305,
21934<1>0;306,
21935<1>0;307,
21947<1>0;308,
21976<1>21989;309,
22085<1>0;310,
22191<1>0;311,
22254<1>0;313,
22264<1>0;314,
22286<1>22287;315,
22312<1>0;316,
22330<1>22333;317,
22338<1>0;318,
22379<1>22382;319,
22605<1>22606;320,
22617<1>22624;321,
22681<1>22687;322,
22872<1>24358;323,
22997<1>23011;324,
23179<1>23183;325,
23624<1>23637;326,
23723<1>23735;327,
23850<1>23859;328,
23913<1>23925;329,
23928<1>23935;330,
24069<1>24073;331,
24079<1>24086;332,
24359<1>25479;333,
24426<1>24429;334,
24593<1>24605;335,
24791<1>24795;336,
24803<1>24806;337,
24809<1>24829;338,
25396<1>25416;339,
25443<1>25449;340,
25480<1>25832;341,
25833<1>25869;342,
25870<1>26845;343,
25989<1>25997;344,
26193<1>26201;345,
26846<1>28000;346,
26995<1>27002;347,
27189<1>0;348,
27190<1>0;349,
27414<1>27427;350,
27778<1>27782;351,
28001<1>28955;352,
28519<1>28526;353,
28703<1>28706;354,
28723<1>28723;355,
28746<1>28749;356,
28956<1>30671;357,
28981<1>28992;358,
29016<1>29016;359,
29028<2>0;360,29041;361,
29072<1>29075;362,
29159<2>0;363,29175;364,
29241<1>29269;365,
29270<1>0;366,
29409<1>29413;369,
29416<1>29426;370,
29445<1>29454;371,
29476<1>29488;372,
29545<1>29552;373,
29664<1>29689;374,
29701<1>29704;375,
29838<1>29842;376,
30237<1>30244;377,
30467<1>30487;378,
30489<1>30504;379,
30506<1>30525;380,
30672<1>31538;381,
30681<1>30691;382,
30790<1>30828;383,
30860<1>30863;384,
30944<1>30947;385,
31025<1>31051;386,
31395<1>31403;387,
31460<1>31461;388,
31465<1>31471;389,
31539<1>32307;390,
32308<1>34395;391,
32420<1>32429;392,
32533<1>32541;393,
32595<1>32605;394,
32697<1>32706;395,
32775<1>32793;396,
33232<1>33522;397,
33310<1>0;398,
33397<1>0;400,
33455<1>33474;402,
33572<1>33579;403,
33726<1>33755;404,
33759<1>33798;405,
33828<1>33857;406,
33866<1>33871;407,
33891<1>33895;408,
34021<1>34026;409,
34200<1>34207;410,
34396<1>35746;411,
34506<1>34520;412,
34523<1>34527;413,
34530<1>34540;414,
34681<2>34701;415,0;416,
34693<1>0;419,
34702<3>34793;422,0;423,0;424,
34746<2>0;428,0;429,
34870<1>34883;433,
34893<1>0;434,
34896<1>0;436,
35023<3>35121;438,0;439,0;440,
35077<2>0;445,0;446,
35191<1>35198;451,
35206<1>35215;452,
35235<1>35239;453,
35659<1>35664;454,
35682<1>35692;455,
35747<1>37292;456,
35912<3>36006;457,0;458,0;459,
35958<2>0;463,0;464,
36011<1>36013;468,
36028<1>36041;469,
36255<1>36258;470,
36264<1>36294;471,
36362<1>36365;472,
36371<1>36387;473,
36532<1>36548;474,
36560<1>36572;475,
37025<1>37089;476,
37293<1>38558;477,
37302<1>37312;478,
37323<1>37333;479,
37336<1>37340;480,
37482<1>37493;481,
37646<1>37654;482,
37658<1>37666;483,
37868<1>37871;484,
37969<1>37972;485,
37975<1>38006;486,
38086<1>38097;487,
38237<1>38248;488,
38349<1>38357;489,
38405<2>0;490,38416;491,
38418<2>0;492,38431;493,
38436<1>38451;494,
38453<1>38470;495,
38559<1>40740;496,
38562<1>38574;497,
38673<1>38680;498,
38758<1>38767;499,
38849<1>38853;500,
39210<1>39217;501,
39263<1>39274;502,
39318<1>39326;503,
39575<1>39584;504,
39645<1>39647;505,
40051<1>40059;506,
40219<1>40227;507,
40310<1>40318;508,
40378<1>40385;509,
40437<1>40446;510,
40741<1>42115;511,
40782<1>40790;512,
41973<1>0;513,
42116<1>43495;514,
43496<2>0;515,46152;516,
43544<1>43552;517,
43610<1>43618;518,
43676<1>43684;519,
43716<1>43738;520,
43904<1>0;521,
43920<1>0;522,
43967<2>44210;523,0;524,
44035<1>44209;525,
44210<1>0;526,
44409<1>44430;529,
44433<1>44458;530,
44526<1>45426;531,
44703<1>45426;532,
44711<1>44729;533,
44997<1>45004;534,
45090<1>45111;535,
45189<1>45200;536,
45427<2>0;537,45435;540,
45629<1>46152;541,
45652<1>45660;542,
45722<1>0;543,
45848<1>45864;544,
46041<1>46044;545,
46121<1>46151;546,
46153<1>48464;547,
46188<1>46198;548,
46701<1>46710;549,
46711<1>0;550,
47924<1>48464;552,
48091<1>48105;553,
48247<1>48267;554,
48465<1>49520;555,
49521<1>50241;556,
49831<1>49836;557,
50242<1>50264;558,
50265<1>51752;559,
50367<1>50375;560,
50502<1>50508;561,
50537<3>50683;562,0;563,0;564,
50571<2>0;568,0;569,
50600<2>0;573,0;574,
50641<2>0;578,0;579,
50688<1>0;583,
50724<1>50729;585,
50856<1>50858;586,
51519<1>51528;587,
51753<1>52803;588,
51757<1>0;589,
51893<1>51895;591,
52123<1>0;592,
52797<1>52802;594,
52804<1>53736;595,
52961<1>52965;596,
53126<3>53205;597,0;598,0;599,
53151<2>0;603,0;604,
53185<2>0;608,0;609,
53222<1>0;613,
53238<1>0;615,
53252<1>53259;617,
53372<1>0;618,
53474<1>53485;620,
53488<1>53501;621,
53504<1>53507;622,
53603<1>0;623,
53626<1>0;625,
53737<1>55523;627,
54597<3>54807;628,0;629,0;630,
54644<2>0;634,0;635,
54684<2>0;639,0;640,
54762<3>0;644,0;647,0;648,
55093<1>55107;652,
55185<1>0;653,
55330<1>0;655,
55437<1>55440;657,
55524<1>56851;658,
55617<1>55623;659,
55677<1>55688;660,
55714<1>55723;661,
55727<1>55736;662,
55739<1>55746;663,
55748<1>55754;664,
55765<1>55771;665,
55775<1>55780;666,
56047<1>0;667,
56086<1>56095;669,
56443<1>56446;670,
56788<1>56810;671,
56812<1>56843;672,
56852<1>58295;673,
57240<1>0;674,
57269<1>0;676,
57288<1>0;678,
57363<1>0;680,
57378<1>0;682,
57385<1>0;684,
57402<1>57412;686,
57419<1>57422;687,
57497<1>57507;688,
57510<1>57511;689,
57724<1>57726;690,
57855<1>57857;691,
57879<1>57889;692,
58296<1>60436;693,
58329<1>0;694,
58740<1>58754;696,
58772<1>58788;697,
58872<1>0;698,
58879<1>0;700,
58882<1>0;702,
58945<1>58950;704,
59275<1>59291;705,
59359<1>59363;706,
59602<1>59610;707,
59679<1>59695;708,
59926<1>59934;709,
60070<1>0;710,
60138<1>60138;711,
60437<1>61726;712,
60597<1>60603;713,
60840<1>60848;714,
60877<1>60879;715,
60919<1>60935;716,
60948<1>60950;717,
61098<1>61101;718,
61117<1>61124;719,
61347<1>61355;720,
61392<1>61402;721,
61414<1>0;722,
61416<1>0;724,
61450<1>61460;726,
61727<2>0;727,62365;728,
62366<1>63859;729,
62428<1>62446;730,
62556<3>62591;731,0;732,0;733,
62581<2>0;737,0;738,
62652<1>62659;742,
63147<3>63183;743,0;744,0;745,
63246<1>63255;749,
63455<1>63859;750,
63464<1>63475;751,
63860<1>65332;752,
63915<1>0;753,
63918<1>0;755,
64004<1>64004;757,
64257<1>64278;758,
64565<1>64576;759,
64580<1>64584;760,
64782<1>0;761,
65015<1>0;763,
65054<1>65058;765,
65165<1>65175;766,
65333<1>66261;767,
65409<1>65424;768,
65427<1>65431;769,
65614<1>0;770,
65642<1>65653;772,
66000<1>66001;773,
66262<1>67483;774,
66636<1>66649;775,
66759<1>66763;776,
66902<1>0;777,
66909<1>0;779,
67484<1>68530;781,
67882<1>67886;782,
68027<1>0;783,
68097<1>68106;785,
68531<1>69740;786,
68549<1>0;787,
68569<1>68593;789,
68618<1>68633;790,
69086<1>0;791,
69090<1>69107;793,
69161<1>0;794,
69314<1>69316;796,
69357<1>69359;797,
69415<1>0;798,
69741<1>70486;800,
69745<1>0;801,
70487<1>72691;803,
70975<1>0;804,
70982<1>0;806,
70985<1>72142;808,
71086<1>71089;809,
71412<1>71418;810,
71423<1>71428;811,
71866<1>71871;812,
71875<1>71885;813,
72065<1>0;814,
72112<1>0;816,
72119<1>0;818,
72134<1>0;820,
72141<1>0;822,
!
=2
!
=3
307<2>0;247,0;250,
322<2>0;260,0;261,
!
 *
@@ details
823
1536;2:8
1216;9:13
0;2:3
16384;14:14
6144;15:15
16384;14:14
7168;16:16
6144;15:15
16384;14:14
2048;15:15
6144;15:15
16384;14:14
6656;16:16
6144;15:15
16384;14:14
6656;16:16
16384;14:14
6144;17:17
2560;18:21
6336;22:25
0;4:15
2240;22:25
0;16:27
2048;16:16
6144;17:17
2560;15:15
2048;15:15
2048;15:15
2560;26:31
2048;15:15
6144;15:15
6144;16:16
7168;15:15
2048;32:38
2048;15:15
16384;14:14
2048;16:16
7168;15:15
67135489;39:43
12288;15:15
6656;15:15
1728;22:25
1048580;28:31
1048576;32:36
6144;16:16
6144;15:15
6144;15:15
2048;15:15
6144;16:16
6144;16:16
16384;14:14
7168;16:16
6144;15:15
6144;15:15
7168;16:16
6144;15:15
16384;14:14
2048;15:15
6144;15:15
6144;15:15
2048;15:15
6144;15:15
2048;15:15
16384;14:14
2048;16:16
2240;22:25
0;37:45
2240;22:25
0;46:56
2752;22:25
0;57:68
67135489;44:55
12288;56:67
1216;68:70
1048576;69:80
1048576;7788:7847
1048576;81:83
1216;71:75
1048576;84:98
1048576;99:118
3072;76:77
16384;14:14
0;78:85
6144;16:16
1024;86:95
16384;14:14
2048;15:15
6656;16:16
2048;16:16
6144;16:16
6656;16:16
16384;14:14
2560;16:16
3264;22:25
0;119:129
2240;22:25
0;130:140
6144;15:15
6144;16:16
6144;16:16
6144;15:15
6144;15:15
16384;14:14
67135489;44:55
12288;56:67
1216;68:70
1048576;141:152
1048576;11624:11683
1048576;153:155
1216;71:75
1048576;156:170
1048576;171:191
2048;76:77
6144;15:15
2048;15:15
6144;15:15
2048;15:15
2048;16:16
6144;76:77
6144;15:15
6144;16:16
2048;15:15
2048;15:15
6144;15:15
6144;15:15
16384;14:14
6144;15:15
0;15:15
2048;15:15
6144;15:15
6144;15:15
6144;15:15
2048;15:15
6144;15:15
5120;78:85
6144;15:15
2048;15:15
6144;15:15
2048;15:15
6144;15:15
6144;15:15
2048;16:16
2048;15:15
16384;14:14
6144;15:15
6144;15:15
6144;15:15
2048;15:15
6144;15:15
6656;15:15
6144;15:15
6656;15:15
6144;96:102
4096;15:15
6144;15:15
6656;96:102
0;15:15
2048;16:16
7168;16:16
6144;16:16
6144;15:15
2048;15:15
2560;15:15
4096;15:15
6144;76:77
6144;15:15
6144;15:15
2048;15:15
4096;15:15
6144;76:77
5120;15:15
6144;15:15
6144;15:15
1216;103:107
0;192:205
16384;14:14
6144;16:16
192;108:110
0;206:217
192;108:110
0;218:232
1216;103:107
0;233:243
5312;103:107
0;244:254
16384;14:14
3072;16:16
6144;16:16
2048;111:118
1024;78:85
2048;119:130
6144;17:17
6144;18:21
5120;78:85
6144;15:15
6144;15:15
2048;16:16
6144;18:21
2048;111:118
6144;18:21
2048;17:17
2048;131:148
6144;18:21
6656;15:15
6144;15:15
2560;149:162
1024;78:85
6144;16:16
6144;16:16
16384;14:14
6144;16:16
6144;111:118
0;163:171
6144;16:16
16384;14:14
6144;15:15
2048;15:15
67135489;44:55
12288;56:67
1216;68:70
1048576;255:262
1048576;18558:18631
1048576;263:265
4096;56:67
1216;68:70
1048576;266:271
1048576;18632:18694
1048576;272:274
4096;56:67
1216;68:70
1048576;275:279
1048576;18695:18751
1048576;280:282
4096;56:67
1216;68:70
1048576;283:289
1048576;18752:18834
1048576;290:292
192;108:110
0;293:300
192;108:110
0;301:306
6144;15:15
6144;15:15
6656;15:15
3776;172:181
33685504;182:194
1048576;307:306
1217;195:199
131072;182:194
1048576;307:306
2240;108:110
0;307:320
135299072;19437:19447
67125249;44:55
12288;56:67
1216;68:70
1048576;321:321
1048576;19448:19499
192;200:205
131072;182:194
1048576;322:321
1048580;322:321
1048576;322:324
4096;56:67
1216;68:70
1048576;325:325
1048576;19500:19552
1048576;326:328
192;108:110
0;329:333
192;108:110
0;334:340
6656;15:15
2048;15:15
6144;15:15
6144;15:15
192;108:110
0;341:345
192;108:110
0;346:352
192;108:110
0;353:360
192;108:110
0;361:366
16384;14:14
6144;15:15
2048;15:15
2560;15:15
2048;16:16
6144;16:16
6144;15:15
2048;15:15
6656;15:15
2048;15:15
6144;15:15
4288;22:25
0;367:380
5312;103:107
0;381:390
16384;14:14
6144;16:16
4608;18:21
6144;111:118
4096;16:16
2048;18:21
6144;206:212
2048;17:17
6144;16:16
6144;18:21
192;108:110
0;391:399
6144;213:216
4608;17:17
2048;15:15
6144;18:21
2048;15:15
2048;17:17
2048;15:15
6144;15:15
6144;16:16
2048;15:15
16384;14:14
6144;15:15
2048;16:16
7168;16:16
6144;16:16
4608;16:16
6144;15:15
2048;15:15
2048;15:15
2048;15:15
16384;14:14
6144;16:16
6144;16:16
6144;15:15
6144;16:16
6144;15:15
7168;15:15
6144;16:16
16384;14:14
16384;217:235
16384;14:14
6144;16:16
7168;16:16
16384;14:14
7168;15:15
2048;17:17
4096;18:21
6144;16:16
6144;16:16
16384;14:14
6144;16:16
4096;15:15
6144;15:15
6144;15:15
16384;14:14
6144;15:15
6144;15:15
0;236:247
0;15:15
6144;15:15
0;236:247
0;15:15
3072;16:16
192;22:25
1048580;400:414
1048576;415:430
6144;15:15
6144;16:16
2048;16:16
4608;15:15
6144;16:16
2048;15:15
4608;15:15
6144;15:15
2048;15:15
2048;15:15
2048;15:15
512;15:15
16384;14:14
6144;16:16
6144;15:15
2048;15:15
2048;15:15
2048;15:15
6144;16:16
6144;16:16
6144;16:16
16384;14:14
16384;14:14
4608;15:15
6144;15:15
6144;16:16
6144;15:15
2048;16:16
4096;78:85
2240;22:25
0;431:436
2240;22:25
0;437:446
6144;15:15
6144;16:16
6144;15:15
2048;15:15
2048;15:15
6144;16:16
4608;16:16
6144;16:16
6144;16:16
16384;14:14
6144;15:15
6144;15:15
2048;15:15
16384;78:85
12480;22:25
1048580;447:451
1048576;452:470
4800;22:25
1048580;471:474
1048576;475:488
67125249;44:55
12288;56:67
1216;68:70
1048576;489:489
1048576;34702:34745
1048576;490:492
4096;56:67
1216;68:70
1048576;493:505
1048576;34746:34793
1048576;506:508
6144;16:16
3264;22:25
0;509:518
2240;22:25
0;519:538
67133441;44:55
12288;56:67
1216;68:70
1048576;539:539
1048576;35023:35076
1048580;540:542
1048576;543:545
4096;56:67
1216;68:70
1048576;546:546
1048576;35077:35121
1048580;547:549
1048576;550:552
4608;15:15
2048;15:15
6144;16:16
6144;15:15
6144;16:16
16384;14:14
67133441;44:55
12288;56:67
1216;68:70
1048576;553:553
1048576;35912:35957
1048576;554:556
4096;56:67
1216;68:70
1048576;557:557
1048576;35958:36006
1048576;558:560
6144;15:15
2048;16:16
6144;15:15
6144;16:16
6144;15:15
4608;16:16
6144;16:16
512;16:16
6144;15:15
16384;14:14
6144;16:16
6144;15:15
6144;15:15
2048;15:15
6144;15:15
6144;15:15
6144;16:16
6144;16:16
2048;15:15
2048;15:15
6144;15:15
6144;15:15
2048;236:247
0;15:15
2048;236:247
1024;15:15
2048;15:15
3072;15:15
16384;14:14
6144;16:16
2048;16:16
3072;15:15
6144;15:15
6144;16:16
6144;16:16
512;16:16
6144;16:16
3072;15:15
6144;16:16
6144;16:16
6144;16:16
6144;15:15
2048;15:15
16384;14:14
6144;15:15
6144;149:162
16384;14:14
1024;248:259
16384;14:14
7168;16:16
6656;15:15
6144;16:16
2560;16:16
6144;260:269
6656;260:269
4096;78:85
6144;260:269
6144;16:16
192;22:25
1048580;561:564
1048576;565:574
7168;16:16
6144;16:16
5120;78:85
67118081;39:43
3072;16:16
2048;16:16
6144;16:16
6144;16:16
3264;22:25
1048580;575:579
1048576;580:587
4096;16:16
5120;78:85
6656;16:16
6144;270:271
2048;16:16
6144;15:15
2048;16:16
16384;14:14
6144;16:16
7168;15:15
192;22:25
0;588:594
1024;78:85
2048;15:15
2048;15:15
16384;14:14
16384;14:14
2560;15:15
1024;86:95
16384;14:14
6144;16:16
6144;15:15
67139585;44:55
14336;56:67
1216;68:70
1048576;595:608
1048576;50537:50570
1048576;609:611
4096;56:67
1216;68:70
1048576;612:619
1048576;50571:50599
1048576;620:622
4096;56:67
1216;68:70
1048576;623:643
1048576;50600:50640
1048576;644:646
4096;56:67
1216;68:70
1048576;647:661
1048576;50641:50683
1048576;662:664
192;108:110
0;665:678
2048;15:15
6144;15:15
2048;15:15
16384;14:14
192;108:110
0;679:699
6144;15:15
192;108:110
0;700:720
0;16:16
16384;14:14
6656;16:16
67139585;44:55
14336;56:67
1216;68:70
1048576;721:732
1048576;53126:53150
1048576;733:735
4096;56:67
1216;68:70
1048576;736:747
1048576;53151:53184
1048576;748:750
4096;56:67
1216;68:70
1048576;751:760
1048576;53185:53205
1048576;761:763
192;108:110
0;764:775
192;108:110
0;776:787
2048;15:15
192;108:110
0;788:799
6144;15:15
6656;15:15
2048;15:15
192;108:110
0;800:809
192;108:110
0;810:821
16384;14:14
67139585;44:55
14336;56:67
1216;68:70
1048576;822:828
1048576;54597:54643
1048576;829:831
4096;56:67
1216;68:70
1048576;832:841
1048576;54644:54683
1048576;842:844
4096;56:67
1216;68:70
1048576;845:854
1048576;54684:54761
1048576;855:857
1216;71:75
1048576;858:872
1048576;873:893
4096;56:67
1216;68:70
1048576;894:905
1048576;54762:54807
1048576;906:908
6144;15:15
192;108:110
0;909:920
192;108:110
0;921:930
6144;15:15
16384;14:14
2048;16:16
6144;15:15
6144;15:15
2048;15:15
2560;15:15
2048;15:15
6144;15:15
2048;15:15
192;108:110
0;931:940
6144;15:15
6656;15:15
0;15:15
2048;15:15
16384;14:14
192;108:110
0;941:949
192;108:110
0;950:962
192;108:110
0;963:971
192;108:110
0;972:980
192;108:110
0;981:990
192;108:110
0;991:1001
6144;15:15
6144;15:15
6144;15:15
6144;15:15
6144;15:15
6144;15:15
6144;15:15
16384;14:14
192;108:110
0;1002:1010
6656;15:15
7168;15:15
192;108:110
0;1011:1023
192;108:110
0;1024:1032
192;108:110
0;1033:1045
6656;15:15
6656;15:15
6144;15:15
6144;16:16
2560;16:16
3072;16:16
6144;96:102
6656;15:15
16384;14:14
6144;16:16
6144;16:16
6144;15:15
2048;16:16
6656;15:15
6144;16:16
2048;16:16
6144;16:16
3072;16:16
6336;22:25
0;1046:1061
2752;22:25
0;1062:1077
6144;16:16
16384;248:259
16384;14:14
16384;14:14
6656;15:15
67139585;44:55
14336;56:67
1216;68:70
1048576;1078:1085
1048576;62556:62580
1048576;1086:1088
4096;56:67
1216;68:70
1048576;1089:1096
1048576;62581:62591
1048576;1097:1099
4608;16:16
67137537;44:55
14336;56:67
1216;68:70
1048576;1100:1108
1048576;63147:63183
1048576;1109:1111
6656;16:16
1024;78:85
6144;16:16
20480;14:14
192;108:110
0;1112:1119
192;108:110
0;1120:1127
4608;15:15
6144;16:16
6656;15:15
2048;15:15
192;108:110
0;1128:1135
192;108:110
0;1136:1143
6144;16:16
6144;15:15
20480;14:14
6144;15:15
6144;15:15
192;108:110
0;1144:1151
6656;15:15
6656;16:16
20480;14:14
6144;15:15
6144;15:15
192;108:110
0;1152:1162
192;108:110
0;1163:1170
16384;14:14
6144;16:16
192;108:110
0;1171:1181
6144;16:16
20480;14:14
192;108:110
0;1182:1190
512;16:16
4608;16:16
192;108:110
0;1191:1199
6144;15:15
192;108:110
0;1200:1210
6144;15:15
6656;15:15
192;108:110
0;1211:1221
16384;14:14
192;108:110
0;1222:1230
1024;14:14
192;108:110
0;1231:1239
192;108:110
0;1240:1252
4096;78:85
6144;15:15
4608;16:16
6144;16:16
4608;16:16
6144;16:16
192;108:110
0;1253:1261
192;108:110
0;1262:1272
192;108:110
0;1273:1285
192;108:110
0;1286:1294
192;108:110
0;1295:1307
 *
%% ties
&!2
!
&&2
3~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
21~
!
&!3
!
&&2
23~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
43 44~
!
&!3
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
67~
!
&!3
!
&&2
69~
!
&!3
!
&&2
71~
!
&!3
!
&!2
!
&!2
!
&&2
75 76 77~
!
&!3
!
&!1
!
&!3
!
&&2
79 80~
!
&!3
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
95~
!
&!3
!
&&2
97~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
107 108 109~
!
&!3
!
&!1
!
&!3
!
&&2
111 112~
!
&!3
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
175~
!
&!3
!
&!2
!
&!2
!
&&2
179~
!
&!3
!
&&2
181~
!
&!3
!
&&2
183~
!
&!3
!
&&2
185~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
221 222 223~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
226 227 228~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
231 232 233~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
236 237 238~
!
&!3
!
&!1
!
&!3
!
&&2
240~
!
&!3
!
&&2
242~
!
&!3
!
&!2
!
&!2
!
&!2
!
&&2
248~
!
&!2
!
&!3
!
&&2
251 254~
!
&!2
!
&!3
!
&&2
253~
!
&!3
!
&!1
!
&!2
!
&!2
!
&&2
258 259 263 264~
!
&!3
!
&!1
!
&&2
262~
!
&!2
!
&!3
!
&!3
!
&!3
!
&!2
!
&&2
267 268 269~
!
&!3
!
&!1
!
&!3
!
&&2
271~
!
&!3
!
&&2
273~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
279~
!
&!3
!
&&2
281~
!
&!3
!
&&2
283~
!
&!3
!
&&2
285~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
298~
!
&!3
!
&&2
300~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
312~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
367 368~
!
&!3
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
399~
!
&!3
!
&&2
401~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
417 418~
!
&!3
!
&!3
!
&&2
420 421~
!
&!3
!
&!3
!
&!2
!
&!2
!
&&2
425 426 427~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
430 431 432~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
435~
!
&!3
!
&&2
437~
!
&!3
!
&!2
!
&!2
!
&&2
441 442 443 444~
!
&!3
!
&!1
!
&!3
!
&!3
!
&!2
!
&&2
447 448 449 450~
!
&!3
!
&!1
!
&!3
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
460 461 462~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
465 466 467~
!
&!3
!
&!1
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
527 528~
!
&!3
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
538 539~
!
&!3
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
551~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
565 566 567~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
570 571 572~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
575 576 577~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
580 581 582~
!
&!3
!
&!1
!
&!3
!
&&2
584~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
590~
!
&!3
!
&!2
!
&&2
593~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
600 601 602~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
605 606 607~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
610 611 612~
!
&!3
!
&!1
!
&!3
!
&&2
614~
!
&!3
!
&&2
616~
!
&!3
!
&!2
!
&&2
619~
!
&!3
!
&!2
!
&!2
!
&!2
!
&&2
624~
!
&!3
!
&&2
626~
!
&!3
!
&!2
!
&!2
!
&!2
!
&&2
631 632 633~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
636 637 638~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
641 642 643~
!
&!3
!
&!1
!
&!3
!
&&2
645 646~
!
&!3
!
&!3
!
&!2
!
&&2
649 650 651~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
654~
!
&!3
!
&&2
656~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
668~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
675~
!
&!3
!
&&2
677~
!
&!3
!
&&2
679~
!
&!3
!
&&2
681~
!
&!3
!
&&2
683~
!
&!3
!
&&2
685~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
695~
!
&!3
!
&!2
!
&!2
!
&&2
699~
!
&!3
!
&&2
701~
!
&!3
!
&&2
703~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
723~
!
&!3
!
&&2
725~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
734 735 736~
!
&!3
!
&!1
!
&!3
!
&!2
!
&&2
739 740 741~
!
&!3
!
&!1
!
&!3
!
&!2
!
&!2
!
&!2
!
&&2
746 747 748~
!
&!3
!
&!1
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
754~
!
&!3
!
&&2
756~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
762~
!
&!3
!
&&2
764~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
771~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
778~
!
&!3
!
&&2
780~
!
&!3
!
&!2
!
&!2
!
&&2
784~
!
&!3
!
&!2
!
&!2
!
&&2
788~
!
&!3
!
&!2
!
&!2
!
&&2
792~
!
&!3
!
&!2
!
&&2
795~
!
&!3
!
&!2
!
&!2
!
&&2
799~
!
&!3
!
&!2
!
&&2
802~
!
&!3
!
&!2
!
&&2
805~
!
&!3
!
&&2
807~
!
&!3
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&!2
!
&&2
815~
!
&!3
!
&&2
817~
!
&!3
!
&&2
819~
!
&!3
!
&&2
821~
!
&!3
!
&&2
823~
!
&!3
!
