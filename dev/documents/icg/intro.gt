
`p.
Formal presentations of natural-language grammar are 
often classified as either `q.dependency` grammars 
(concentrating on inter-word relationships) 
or `q.constituency` grammars (focusing on 
phrase structure and phrase hierarchies).  
Unlike more rigidly formal models, 
Ronald Langacker's `i.Cognitive` Grammar 
does not usually engage in very fine-grained, 
schematic representations of all the structuring 
elements contributing to a sentence's or 
expression's meaning.  Instead, Langacker will 
call attention to particular features in an 
overall linguistic artifact those which reveal 
specific facets of language as a conceptualized 
system: cognitive construals of referents, 
extra-linguistic situations, and word-senses; 
relations of current sentences to previous 
discourse; speakers' epistemic attitudes and 
relations to the objects topicalized in their 
speec-acts.  In general, the analytic focus 
gives an impression of prioritizing intermediate 
linguistic scales, larger than individual 
words or word-pairs but smaller than sentences 
or complex phrases.   
`p` 

`p.
Langacker does, however, recognize many established 
linguistic terms and constructs insofar as 
they establish grammatic or lexical categories, phrase-types, 
or salient inter-word relations  
(nominals, clauses, anaphora, periphrasis, complements, 
serial verbs, grammatical cases).  Cognitive Grammar 
incorporates many of the structural features identified 
by syntactic theory, or also `q.generative` semantics; 
it differts from established paradigms however by 
seeking explanatory grounds for these identified 
patterns in cognitive construals of situations and 
dialog, rather than in regulated grammatic behavior or 
logically structured semantic models.  The 
rule-bound and logical core of syntax and semantics 
is, at one level, undeniable %-- certainly we 
do hear certain potential rearrangements of an 
expression as grammatically incorrect, while other
seemingly similar transformations are accepted.  
Likewise, although word senses tend to proliferate, 
there is usually some logical structure binding 
senses together in a lexical network.  One 
recurring example in Langacker's writings is 
`i.ring`/: a network of diverse but interconnected 
meanings join the senses of `i.diamond`/, `i.wedding`/, or 
`i.smoke` ring with `i.criminal` or `i.boxing` 
rings.    
`p`

`p.
Because grammar operates according to restricted 
transform rules %-- we can build complex expressions 
from simpler ones only via certain sorts of 
insertions and rearrangements %-- and because 
semantic meanings, while often subtly organized 
(more so than crisp mappings like 
`i.cat` to `i.felid`/), it is tempting to picture 
language as logically structured.  It is tempting 
to envision that meanings emerge from sentences 
via a series of stylized deductions which people 
instinctively follow, like replaying a notated 
chess match or repeating the same morning commute.  
Linguistic novelty is inevitable but constrained, 
requiring just a localized mental readjustment 
against a familiar global pattern, like needing to take 
two subway lines instead of one to arrive at a 
desired transfer point (as a metaphor, say, for 
replacing adjectival phrases with a finite clause: 
`i.I hate the crowded morning subways` with 
`i.I hate the subways that are so crowded in the morning`/). 
`p`

`p.
Langacker invites us, however, to see language as less 
predictable than that; the structured and regulated 
facets of language coexist with a dynamics wherein 
speakers try to match language against their conceptions 
and construals of situations, which are the themes 
and the enveloping social ambients contextualizing 
language artifacts as such.  The basic Cognitive Linguistic 
philosophy that language is `i.subtle` hews closely to 
a parrallel %-- perhaps Cognitive `i.Phenomenological` 
%-- dictum that `i.reality` is subtle and complex, 
especially for social, emotional, goal-oriented humans.  
It is difficult to produce quasi-mathematical logical 
models of human affairs, with our rituals and abstractions 
that have circuitous links with concrete states of affairs: 
registering to vote, a celebratory meal, reading about the 
history of architecture.  The human world is not a 
trivial robot's world of movable blocks and generic 
rooms.  If we cannot give a constructively logical account 
of the social environs which we talk `i.about`/, we 
cannot hope to achieve a plausible analysis of 
language which starts from an assumption of its inherent 
logical order.   
`p`

`p.
Perhaps a more propitious paradign is to see language as a 
membrane where a quasi-logical systematicity 
runs up against a socially (and not logically) organized 
world.  We are simultaneously aware of 
language's structural norms and of the nuances of 
pragmatic social milieus that are impossible to 
encapsulate in a predicate logic.  We therefore 
creatively encode social interactions %-- expressions 
of belief but also practical enactions and syncronizing 
collective construals %-- within the space of 
linguistic possibility, sometimes bending rules to 
communicative effect.  To understand linguistic 
performance we cannot, then, only look to the 
states of affairs apparently introduced via 
linguistic declarations, like `i.John spilled 
wine on the carpet`/.  The actual discourse 
implies more than a propositional gloss might 
identify (e.g. that there is now some wine absorbed 
by the carpet) %-- in particular, we hear that 
the spill was accidental; that the wine was originally in 
some container that would normally prevent spillage; 
and that the carpet is resting on the floor (not, 
say, hanging decoratively high up on a wall).   
`p`

`p.
In short, to properly analyze linguistic meaning we 
have to consider how elements of language combine 
to signify speakers' epistemic `i.attitudes`/: 
choosing `i.spill` rather than `i.pour` connotes 
that the speaker believes the act was unintentional 
(compare `i.John poured wine on the duck`/).  
Epistemic attitudes determine how we understand 
and interpret situations, both in terms of 
their underlying causative explanation 
(as in contrasting intentional from accidental 
actions) and in terms of mental organization 
(such as patterns of visual focus when 
perceptually spanning an environment where 
the situation is playing out).  So 
`i.I walked toward two dogs barking in front of the house` 
suggests a not-identical architectonic of visual foci 
than `i.I walked toward the front of the house where 
two dogs were barking`/.  Similarly, Langacker 
contrasts the `q.direction of mental scanning` in 
(his example 23, p. 82): 

`sentenceList,
`sentenceItem; The hill gently rises from the bank of the river.
`sentenceItem; The hill gently falls to the bank of the river.
`sentenceList`

In brief, the philosophical starting point 
for linguistic analysis should be that 
sentences communicate `i.epistemic attitudes` 
(beliefs but also precis of how speakers 
causally interpret, perceptually scan, 
or seek to influence situations), and not 
(except indirectly) states of affairs.  
That is, language signifies contents of 
an intellectual, not extramental, nature. 
`p`

`p.
To the degree that structural norms operate in 
language, then, they are pressured to be 
representationally adequate for cerebral 
contents, not impersonal states of affairs.  
Setting aside occasional falsification, we 
can assume that people usually desire to 
create faithful linguistic encodings 
of their situational construals, limited 
primarily by a sense of what level of 
detail is dialogically appropriate.  
Speakers will select whatever lexical and 
syntactic packaging is appropriate to best 
describe their mental attitudes, with 
adequate (but not excess) precision.
`p`

`p.  
This is the only essential structuring 
force we should recognize in language: 
speakers are under no obligation to use 
standardized syntactic constructions, or 
even conventionalized word senses.  
Indeed, many linguistic `q.rules` shoud 
be seen as approximate.  There question of 
whether or not a certain construction is 
or is not grammatically acceptable is 
often not black-or-white.  Langacker's 
analyses frequently point to semantically 
and syntactically similar patterns which 
nonetheless have different levels of 
normalcy, as in (LangIntro pp. 430-432):

`sentenceList,
`sentenceItem; Those problems were fully expected by the organizers.
`sentenceItem; To encounter those problems was fully expected by the organizers.
`sentenceItem; That she will graduate in June is expected by her mother.
`sentenceItem; That they would encounter problems was expected by everybody.
`sentenceItem; The painters expect that they will finish on time.
`sentenceItem; That they will finish on time is expected by the painters.
`sentenceList`

He gives each of these different levels of acceptability.  
The most uncomfortable-sounding of these in my mind are  
(3) and (6); Langacker argues that (4) sounds better than (3) but 
that the two sentences have similar rationales both 
in grammatic form (e.g., opening with a subordinate finite clause) 
and in semantic content (that clause profiling a grounded 
process which becomes treated as an object of thought by 
a sentient subject in the main clause).  Nonetheless, 
(3), (4), and (6) are not equally conventional.  The reasons 
for this may be subtle (see LangIntro p. 432), and I will 
discuss this example in more detail below.  The present 
point, though, is that acceptability criteria can 
depend on holistic conceptual and situational features 
of a sentence, rather than on mechanical concordance 
with specific grammatic laws or templates.  This can  
be difficult to reconcile with the idea that grammar 
determines how meanings fuse into wholes %-- if 
holistic meaning is prerequisite for adjudicating 
syntactic form, then how do addressees infer 
the constructional sequence which is proper for 
perceiving the holistic meaning which a speaker intended?
`p`

`p.
In the presence of these issues we should assume that 
grammar is more anarchic than linguists traditionally 
realize.  There may be grammatic rearrangements which 
are reasonable even if unconventional; there may also 
be conventional norms which persist even if 
violations of those may still yield understandable 
speech-acts.  In these sorts of situations, 
the norms of a language will tend to be loosened, 
and the `q.templates` of entrenched constructions tend 
to multiply.  New patterns, or new levels of acceptability 
collectively granted to constructions or usages 
that were once dubious, may emerge under the same 
dynamics as new lexemes and word-senses: if there 
is a semantic niche for some pattern, to either 
communicate some idea more precisely or 
with a new degree of precision in a given 
register (on a scale of casual to formal language).  
For instance, a novel pattern might become popular if 
it yields an idiom in everyday speech which 
equals the precision of an entrenched but overly 
formal-sounding alternative.  One of the 
forces giving impetus to novel constructions 
is, I think obviously, a speech community's 
desire to have a casual, quotidian language 
register equally expressive as more formal, 
professional language.  When an idiom feels 
to old-fashioned or non-casual, English 
speakers seem compelled to settle on an 
alternative: `i.No problem` instead of 
`i.You're welcome`/; `i.dude` or `i.boss` 
instead of `i.sir`/; `i.gotta` instead of `i.must`/.  
`p`

`p.
The pressure against excess novelty comes from 
our concern that linguistic productions will 
be opaque to our addressees, and therefore 
fail to properly interpersonalize our 
private epistemic framings.  But a `i.lack` 
of novelty can be undesirable for 
similar reasons: rote expressions might cause 
listeners to miss the nuances of one's attitudes.  
People will often choose words carefully, 
and expect their hearers or readers to 
understand that they are consciously selecting 
some words, in lieu of others, to signify that 
the rejected words are less appropriate for 
the speaker's mindset: 

`sentenceList,
`sentenceItem; If we had had confidence the president clearly 
did not commit a crime, we would have said so.
`sentenceItem; I think he's rather ... uh ... competitive 
(see LangIntro p. 473).
`sentenceList`   
`p`

`p.
These paradigms do not preclude the use of technical 
vocabulary, or formal representations, to explicate 
the structures which speakers avail themselves of; 
our only requirement is that we keep in the 
background a cognitively-oriented picture of 
the dynamic processes through which linguistic 
norms get perpetuated and, on occasion, changed.  
In general, a choice of wording or phraseology 
which is `i.licensed` by the relevant syntactic 
or semantic rules will only `i.actually` be selected, 
in lieu of alternatives, by (in the speaker's 
estimation) best capturing her epistemic attitudes 
to a desired granularity.  Adjectival phrases, 
for instance, can be `q.factored out` to infinite or 
finite clauses, supporting different scope of supplemental 
detail: 

`sentenceList,
`sentenceItem; I like Ontario wines.
`sentenceItem; I like red wines from Southern Ontario.
`sentenceItem; I like dry red wines that come from Southern Ontario 
wineries using traditional European grape varieties.
`sentenceList`   

Faced with a goal of conveying some mental content, linguistic 
structures act as `q.gears` that can be manipulated to 
fine-tune how the speakers' mental picture, explanation, 
sentiment, and intentions are communicated, along with level 
of detail: 

`sentenceList,
`sentenceItem; John is embarrased because he spilled some wine on the carpet.
`sentenceItem; John shouldn't be embarrased about spilling wine on the carpet, 
because they served us wine in awkward plastic cups.
`sentenceItem; I'm going to tell John not to be embarrased about spilling wine on the carpet, 
because they served us wine in these dumb plastic cups.
`sentenceList`

The formal patterns identified by dependency or constituency 
grammars can therefore be seen as fleshing out the 
structural maxims which `i.allow` constructions to be 
affordances which speakers avail themselves of, 
metaphorical dials which are `q.turned` so as to vary 
different aspects of construal: level of detail, 
perceptual organization (e.g., which visual sites are focal 
and which peripheral), force dynamics, other 
people's intentions and rationales (via a 
`q.theory of other minds`/), causative interpretations 
more generally, and so forth.    
`p`

`p.
Since linguistic order is part of what makes these `q.gears` 
work, it is consistent with Cognitive Grammar to seek 
rigorous theories for the structural patterns that
articulate the space of variation within which 
cognitive construals `i.can` be finely portrayed.  
Langacker often defers to established classifications and 
characterization of the structure and behavior of 
lexemes, phrases, and clauses.  In the same vein, 
it may be useful to consider formal parse-reconstructions 
of sentences, or annotations of word-pairs, to 
notate the structural patterns which dependency 
or constituency grammars identify in linguistic 
performance %-- insofar as this formalization is 
understood not as a mechanical recipe for 
expressions' meaning but rather as an itemization 
of structural features through which expressions are 
mapped against cognitive attitudes. 
`p`

`p.
This then leads to the question of what 
`i.sorts` of formal representations are most 
appropriate for Cognitive Grammar.  There 
are points in Langacker's analyses which 
seem to cut against both Dependency and 
Constituency traditions.  For example, 
consider his analysis of how adjectives which 
each modify one noun cluster together: 

`quote,
Even confining our attention to modifiers traditionally classed as adjectives, we
observe considerable semantic diversity and numerous departures from the prototype
of describing inherent properties.  For example, some adjectives specify position in
a sequence or location in time: `i.my first teacher`/, `i.our next president`/, 
`i.a prior commitment`/, 
`i.future events`/, `i.a former girlfriend`/.  
Others assess the validity of the nominal 
type specification: `i.genuine leather`/, `i.fake Rolex`/, `i.putative expert`/, 
`i.real gold`/, `i.counterfeit tickets`/, `i.true patriot`/.  
These shade into adjectives indicating the referent's status
with respect to a category: `i.typical doctor`/, `i.perfect circle`/, 
`i.complete idiot`/, `i.canonical example`/, `i.ordinary member`/, 
`i.representative instance`/.  Rather than intrinsic properties, 
many adjectives describe how a thing is experienced by others: 
`i.comfortable chair`/, `i.scary movie`/, `i.offensive statement`/, 
`i.pleasant evening`/, `i.welcome break`/, 
`i.unsatisfactory answer`/.  
These in turn shade into evaluative assessments whose basis may be
entirely subjective: `i.marvelous report`/, `i.charming couple`/, 
`i.wonderful vacation`/, `i.darling restaurant`/, 
`i.horrible person`/.  Instead of a property, certain adjectives specify which
domain a thing pertains to: 
`i.electrical engineer`/, `i.mental hospital`/, `i.corporate executive`/,
`i.medical textbook`/, `i.culinary institute`/.  
Still other adjectives relate to quantity: `i.abundant
resources`/, `i.rare coins`/, `i.countless opportunities`/, 
`i.infinite patience`/, `i.meager allowance`/.
In fact, absolute quantifiers (`i.many`/, `i.few`/, `i.much`/, `i.little`/, 
`i.several`/, `i.nine`/, etc.) qualify as
adjectives from both a semantic and a grammatical standpoint.
(LangIntro p. 320)
`quote`

Langacker then argues, as delineated by this sort of 
classification, that aggregates of adjectives are 
pieced together in a generally fixed order: 

`quote,
There is arguably
an overall tendency for proximity to the head to correlate with intrinsicness of the
property specified. Quantifiers are always farthest from the head: `i.nine black cats`/,
`i.*black nine cats`/, `i.several important visitors`/, 
`i.*important several visitors`/. Closest to
the head are adjectives that directly pertain to type. Domain adjectives have to be
adjacent to the head: `i.excellent culinary institute`/, 
`i.*culinary excellent institute`/, `i.young
electrical engineer`/, `i.*electrical young engineer`/. 
Indeed, since they also resist predicative use 
(e.g. `i.*The engineer is electrical`/), 
they might best be analyzed as part of the
head. Also close to the head are modifiers that assess a type specification's validity:
`i.large fake diamond`/, `i.*fake large diamond`/, 
`i.cheap imitation leather`/, 
`i.*imitation cheap leather`/. 
A number of specific patterns are well established.  
For example, adjectives
of nationality follow those assessing validity but precede domain adjectives: 
`i.true American patriot`/, 
`i.fake Moroccan leather`/, `i.British mental hospital`/, 
`i.German corporate executive`/, `i.genuine French culinary institute`/.  
Modifiers describing size, color, and
material normally occur in that order: `i.large black woolen coat`/, 
`i.small red cardboard box`/, `i.big blue wooden sign`/ 
(but not `i.*blue big wooden sign`/, `i.*wooden big blue sign`/,
or `i.*blue wooden big sign`/).
The various patterns and tendencies noted are neither exceptionless nor even
close to being exhaustive of English nominal structure. But while the facts of noun
modification are quite complex, they also show a great deal of systematicity.
LangIntro p. 320.
`quote`

This systematicity is, arguably, imperfectly modeled within 
Dependency Grammar, insofar as the foundation of that style 
of analysis lies within individual word-pairs.  
It is true that a noun can pair off with 
numerous adjectives %-- 
see `i.genuine French culinary institute` 
or `i.those two lazy cats` (LangIntro p. 311) 
%-- but full analysis (granting Langacker's 
contentions here) calls for identifying 
rankings `i.between` those pairs.  The `q.ordering` 
of pairs is perhaps better conveyed by a 
phrase-oriented (constituency) model, 
where we can see the adjectival constructions 
as nested inside one another.  So for instance 
`i.my three broken chairs` has the possessive 
as an `q.outermost structural layer` (LangIntro p. 275), 
with `i.broken` a designation of quality and 
`i.three`/, in between, a designation of quantity.  
Each modifier supplies a different sort of detail, 
yielding a progressively more complete phrase.  
However, analyzing the construction in phrase-hierarchical 
terms still does not entirely model the issue of 
this `q.accretion of detail` needing a specific 
`i.sequence`/: why are the phrases 
nested in that order conventionally 
(quality, then quantity, then possessive/grounding) 
when other nestings %-- which would ultimately 
aggregate to the same combination of specifiers 
%-- are dubious or wrong (`i.broken my three 
chairs`/, `i.three my broken chairs`/, 
`i.my broken three chairs` %-- although this last is at 
least plausible)?  
`p`

`p.
Another area where both Dependency and Constituency grammars 
feel strained as representational vehicles concerns 
ambiguities in how we hear sentence elements as 
grouped together.  Langacker frequently discusses 
constituency as a noticeable but partial 
pattern, so that sentences do not necessarily 
have a fixed construction hierarchy: 

`quote,
There is no consensus about the internal constituency of nominals, due in part to the matter
being quite complex.  
But let me suggest a more basic reason: that there `textbf.is` no definite
constituency.  As viewed in CG [Cognitive Grammar], 
constituency is neither essential nor fundamental to
grammar.  While certain hierarchical arrangements are fixed and well 
established, constituency groupings are often flexible and variable, if not just 
indeterminate. 
We have no reason to think that the structures constituting a symbolic assembly
all have to be arranged in strictly hierarchical fashion, nor does any single hierarchy
capture all aspects of grammatical organization.
From the CG standpoint, questions of constituency have to be addressed as part
of a broader consideration of symbolic assemblies. A symbolic assembly consists of
semantic structures, phonological structures, and symbolic links between the two.
LangIntro pp323-4
`quote`

`quote,
CG ... eschews the 
standard assumption of a single, fixed constituency. Rather than being fundamental, 
constituents emerge within symbolic assemblies as a special case (albeit a typical one)
of the configurations their elements can assume.  
It is only to be expected that the same symbolic components might sometimes be grouped in alternate ways, without
significantly affecting the ultimate composite structure. 
It may also happen that grammatical constituents do not emerge at all. 
Given three component elements, like `i.small`/,
`i.table`/, and `i.near the door`/, 
nothing prevents them from combining at a single level of
organization, with no internal grouping: `i.( (small) (table) (near the door) )`/. And should
grouping occur at one pole, there is no necessity that it be concordant with grouping
at the other. 
...
In the absence of clear-cut evidence, the proper analysis may simply be
indeterminate %-- certainly for the analyst, and very possibly even for speakers.
p. 325
`quote`
`p`

`p.
Langacker argues that constituency patterns depend on how tightly 
coupled two or more words would be to form multi-word units.  
Thus `i.put through` (as in a phone call) serves as a 
compound verb, whereas `i.ran through` is more likely a 
motion verb along with a path description (his 
example 49, p. 404):

`sentenceList,
`sentenceItem; He ran through the mall.
`sentenceItem; He put through the call.
`sentenceItem; Through the mall he ran.
`sentenceItem; The call was put through.
`sentenceItem; He put the call through.
`sentenceList`

Langacker argues that `i.Through the call he put`/, 
`i.The mall was run through`/, and 
`i.He ran the mall through` are erroneous.  
This suggests that `i.through` binds `q.tightly` 
to `i.put` but only loosely to `i.run`/.`footnote.
Interestingly, we can give the example of 
`i.run through` as a compound verb, in the sense of `q.mentally 
review`/, which behaves like `i.put through` instead of 
like the `q.physical` run-through: `i.through the options he 
ran in his mind` is dubious at best).  
`footnote`

But numerous factors influence whether and to what 
degree we hear words as `q.coupled`/.  Even if 
phrases `i.logically` gather up into a hierarchy, 
in terms of the situation they describe, Langacker 
suggests that we may perceive them in a more linear 
fashion and only retrospectively build up 
the logical picture.  He cites the example 
`i.Alice said that Bill believes 
that Cindy claims that Doris swallowed a spider`/: 

`quote,
The constituency [here] is seldom seriously questioned. But perhaps it
ought to be.  One factor long recognized as problematic is that the nesting ascribed
to such expressions is at odds with their phonological realization. 
Intonation suggests [a] nonnested structure ... each clause is a separate intonational unit
bounded by a slight pause from the one that follows. 
Furthermore, a primary reason for adopting a layered structure is the tacit assumption that basic grammatical relationships have to be 
reflected in constituency.  In CG this assumption is seen as being
gratuitous. Grammatical relationships have a conceptual basis and can be
captured by correspondences irrespective of the order of grammatical combination.
A viable description is therefore possible adopting an unlayered structure... .
LangIntro p. 417
`quote`
`p`

`p.
Although these points are made in the context of 
`i.constituency`/, they obviously influence our sense of 
words' pairwise interconnections also.  For example, 
in `i.small table near the door` do we perceive 
`i.near the door` as modifying `i.table` or 
`i.small table`/?  That is, do we mentally 
adjoin `i.near the door` to `i.small` as a package 
of specifiers explicating how we perceive the `i.table`/?  
Or do we adjoin `i.near the door` with `i.table` because 
that is the noun which the phrase modifies?  
`p`


`p.
Along similar lines, Langacker uses several examples to 
contrast subordinate clauses and non-clausal phrases, 
such as (examples 12-13, pp. 415-6):

`sentenceList,
`sentenceItem; They began arguing before they even sat down.
`sentenceItem; They began arguing before dinner.
`sentenceList`

Langacker's theme here is distinguishing `i.subordination` 
from `i.coordination` between clauses: 
`q.in contrast to the spirit of coordination, the
relation between the clauses is inherently asymmetrical ... 
Underscoring their asymmetry is
the possibility of replacing the clause in question with a nonclausal structure` 
(LangIntro p. 415).  He goes on: 

`quote,
With this approach, we are able to maintain a clear and precisely defined 
distinction between coordination and subordination. But is the distinction really sharp?
We have already seen that coordinate structures exhibit various kinds and degrees of
asymmetry. Should we not also expect the converse, that certain subordinate struc-
tures might tend toward symmetry? Instead of a strict dichotomy, we might well
anticipate a fuzzy boundary with transitional cases. 
`quote` 

I read this as questioning whether we hear `i.before they even sat 
down`/, say, as a subordinate detail `i.supplementing` the 
main idea of the sentence (their arguing), or as a co-ordinate 
specification which for the speaker has almost equal weight, 
her mental focus simultaneously on their arguing and on 
the remark-worthy length of time that such has been going 
on (as if to say `i.They're arguing, and they've been doing 
so since before dinner`/). 
`p`

`p.
But I think this question in turn depends on how tightly we 
hear `i.before dinner` or `i.before they even sat down` coupled to 
the antecedent phrase and to its parts.  In (), 
`i.before dinner` can be treated as a further detail on 
`i.started`/: it establishes a temporal time frame 
augmenting how `i.start` profiles the beginning-point of 
some process.  Or we can read `i.before dinner` as a supplement 
to `i.started arguing`/: we have the verb-phrase `i.start arguing` 
indicating that the speaker is presenting information about 
that specific phenomenon (the time at which their arguing 
began), and `i.before dinner` is used to convey a time 
scale (if they `i.started` arguing a relatively long time 
ago, they have `i.been` arguing for a long time).  
Moreover, we can potentially read `i.before they even sat down` 
as a refinement of the whole finite clause 
`i.They started arguing`/: the sentence profiles the 
fact that they are arguing, and then clarifies that 
this situation has been going on since before they sat down.  
The latter reading %-- that `i.before they sat down` is a 
separately conceivable idea (since `i.they sat down` and `i.They started 
arguing` are both finite clauses) that adds a temporal 
framing to the overall idea of their arguing %-- seems 
to invite the model wherein the latter clause is co-ordinating 
and not subordinate. 
`p`

`p.
But note that these varying summaries of the effective 
linguistic structure here depend on where we 
`q.attach` the second clause (`i.before they sat down` or 
`i.before dinner`/) to a word or phrase in the 
antecedent part: does the second clause modify 
`i.started`/, or `i.started arguing`/, or the whole 
clause `i.they started arguing`/?  Is there some  
fixed sense that we can distinguish which model of 
the word-to-word, word-to-phrase, or phrase-to-phrases 
linkage is `q.correct`/?  Indeed, our conception 
seems more holistic: surely we understand 
that `i.before dinner` is a temporalizing detail 
relevant to, and epistemically qualifying, 
`q.started`/ `i.and` `q.started arguing`/, 
`i.and` `q.they started arguing`/.  So perhaps 
trying to select one parse or another is 
conceptually misguided: our conceptualization of 
the sentence does not `i.work` in such a way that 
such distinction is meaningful.  However, 
we say this only from a retroactive awareness 
of the sentence's meaning.  This does not 
preclude a more specific structuration at 
some subconscious level that we would 
not necessarily find well-motivated 
based on our conscious sense of the sentence's 
meaning, precisely because we cannot 
undo our knowledge of what the sentence means 
in its totality, and retrace with consious 
awareness how we `i.arrive` at that meaning.  
`p`

`p.
In effect, it is possible that subtle variations 
in how we see a sentence as structurally 
unified %-- what network of conceptual 
and `q.reception` couplings (in the sense of our 
initial reception of a language-artifact) 
connects one part of a sentence to other parts, so 
that we can trace how a word, phrase, or clause 
`i.directly` modifies some other word or phrase, 
as compared to indirect modification through some 
intermediary.  Sentences are obviously a 
`i.network` where we find some elements 
`q.connected` to some other elements as modifiers 
to modifieds, where `q.modifying` involves some 
conceptual re-interpretation or supplementation of 
detail.  Since sentences have a holistic meaning, 
every part of a sentence has some determinate 
influence on every other part; but some modifications 
happen explicitly through syntactic and semantic 
norms.  Therefore we can see sentences as a network 
where elements are `q.nearer` or `q.further` from 
one another, where the nearest elements are those 
that expressly modify one another.
`p`

`p.
This implies that graph-like, structural re-presentations 
of sentences are useful glosses on their network 
structure.  However, we still have issues of 
which modifications are seen as more or less immediate, 
and so which elements are more or less tightly coupled.  
Is the more immediate, conceptually salient, 
mentally noted modification-pattern the effect 
of `i.before they even sat down` on `i.started` 
(providing a temporal anchor to `i.start`/'s notion of 
initiation), `i.started arguing` 
(as a temporalizing container on the profiled composite 
process, the act of starting to argue), or 
`i.They started arguing` (as a complete idea to which 
the second clause adds temporal detail)?  Even if we 
believe each of these patterns of conceptual alteration are 
equally salient %-- or at least that there is no 
credible theory `q.ranking` one parse over the other 
%-- we can still say that these three options are 
three distinct views of the sentence's `q.network` 
which all have some relevance to how the sentence 
works.  The temporalizing clause has some 
conceptual relevance for modifying and elaborating 
on the prior material at different levels 
(`i.started`/, `i.started arguing`/, 
`i.They started arguing`/).  Each of these 
modifications can be observed by sketching 
the sentence's `q.network`/.  If it is true 
that each level of conceptual influence is 
conceptually relevant, then perhaps we need 
to sketch the network in three different 
ways.  Perhaps a thorough analysis would 
embrace the structural partiality of any 
one network model but use multiple such 
models as a `q.holistic` picture of the 
sentence's conceptual gestalt.
`p`

`p.
Along these lines, we have some license to 
pursue formal Dependency and/or Constituency 
reconstructions of sentences, even if 
the choise of one precise structural model 
may be somewhat arbitrary.  A sentence may 
tolerate several different parses and 
reconstructions, which all reveal some layer 
or detail of its conceptual patterning 
(this is separate and apart from ambiguity; 
I refer here only to the conceptual holism which 
makes it difficult to argue that the smaller 
parts of a sentence are linked or pieced together 
in some single manner).  Each notated Dependency 
Graph or Constituency Tree may be a valid but 
incomplete summary of `i.some aspect of` a 
sentences' inner working.  A full picture may 
emerge from the superposition of multiple 
such formal reconstructions.
`p`

`p.
Where does this leave us with regard to formal methodology?  
Parse graphs, trees, or any other sentence-reconstruction 
is a useful `i.tool` for `i.investigating` sentences, 
but we should not confuse any one formal representation 
with the `q.meaning` of a sentence, or deem the 
process of arriving at a formal re-construction to be 
akin to `i.understanding` the sentence.  Such an 
assessment bears on the issue of `q.computational` 
linguistics, and the philosophical question of 
whether computers can `q.understand` language 
%-- and whether their automatically creating a 
structured model, like a parse-graph, 
serves as proof of something like 
linguistic competence, or perhaps a 
preliminary step `i.toward` linguistic competence. 
`p`

`sectline;
`p.
When linguistic methodology is described as 
`i.computational`/, this usually implies that 
the research is not only developing or employing 
some formal representation of linguistic 
elements, but is moreover using some kind 
of computational mechanism to instantiate 
and/or analyze the relevant formal structures, 
concretely filled in with particular 
linguistic content.  In the case of 
constituency trees or dependency graphs, 
for example, there may be a software 
package which implements the trees or 
graphs as digital data structures, 
manifested by assigning words (or other lexemes) 
to the requisite nodes (or `q.leaves`/).  
In that digital form, the data structures may 
then be available for further manipulation, 
such as being visually presented in some 
diagrammatic manner or being paired with 
other data structures, such as tables itemizing 
the lexical interpretation and/or grammatic 
categorization of the component lexemes.
`p`

`p.
The term `i.computational linguistics` probably also 
implies that these digital representations 
are %-- or will potentially be %-- `i.automatically` 
generated by some Natural Language Processing 
system.  For most researchers, that is, the use of 
computers as linguistic tools is bound up with a 
goal to automate various dimensions of linguistic 
processing, which in turn could provide for Artificial 
Intelligent agents that mimic human linguistic 
competence.  Machine translation, for instance, 
could be effectuated by parsing sentences to 
dependency graphs, transforming the resulting graphs 
by translating words between languages, and 
synthesizing sentences in the target language 
via the output graphs.  The machine-translation `q.problem` 
is thereby reduced to three smaller problems whose 
`q.solutions` can be chained together %-- starting 
with the problem of mapping linguistic content to 
formal re-constructions.  
`p`

`p.
For linguists, of course, automating the annotation of 
linguistic components is more convenient than 
doing the same work manually.  In the context of 
Natural Language Processing, automation is more 
than just a convenience: here, theoretical 
paradigms and concrete models are assessed 
specifically on how well they engender 
computational artifacts which manipulate 
linguistic givens in human-like ways without 
human intervention.  A module which transforms 
unadorned sentences into formal dependency 
graphs, say, would be deemed successful if 
most such parses are identical to those 
produced by human annotators.  To be sure, 
the more that reconstruction of linguistic 
content between different surface-level or 
formal representations can be automated, 
the more that Artificially Intelligent 
language engines can be constructed merely 
by interconnecting these automated capabilities.  
However, we should not assume that formal 
representations are `i.only` valuable in 
the context of these automated systems.  
`p`

`p.
Instead, it is possible to employ 
formal or, indeed, computational structures 
as vehicles for linguistic explanation or 
exploration, whether or not the structures 
can be automatically obtained from 
surface-level language (or from other formalisms).  
As a case in point, I will here propose a 
formal model for Cognitive Grammar which, 
in broad outline, takes its structure from 
recent investigations merging Conceptual 
Space Theory, in the sense of Peter `Gardenfors;, 
with a notion of `q.hypergraph` grammar.  
This combination %-- proposed in a schematic 
manner particularly in `cite<BobCocke>;, which 
apparently reflects the experiences of 
several seminars conducted by a group of 
linguists and mathematicians %-- outlines 
an overall linguistic model where hypergraph 
structures are featured in grammatic description 
and Conceptual Spaces are recognized as a 
foundation for semantics.  The proposal, 
which I will examine in more detail below, 
is sufficiently schematic that it may be 
best treated as a metatheoretical template 
%-- open to elaboration both in terms 
of how hypergraph grammars are specified and 
in terms of how Conceptual Space theory 
is understood to anchor semantics 
(here I will consider both dimensions in the 
context of Langacker's Cognitive Grammar, 
e.g. comparing Langacker's notion of 
`i.domains` to that of `Gardenfors;). 
`p`  

`p.
Assuming, in any case, that we have a specific 
`q.Conceptual Hypergraph` model (unifying 
an elaborated Hypegraph Grammar theory with a 
Conceptual Space semantics), this model can be 
considered a `i.computational` model insofar 
as the resulting hypergraph-based data structures 
are amenable to computational treatments: 
for example, being implemented as data structures 
in a programming language, and subject to manipulation 
via these computational representations.  
Indeed, I will argue that a variation of 
the Conceptual Hypergraph model can be adopted as a 
basis for `i.formal` languages, such as 
computer programming languages.  Accompanying this 
essay is demonstrative implementation of a 
programming language which illustrates this explicitly, 
using a `q.Hypergraph Virtual Machine` and 
hypergraph-based Intermediate Representation to 
concretize the compiler and runtime algorithms needed 
to translate source code (conformant to a hypergraph 
grammar) into machine-readable instructions.  
In this sense, I claim that a Conceptual Hypergraph model 
of language in general is interesting in computational 
contexts in part because it has applications to 
both natural and artificial languages. 
`p`

`p.
Nevertheless, I do not claim here that the Conceptual Hypergraph 
model is well-suited to computational `i.automation`/.  
Although I will describe a formal representation for linguistic 
data within this model, I am not concerned with whether 
computers could be programmed to derive these representations 
automatically, in contrast to the process of 
humans manually assembling the hypergraph structures as 
explanatory models for given sentences.  While I think 
Conceptual Hypergraph models are subject to many 
interesting computational treatments (including 
via compiler/runtime pipelines), I do not endorse 
any philosophical speculation that natural language 
is sufficiently formalizable to be emulated 
by machines (even given practically infinite computing 
power and `q.training data`/).
`p`

`p.
Let me, in particular, make a contrast between 
`i.explanatory` formal models as opposed to 
formalisms aspiring to `i.simulate` human linguistic 
performance.  In the former sense, models are used 
by humans to better understand language as a 
structural and/or cognitive phenomenon.  
The compositional requirements and transformative 
options available to formal models can then 
serve as proxies or intuitions for the rules 
and transformations internal to language.  
In this explanatory role, formal models do not need 
to be precise reconstructions of linguistic behavior, 
because we do not need them to serve as our sole 
source of linguistic explanation: formal models are 
simply one vehicle among others for 
investigating linguistic phenomena and communicating 
linguistic theories.  By contrast, when we 
reify formal models to the point of `i.subsuming` 
all linguistic phenomena %-- in the sense that 
models should engender `AI; engines that replicate 
human linguistic competence %-- we impose an 
unreasonably high burden.  Formal (and computational) 
models can be theoretically useful even if they 
are not totalizing enough to reduce all language 
to computationally tractable algorithms.
`p`

`p.
In this spirit I will argue, then, that 
a Conceptual Hypegraph model can be used to 
bridge Cognitive Grammar to formal linguistic 
methodologies %-- not in the sense that 
Hypergraph Grammar and Conceptual Space theory 
jointly embody a complete formal specification 
for language, nor that the model allows us to 
equip `AI; with something like `q.cognition`/, 
but rather that a certain class of 
Hypergraph Grammar and Conceptual Space constructions 
can be simultaneously illustrative specifications 
of Cognitive Grammar constructions and 
also elements of a system with formal properties, 
subject to formal presentations and analysis.  
`p`

`p.
I believe that language is neither completely 
formal nor completely informal; as a result, 
formal models are neither entirely irrelevant 
nor holistically adequate for linguistic explanation.  
It is true that language is a human (communal) 
activity, and surely acquires some of its structure 
from this interpersonal dimension, just as 
social norms or cultural value systems reflect 
certain regularities that can be structurally 
analyzed.  However, structural explanation has a 
deeper warrant in the context of linguistics than 
in, say, sociology or anthropology.  
There are constructive patterns that are directly 
manifest in well-formed sentences, and which 
are in that guise amenable to structural analysis; 
this differs from Structuralism in mythology, for 
instance, where interpretive sophistication is needed 
to reveal patterns that imply compositional `q.laws` 
somehow operative in myth-making.  Even language 
with no formal rule-books, no instructional pedagogy, 
or no written form, evince precise grammatic 
and morphological systems %-- equally or more complex 
than cosmopolitan languages regulated by institutions 
like the `AcademieFrancaise;.      
`p`

`p.
In short, it is possible to find within language precisely 
specifiable patterns that are not imposed by 
any external authority, nor vague enough to be 
biproducts of a certain manifest structurality that 
we might find in many regions of human communal activity.  
This implies that there is something within meaning or 
meaningfulness %-- or our cognition thereof %-- which 
trends toward structural regularity.  It would certainly 
be part of the linguistic purview to explain how this 
trend arises (organically from the mental and intersubjective 
processes which yield language) and to document 
structural regularities at work in specific languages.  
On the other hand, however, such an analysis has no bearing on 
whether the analysis of structural patterns supplies a 
comprehensive method for analyzing language as a whole, 
or how extensively signification relies on structurally 
enumerable patterns rather than gestures, contextual 
cues, communicative conventions among narrowly defined 
dialect communities, and so forth.  To analyze the 
speech of several friends discussing a baseball game, 
it may be that only a relatively small percentage of 
the communicative principles manifest in their activity 
can be explained by structural semantic or syntactic 
models; such models should then be deemed 
necessary but not sufficient for comprehensive 
treatments of language.
`p`

`p.
Structural patterns may emerge from how we cognize 
and then share impressions of empirical situations; 
or in the internal logic of states of affairs that 
are worthy of or conducive to linguistic expression; 
or some combination.  It is reasonable to assume 
that most (or all) linguistic statements have some 
basis in fact %-- that they are associated with 
some `i.propositional content` which the speaker either 
asserts or somehow comments on.  The point is not 
that every sentence has a meaning which can be directly 
reproduced propositionally, but that most or all sentences' 
meanings take their departure from a propositional 
content that the speaker profiles in some orientation 
%-- as asserted, desired, interpreted, explained, 
doubted, and so forth.  Often the communicative burden 
lies not in merely designating the concomitant 
proposition but in expressing a meta-level attitude 
to that content, so that the speech-act signifies and 
then elaborates upon it:

`sentenceList,
`sentenceItem; You know that Warren leads Biden in the polls, don't you?
`sentenceItem; I hope that Warren can appeal to moderates.
`sentenceItem; I'm sure that Warren will pivot to the center and 
will poll well with moderates.
`sentenceList`

The proposition that `i.Warren will poll well with moderates` 
certainly denotes a testable premise %-- it will prove true or 
false as the polls are in fact taken over time %-- but 
the point of (3) is not only to assert that claim, but to 
add interpretive and causative detail: (3) implies 
that Warren will poll well `i.because` she 
`q.pivots to the center`/.  So we cannot `q.reduce` the 
meaning of (3) to the propositional content glossed 
as `i.Warren will poll well with moderates`/; but 
denoting that content certainly seems to be 
a prerequisite to defining (3)'s actual meaning. 
`p`

`p.
Of course, (2) and (3) can be seen as packaging 
one sort of propositional content inside a second, where 
the `q.outer` content reflects states of affairs pertaining 
to propositional attitudes.  Notice that (2) and (3) are 
falsifiable.  Perhaps (2) is spoken by a critic of 
Warren, who is rooting `i.against` her; but (2) is an 
indirect or diplomatic way of calling attention to her problems 
appealing to moderates (to hope for something implies some 
doubt whether that will in fact transpire).  Conversely, 
(3) might be said by a Warren supporter trying to alleviate 
the concerns of (2).  People can consciously 
falsifiable reports of their propositional attitudes, 
for various rhetorical or manneristic reasons.  
But this implies that propositional attitudes are 
themselves falsifiable contents, so it can be 
true or false that someone hopes for or believes 
something.  As a result, we can argue that the 
actual propositional content invoked by some sentences 
can be factored into an `q.extramental` objective part 
and a speaker-relative attitudinal report.  
`p`

`p.
Speech-acts, of course, also signal speakers' desires 
to `i.make` something the case, including via commands 
and requests.  In the above examples, (1) is 
apparently performative in a similar way, since 
(1) implies that the speaker `q.wants` the addressee 
to accept the designated claim as true.  The 
insinuation is that `i.Warren outpolls Biden` 
(as an asserted fact) `i.and moreover` that 
this fact should by collectively agreed upon, 
among the inventory of shared posits that 
undergird any conversation.  So (1), via some 
indirection, expresses a kind of second-order 
`q.attitudinal` content analogous to (2) and 
(3) (the speaker's belief that `i.Warren outpolls Biden` 
is sufficiently extablished as fact to be 
collectively `q.enregistered`/), while (1) also 
implicates a request (that the other 
conversants consent to this epistemic entrenchment).    
`p`

`p.
In effect, language would be impossible if we could not 
express both propositional contents and attitudes 
to those contents (wherein propositional attitudes 
are in turn a form of propositional content).  
In this sense language will reveal some 
structural patterns merely insofar as language's 
signifying resources include, as a proper part, 
exposition of factual beliefs and assertions.  
Here I am not implying that patterns in 
propositional structure are directly translated 
to patterns in language %-- by way of illustration, 
the usages in (3) are, on reflection, highly metaphorical 
and stylized.  The conventionalized trope of 
a politician `q.pivoting to the center` employs a 
spatial construal to reference an empirical state 
of affairs which, logically reconstructed, 
would be very complex and contextual.  
While we can provide a workable predicate 
gloss %-- someone `q.pivots to the center` if 
their political comments or speeches reveal a 
particular pattern `visavis; the projection of 
distinct political topics onto a common centrist/extremist 
scale, such that someone's posturing over time figures 
summarily as `q.movement` in this abstract space 
%-- the semantics of the idiom `i.pivot to the center` 
is not a facile logical construction like 
`i.bachelor` and `i.unmarried man`/, or `i.ring` and 
`i.jewelry worn on one's finger`/.   
`p`

`p.
Merely noting the predicate structure of signified 
propositional contents, that is, may be at best 
tangential to actual linguistic explanation, because 
it cannot account for how metaphorical cases 
like `q.pivot to the center` are cognized and 
conventionalized.  However, the requirement that 
language must signify states of affairs 
%-- via metaphor or conceptual schema or something 
more transparently compositional %-- is a reasonable 
starting point for exploring where language's structural 
sophistication comes from, because it certainly 
seems that propositional contents have structural 
patterns, so that these must eventually fall out 
of linguistic processing.  Of course, we may also 
argue that linguistic structure derives from how 
we conceptualize states of affairs %-- and encode 
these conceptualizations in language %-- 
as much as from the structural logic of states of 
affairs themselves.    
`p`

`p.
As explanatory intermediaries, both cognitive and 
propositional structure can coexist as 
originations for formalisms manifest in language.  
We need not regard analysis of cognitive frames 
exerting a morphosyntactic influence on the shape 
of aggregate linguistic units as denying a parallel 
influence from the situational context with which 
discource must, to some measure, propositionally 
align; or vice-versa.  And yet there do seem 
to be paradigm-grounded pressures to situate 
language morphology (in the informal sense of 
higher-level linguistic `q.shape`/) `i.either` in 
expressed propositions `i.or` in cognitive construals.  
To some degree this may reflect metatheoretic 
divergence in the basic terms of discussion: 
is the `i.meaning` of (4) here to be the 
proposition that Warren will win, or the speaker's 
belief that Warren will win?

`sentenceList.
`sentenceItem; Warren will probably win the nomination.
`sentenceItem; Biden's support among the base is eroding.
`sentenceList`

Likewise, should we understand the more metaphorical 
phraseology in (5) as a conventionalized idiom that 
has become a kind of entrenched signifier, no longer 
really operating metaphorically; or rather as  
projections of metaphorical devices which the speaker 
uses conceptually to think through the situation, 
anterior to her opinions being verbalized?     
`p`

`p.
Taking a broad view, language involves a (typically iterative) 
process where a speaker has some thought, encodes it via 
language, yielding a spoken or written aggregate which 
an addressee encounters, processes, and then (hopefully) 
understands.  Most of this larger process occurs within 
the minds of the conversants participating; but 
it could reasonably argued that the specifically 
`i.linguistic` (and not psychological or sociocultural) 
concern here is focused on the observable properties of 
the spoken or written content produced.  The mental 
provenance and reception surrounding that content may be 
very real, but (or so one might say) cognitive 
analysis only has bearing on the specific 
disciplinary focus of `i.linguistics` insofar as 
the objective language artifact bears the 
imprint of certain cognitive processes, which are 
irreducible in a comprehensive account of why 
the artifact exists.  To make an analogy, 
suppose a footballer executes a pass in accorded 
with a specific tactic which his team rehearses 
and discusses in practice.  There are surely many 
cerebral and physiological factors contributing to 
the player making that pass, but a football expert 
would be expected to focus on the pass itself 
%-- its tactical rationale and execution technique.  
In this analogy, actual linguistic content 
%-- sentences, say %-- are like the football in a football 
match; they are tangibly observable.  
The ball is played over the course of a match, and its 
spatial position can be described %-- as can player-specific 
factors affecting the ball's location, such as how they 
execute on-ball maneuvers, and their decision-making 
insofar as it fits within the overall scheme of 
football's rules and tactics.  But the analytic 
attention should rest with the ball itself, and 
only expand to include players' thought processes 
insofar as these bring explanatory value to 
observations of the game's objective features.
`p`

`p.
Analogously, language is, objectively, a realm of spoken and 
written productions or perhaps a system for creating new 
ones.  Consequently, someone can reasonably suggest that 
cognitive analyses explaining how a speaker 
came to enunciate `i.this particular` sentence may be 
correct, but also are not in and of themselves 
descriptions of speakers' meanings.  Reprising earlier 
examples, someone might say that political discourse 
(being fairly important and repetitive in a modern 
democracy) has evolved certain entrenched idioms, so 
we speak of `i.pivoting to the center` or `i.eroding support` 
as a shorthand for characteristic, objectively definable 
situations in the realm of politics.  The entrenched 
expressions and their objective content %-- however 
metaphoric the latter may seem, and abstract/contextual 
the former %-- stand in a signifier/signified relation not 
that different from a more mundane/concrete semiosis 
like `i.cat` and one class of animals, or `i.water` 
and `HTwoO;.  We are, in other words, prepared to 
accept that `q.cat` is a word which in some sense 
`i.means` the clade Felidae, and `q.water` means 
`HTwoO;.  We do not take these `q.meanings` to be 
cognitive phenomena, even if we ackowledge that 
cognition is a kind of metaphysical prerequisite 
%-- no word would mean anything if not for 
each language-user having their own mental inventory 
of correct words for different contexts.  
Yet what makes these inventory items `i.words` is 
their stable co-existence in the minds of 
many (or all) speakers of a language.
`p`

`p.
The whole point of agreeing that `i.cat`/, say, 
is a word in English is to recognize a 
substratum in `i.cat` which is at a theoretic 
level non-cognitive even if it `i.is` cognitive 
as a matter of ontological foundation.  
As a `i.word`/, `i.cat` has a theoretical 
status which abstracts from its cognitive manifestation 
in any one English speaker's mind, such that it 
can be defined `q.extramentally` in terms of the 
pairing between the word and the set of mammals which 
it conventionally designates.  To be properly 
linguistic, it might seem, our understanding of 
lexical and syntactic phenomena must perhaps 
acknowledge the cognitive nature of linguistic 
conventions as a matter of physical empiricism 
%-- words don't `q.exist` in some ethereal 
netherworld %-- but then transcend this cognitive 
layer by analyzing language enough that the synergy 
among different speakers' linguistic predilections, 
rather than any intramental immanence, becomes 
the theoretical center-stage.  Linguistics 
proper therefore arises insofar as the 
cognitive dimension %-- siting linguistic 
rules and judgments in people's minds simply 
as a matter of ontological parsimony %-- 
becomes in a sense canceled out by multi-person 
alignments (which are preconditions for 
truly linguistic phenomena).  Linguistic extra-mentality is 
then akin to how political scientists take 
demographic and electoral trends as their scientific 
givens, even though elections only `q.exist` through the 
voting decisions of concrete individuals. 
`p` 

`p.
I will argue in the next section that Cognitive Linguists 
perhaps discount the pervasiveness or rationale behind 
these extra-mental intuitions, and therefore have not 
fully responded to (or potentially bridged to) 
formal methodology which, in turn, are inclined 
to underestimate the cognitive contributions to meaning.  
My own opinion, already intimated, is that language 
actually unifies formal and contextual dimensions, so 
that methodologies which foreground cognitive construal 
and those which prioritize formal substrata are 
both responding to real `q.signals` in linguistic data, 
and therefore both have merit.  However, I also 
believe that Cognitive and (say) Computational linguists 
have not-entirely-aligned intuitions of the basic demarcation 
of the linguistic discipline itself, which renders a 
certain paradigmatic disconnect almost inevitable.  
`p`

`p.
To see this, consider again the minimal example 
wherein, say, English speakers use the phoneme 
`i.cat` to express an idea which (to some approximation) 
matches the biologists' `i.Felidae`/.  I think everyone 
would agree that a certain cognitive construal is thereby 
conventionalized and internalized to the point where, 
from each individual person's point of view, it becomes 
a `i.de facto` fact of the world: everyone knows that 
everybody else expects us to say `i.cat` when we intend 
to talk about Felids, so %-- however much the association 
may lie in people's minds %-- we all accept the useful 
fiction that `i.cat` means `i.Felid` as a matter of 
objective fact; just as Obama's or Trump's victories were  
objective facts notwithstanding `q.victory` meaning 
many individual persons' decisions to vote for them 
(and even more people's decision to recognize the election 
as legitimate, notwithstanding their own vote).  
Against this background, we can say both that 
linguistics concerns cognitive inclinations that 
are synchronized between people to the point 
of conventional entrenchment; `i.and` that 
linguistics concerns regularities conventionalized 
to the point where cognitive processes can be 
excluded from the fields disciplinary scope 
(and when they can't %-- e.g. in the analysis 
of rhetorical persuasion or of literary artistry 
%-- they tap into aspects of language which linguistics 
is not itself about).  But this leaves us with 
two different conceptions of the linguistic enterprise, 
differences which can be seen most clearly not on 
the methodological level but in the `q.philosophical` 
process of identifying the specific analyzands 
which are in fact the substance of linguistic science. 
`p`

`p.
Moreover, I think this divergence is clearest in the 
context of propositional content.  As I argued earlier, 
most or all sentences are specifically aligned 
with (and project propositional attitudes toward) 
specific propositions; such that the meaning of 
a sentence is dependent on (even when it does not 
coincide with) a proposition which the sentence 
signifies.  This means that language is a system 
which can designate propositions in often 
unambiguous ways %-- where the connection between 
the concrete utterance and the relevant predicate 
complex has essentially the status of 
objective (albeit social) fact (`q.objective` in 
the emergent sense akin to the objective fact of 
an electoral victory, say).  Moreover, this 
connection is largely a matter of agreement and consent, 
without any special interpretation or ad-hoc 
protocols enacted between speakers.  Suppose 
someone overseas desires a baguette from a bakery and, 
not speaking the local language, points to her bread 
of choice; the shopkeeper responds by raising two fingers; 
she responds by giving him two Euros, and he gives her 
the baguette.  Hence their activities have been coordinated, 
but according to relatively unstructured gestures conceived 
on the spot (except insofar as some gestures, like pointing 
or the use of fingers to represent number, have an 
almost-linguistic determinacy).  Conversely, 
if the two participants both speak English, we might 
instead have 

`sentenceList,
`ex; May I have a baguette?
`ex; Two Euros, please.
`sentenceList`

We can reasonably assume that the shopper in (1) expresses her 
desire to `i.purchase` bread (she doesn't expect the baker to 
just hand her the bread as charity, or for her to examine).  
Likewise we can assume that the baker indicates in (2) that the 
bread costs two Euros, so that in the proper discharge of 
his duty he will give her the bread once she gives him the Euros.  
We can clearly identify the relevant propositional contents: 

`sentenceList,
`ex; I would like to purchace a baguette.
`ex; If you give you two Euros, I will give you a baguette
and recognize it as yours.
`sentenceList` 

But fully explicating the logical details can seem awkward, 
even impolite.  In reality, we almost always use language 
to designate propositional content which is not precisely 
modeled by the language itself, and therefore 
arises by virtue of conventions and implicatures.  
Language possesses a system of transformations such 
that linguistic structures map to propositional 
contents, even if the former structure is morphologically 
divegent from the structure of the relevant propositions.
`p`

`p.
In effect, language designates propositional content, but 
does so in accord with its own rules and conventions, 
which are not the same as transparent logical articulations 
of predicate structure.  While the `i.processing` which puts 
these rules in effect is mental, their being linguistic 
expressly means that the rules have a commonality which 
transcends any person's (or even any conversation's) 
cognitive dispositions.  We would probably agree  
(barring some extraordinary contextual detail) that
(1) `i.necessarily` signifies the shopper's desire 
to buy a baguette, even if she or the baker (or both) 
are inclined to impose some idiosyncratic interpretation.  
We can of course imagine competing scenarios %-- perhaps the 
baker intends (with her prior knowledge) to give her the baguette 
for free, but she has to pretend an intention to buy the 
bread so as hide their collusion from other shoppers.  
In this case (1) might indeed be a special signal the colluders 
adopt, with an idiosyncratic meaning; but surely they 
are still aware of their departure from correct usage, and 
this departure is part of the metalinguistic rule of 
their convention: they agree to use the sentence 
`i.which on ordinary terms` describes the speaker's 
intention to buy bread as, between them, a coded message.  
Likewise, as some inside joke, two friends might use 
the word `i.cat` to reference `i.dogs`/; but here they 
are self-consciously investing `i.the word that means 
`q.cat`/` with some deviant meaning layered on top of 
its normal one.      
`p`

`p.
So `i.how` linguistic structure translates to propositional 
structure is something outside the control of individual 
people, and so it manifests itself, as a kind of scientific 
fiction, as objective fact.  A linguist (or logician 
or philosopher) is then on credible grounds in believing 
that the locus of scientific inquiry into language lies with 
how linguistic structures communicate propositional 
content even though the structures generated by 
syntax and semantics are not facile duplications of 
the structures of propositions, or predicate-complexes.  
Language appears to be an encoding of logic via 
structures without trivial logical rationales.  
Accordingly, scientists might well be interested in the 
`q.hidden logic` whereby the surface structure of 
language `q.compiles` to the deep structure of 
propositions.  The central focus of that analysis 
would then be propositional content as an extramental 
signification of sentences (as much as `i.cat` is 
an extramental signifier for `i.Felidae`/) 
and the set of rules which govern the reduction 
of linguistic structure to predicate structure.  
Language is fully explicated, on this account, 
when a list of rules is provided such that 
every alignment between a sentence 
(with its specific surface structure) 
and its propositional content (with a 
specific predicate structure) is accounted 
for, so we can have a theory for 
why `i.that` surface (linguistic) structure 
maps to `i.this` deep (predicate) structure.  
`p`

`p.
From this perspective, cognitive processing is not so 
much unimportant as tangential; the surface-to-deep, 
language-to-predicate transformation is certain 
a cognitive phenomenon, but `i.as linguistic` 
it only exists as a cerebral faculty of one person 
if it exists as a shared competence among all 
speakers of the language/dialect.  For `i.theoretical` 
elaboration, then, this `q.predicate reduction` is 
`i.not` cognitive, in the sense that its cognitive 
givenness should not factor in to its theoretical 
treatment.  Instead, language should be seen as a 
`q.predicate reduction` system %-- mapping 
surface linguistis structure to deep predicate 
structure, in the course of signifying propositional 
contents via linguistic media %-- which can be 
described in structural and logical terms, 
witholding appeals to the cognitive nature of 
the mental operations wherein the isolation of 
predicate formations occurs.  
`p`

`p.
I believe this framing of the linguistic enterprise is 
too simplistic, so I will rebut it somewhat in the 
next section, but I also believe that it 
should be assessed on its own terms.  The paradigm of 
`q.truth theoretic` semantics %-- which Langacker designates 
as ... %-- is most directly contrastible with 
Langacker's own Cognitive-Grammatic project when the 
differences are expressed as matters of scientific ontology 
and theoretical posits.  A ... semantics (or philosophy 
of language) can say that there exists objective 
fixations in how language (surface) structure maps to 
predicate (deep) structure %-- regulations which are 
extramental in the same way that propositions themselves 
are, and likewise linguistic artifacts (like spoken words).  
Linguistics per se is (so one might argue) the investigation 
of how, via a catalogue of structural transforms, 
languages encode propositions, via mechanisms which 
do not rely on cognitive faculties (like observations of 
similarity or interpretation of gestures) apart from 
those explicitly regulated by syntactic and semantic 
norms.  There may be alot of further analysis we can 
perform on the conceptual motivation behind 
`q.predicate reduction` rules, on what occurs mentally 
when people formulate and then interpret 
linguistic artifacts under the aegis of predicative 
communication, and on the role of cognitive construals 
in novel or inventive uses of language %-- but these 
are analyses of conceptual processing of linguistic 
structures, not analyses of these structures themselves.  
We cannot assume that the presence of linguistic 
structure in the mind, as something we think about and 
acquire competence for, renders those structures 
intrinsically mental.  By analogy, we can think about 
and become competent in abstract algebra or complex 
analysis; and symmetry groups or complex numbers can 
indeed be objects of thought, indeed objects 
toward which we apply mental exercises to the point that 
manipulating their structural domains becomes a kind 
of cerebral faculty and skill; but none of this makes 
the structures of group theory or complex numbers 
internally `i.cognitive`/.  They are, instead, 
abstract givens which become mental only insofar as 
they become cognitive foci whose thinking-about we 
rehearse to the threshold of a certain cerebral 
competence.  Arguably, syntax and semantics 
are in the same boat %-- they are objective 
and extramental systems, but because it is important 
for most people to master at least one language, 
we expend effort (albeit potentially unconsciously, when 
we learn languages as childen) to internalize these structures 
enough that we can form and understand sentences with little 
or no conscious effort.  That is, a lot of our human 
engagement with language is as something that stretches 
outside our mental reach but which we want to internalize 
as much as possible %-- we want to `i.learn` and 
`i.master` languages, so we are engaged with linguistic 
structures as objects of thought and competence, with 
our cognitive facility in manipulating language.  
However %-- or at least this is an intuition we 
have to respond to %-- study of our cognitive facility 
in manipulating language is a different disciplinary 
obligation than studying language itself.   
`p`

`p.

`p`

