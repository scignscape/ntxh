`section.Procedural Networks and Link Grammar`
`p.
My goal in this section is to incorporate Link Grammar into
a phenomenological and Cognitive Grammar perspective, more
than to offer a neutral exposition of Link Grammar theory.
Therefore I will accept some terminology and exposition
not necessarily originating from the initial
Link Grammar projects (though influenced by
subsequent research, e.g. computational models
developed by Matt Selway and Kenneth Holmqvist
`cite<MattSelway>;; `cite<HolmqvistDiss>;,
`cite<KennethHolmqvist>;).  I also
want to wed Link Grammar to my own semantic intuitions,
set forth earlier, that word-meanings and morphosyntactic
interpretations should be grounded on pre- or para-linguistic
cognitive `q.scripts` that are activated (but not
structurally replicated, the way that according
to truth-thoretic semantics linguistic form
evokes-by-simulating propositional form) by linguistic
phenomena.
`p`


`p.
Link Grammar is, depending on one's perspective, either related
to or a variant of Dependency Grammar (DG), `cite<DebusmannThesis>;,
`cite<Debusmann>;, `cite<DebusmannDuchierRossberg>;,
`cite<Nivre>;, `cite<HongEisner>;, `cite<AkbikBross>;, which in turn is
contrasted with `q.phrase structure` grammars, such as
Head-Driven Phrase Structure Grammar (HPSG)
`cite<KongRushSmith>;, `cite<KepserMonnich>;,
`cite<Schneider>;, `cite<XiaPalmer>; (for example).
Link and Dependency Grammars define syntactic structures in
terms of word-pairs; phrase structure may be implicit to
inter-word relations but is not explicitly modeled by
DG formalisms %-- there is typically no representation
of `q.noun phrass` or `q.verb phrases`/, for example.
Phrase structure is instead connoted via how relations
fit together %-- in `i.rescued dogs were fed`/, for instance,
the adjectival `i.rescued`/-`i.dogs` relation interacts
with the `i.dogs`/-`i.fed` (or `i.dogs`/-`i.were` plus
`i.were`/-`i.fed`/) predication, an interaction that in a
phrase-structure paradigm is analyzed as the noun-phrase
`i.rescued dogs` subsuming the noun `i.dogs`/.  Dependency
analyses often seem more faithful to real-world semantics
because, in practice, phrases do not `i.entirely` subsume
their constituent parts.  Linguistic structure is
actually multi-layered, where semantic and morphosyntactic
connections resonate between units within phrases separate and
apart from how linguistic structure is organized into
phrasal units themselves.
`p`

`p.
Except for phrases that coalesce into pseudo-lexemes
or proper names (like `q.United Nations` or `q.Member
pf Parliament`/), or indeed become shortened to single
words (like `q.waterfall`/), we perceive phrases both
as signifying units and as aggregate structures
whose detailed combinative rationale needs contectualization
and interpretation.  In short, phrases are not `q.canned`
semantic units but instead are context-sensitive performances
that require interpretive understanding.  This interpretive
dimension is arguably better conveyed by DG-style models
whose consituent units are word-relations, as opposed
to phrase-structure grammars which (even if only by notational
practice) give the impression that phrases conform
to predetermined, conventionalized gestalts.
`p`

`p.
While Link and Dependency Grammars are both contrasted
with phrase-structure grammars, Link Grammar is also
distinguished from mainstream DG in terms of how
inter-word relations are conceived.  Standard DG
recognizes an assymetry between the elements in word-relations
%-- one element (typically but not exclusively a word) is
treated as `q.dependent` on another.  The most common case is where
one word carries greater information than a second, which
in turn adds nuance or detail %-- say, in `i.rescued dogs`
the second word is more essential to the sentence's meaning.
This potentially raises questions about how we
can actually quantify the centrality of one word or another
%-- in many cases, for instance, the conceptual
significance of an adjective is just as trenchant as the
noun which it modifies.  In practice, however, the salient
aspect of `q.head` vs `q.dependent` assymetry is that any
inter-word pair is `q.directed`/, and one part of the relation
defined as dependent on another, however this
dependency is understood in a given case.
`p`

`p.
By contrast, Link Grammar dos not identify a head-dependent
assymetry within inter-word relations.  Instead, words
(along with other lexically signifant units, like
certain morphemes, or punctuation/prosodic units) are
seen as forming pairs based on a kind of mutual
incompleteness %-- each word supplying some structural
or signifying aspect that the other lacks.  Words, then,
carry with them different varieties of `q.incompleteness`
which primes them to link up with other modls.  Semantic
and grammatical models then revolve around tracing
the `i.gaps` in information content, or syntactic
acceptability, which `q.pull` words into relation
with other words.  This approach also integrates
semantic and syntactic details %-- unlike frameworks
such as Combinatory Categorical Grammar, which
also treats certain words as `q.incomplete` but
identifies word connctions only on
surface-level grammatical terms %-- Link Grammar
invites us to explore how semantic and
syntactic `q.completion` intersects and overlaps.
`p`

`p.
Words can be incomplete for different reasons
and in different ways.  Both verbs and adjectives
generally need to pair up with nouns to form a
complete idea.  On the other hand, nouns may be
incomplete as lexical spotlights on the
extra-linguistic situation: the important point
is not that people feed dogs in general, but that
`i.the rescued` dogs were fed prior to their rescue.
So `i.dogs` `q.needs` `i.rescue` for conceptual
specificity as much as `i.rescue` needs `i.dogs`
for anchoring %-- while also
`i.dogs` needs `i.the` for `i.cognitive` specificity,
because the discourse is about some particular dogs
(presumed known to the addressee), signaled by
the definitive article.  In other cases,
incompleteness is measured in terms of syntactic
propriety, as in: |+|

`sentenceList,
`sentenceItem; `swl -> --- ->  We learned that people fed the rescued dogs. -> syn ;;
`sentenceItem; `swl -> --- ->  No-one seriously entertained the belief that
he would govern in a bipartisan manner. -> syn ;;
`sentenceList`

|.|

In both cases the word `i.that` is needed because
a verb, insofar as it forms a complete predicate with
the proper subject and objects, cannot
always be inserted into an enclosing sentence.
`i.People fed the rescued dogs` is complete as a
sentence unto itself, but it is not complete as a grammatical
unit when the speaker wishes to reference the
signified predicate as an epistemic object,
something believed, told, disputed, etc.  A
connector like `i.that` transforms the
word or words it is paired with syntactically,
converting across part-of-speech boundaries
%-- e.g. converting a proposition to
a noun %-- so that the associated words can be
included into a larger aggregate.
`p`


`p.
The interesting resonance between Link Grammar
and Cognitive Grammar is that this perspective allows
us to analyze how syntactic incompleteness
mirrors semantic incompletenss, and vice-versa.
`q.Incompleteness` can also often be characterized
as `i.expectation`/: an adjective `q.expects` a noun
to produce a more tailored and situationally
refined noun (or nominal `q.idea`/); a verb expects
a noun, to form a proposition.
Analogously,when we have in discourse an adjctive or
a verb we expect a corresponding noun %-- so
via syntactic norms language creates certain expectations
in us and then communicates larger ideas by
how these expectations are met.  Is a noun-expectation
fulfilled by a single noun or a complex phrase?
The notion of semantic and syntactic expectations
also coordinates nicely with type-theoretic
semantics; for example, the verb `q.believe`
pairs with a semantic unit that can be
interpreted in epistemic terms %-- not
any noun but a noun of a kind that can be the
subject of propositional attitudes (beliefs,
opinions, assertions, arguments, etc.).
`p`

`p.
Kenneth Holmqvist,
whom I discussed earlier as integrating Conceptual Space Theory
with Cognitive Grammar, made a study of `q.expectation`
in this or a similar sense a central feature of his
doctoral dissertation `cite<HolmqvistDiss>;; I'll point
out that there is an implicit resonance between `i.expectations`
and link grammar; so Holmqvist's research actually potentially
triangulates between Conceptual Space Theory, Cognitive Grammar,
and Link Grammar.
`p`

`p.
Continuing analysis of `i.that` qua subordinator:
the syntactic incompleteness of propositional
phrases modified by `i.that` can therefore
be traced to the semantic expectations
raised by `i.believe`/, and analogous
verbs (opine, argue, claim, testify).
The object of `i.testify`/, say, is a statement
of potential fact which we know not
to take as necessarily true or honestly
made (part of the nature of testimony is that
it may be deliberately or accidentally
fallacious).  But to properly pair with
`i.testify`/, then, phrases must be
semantically reinterpreted as nominalizations
of propositions, rather than as mere linguistic
exprssion of propositional content via
complete sentences.  The `q.epistemic`
context transforms sentential content into
nominal content available for further refinement: |+|

`sentenceList,
`sentenceItem; `swl -> itm:trumpcolluded -> The Trump campaign colluded with Russia. -> syn ;;
`sentenceItem; `swl -> --- -> Several witnesses testified that
the Trump campaign colluded with Russia. -> syn ;;
`sentenceItem; `swl -> --- -> Reputable newspapers have reported that
the Trump campaign colluded with Russia. -> syn ;;
`sentenceItem; `swl -> --- -> Most Democrats believe that
the Trump campaign colluded with Russia. -> syn ;;
`sentenceList`

|.|

The grammatical stipulation that a modifier like
`i.that` is often necessary in such formulations correlates
with the semantic detail that the `q.claimed`/,
`q.testified`/, or `q.believed` content is not being
directly asserted by the speaker as if in an
unadorned declarative expression, as in
(`ref<itm:trumpcolluded>;).
`p`

`p.
Morphosyntactic transformation similarly models
the correlation between semantic and syntactic
expectation %-- as can be demonstrated by a
variant of the `q.believe` forms, via the phrase
`q.believe in`/: |+|

`sentenceList,
`sentenceItem; `swl ->  --- -> I believed in Father Christmas. -> syn ;;
`sentenceItem; `swl ->  --- -> I believed in Peace on Earth. -> syn ;;
`sentenceItem; `swl ->  --- -> I believed in Obama. -> syn ;;
`sentenceItem; `swl ->  --- -> I believed in lies. -> syn ;;
`sentenceList`

|.|

Whereas `i.that` (after `i.believe`/)
`q.nominalizes` propositions, `i.in` reconceives
(type-theoretically we would say `q.coerces`/)
ordinary nouns into epistemic nouns (compatible
with propositional attitudes).  Obama is not an
`i.idea`/, but the connector `i.in` triggers an
interpretation where we have to read `q.Obama` as
something believed %-- effectively a type-theoretic
tension resolved by understanding `i.Obama` in this
context to designate either his platform or his
ability to implement it.  Interpretive `i.tension`
is a natural correlary to a mismatch in expectations:
`i.believe` expects something epistemic, but the
discourse gives us a proper name.  Analogously
`i.budge` expects a brute physical entity in
its simplest meaning, but in: |+|

`sentenceList,
`sentenceItem; `swl ->  --- -> Obama wouldn't budge on reproductive rights. -> ont ;;
`sentenceList`

|.|

we get a `q.sentient`
noun, and have to read `i.budge` metaphorically.
In short, `i.expectation`/, `i.interpretive tension`/,
and `i.incompleteness` are interlocking facets of
semiotic primitives that gestate into discursive
maneuvers via which ideas are communicated
economically and context-sensitively.
`p`

`p.
Link Grammar, proper, represents only the
most immediate (mostly grammatical) facet of
word links.  For sake of discussion,
I will discuss links in general as markers of
`i.mutual dependency` between words, so a
`q.link grammar` is essentially a
`q.bi-directional` Dependency Grammar.  Mutual
dependencies manifest syntactic norms and
contextual details that make individual
words inadequate signifying vehicles for
a particular communicating content.  This overall
principle becomes concrete in one form via
grammatical relations, which is the layer
modeled by Link Grammar proper.
I have mentioned several ready examples %--
the syntactic dependency of verbs and adjectives
on nouns for them to enter correctly into
discourse (correlate with a semantic dependency
in the other dirction, to shape noun-ideas to the
proper context and signifying intent); also
part-of-speech or `q.subtyping` dependencies
reflecting mandates that (in my examples)
propositional phrases are coerced to nouns or
nouns coerced to `q.epistemic` nouns.  Technical
Link Grammars recognize a broad spectrum of
`q.link relationship` %-- between 50 and 100
for different languages.  Parsing a sentence
is then a matter of identifying
all of the mutual dependencies %-- at least those
evident on a syntactic level %-- that appear as
inter-word links in the sentence.  Phrase
structure may be implicit in some links in
combination %-- for example verb-subject plus
verb-object links generate propositional phrases
%-- but the technical parse is a `q.graph` of
inter-word links rather than a `q.tree` of
phrases ordered heirarchically.  The parse-graph
itself is only a provisional reading of the
sentence, and linguistic understanding
exists only insofar as its skeletal outline
is filled out with semantic and situational
details.  But the graph layer articulated by
a Link Grammar still provides a useful
intermediate representation, showing
mutual dependencies in their syntactic manifestations
that then point toward thir deeper semantic and
situational origins.
`p`

`p.
For each `i.syntactic` bi-dependency, on this theory,
there is a concordant semantic and signifying
bi-dependency, partly conventionalized as a feature
of the language and partly hewn to the current
discourse context.  To leverage Link Grammar
in an overall Cognitive Linguistic environment,
then, we need to examine the semantic relations
that drive syntactic bi-dependency: how the
grammatical structure of one word completing another
is a codification of `i.semantic` mutual dependency.
The `i.semantic` bi-dependencies operate on both
abstract and concrete levels.  Abstractly,
it is obvious that an adjective or a verb depends
on a linked noun to complete an idea.  This
abstract prototype of bi-dependency then takes
concrete forms in each specific discourse,
acquiring detail and specificity from
situational contexts.
`p`

`p.
The crucial dimension in this theory is
neither abstract nor concrete bi-dependency
but the intersection of the two.  The
conventionalized lexical, syntactic, and
morphosyntactic norms of a language present
abstract prototypes of mutual word
dependencies.  The concrete instantiation of
these forms %-- via word-pairs whose
surface presentation indicates the
presence of specific link relations
(often with the aid of morphology, agreement,
spoken inflection, and other
morphosyntactic cues) %-- invites us
to consider how an abstract bi-dependency
becomes situationally concretized in the
present, momentary context.  In essence,
abstract mutual dependencies are the raw
materials from which situational appraisals
are created.  A pairing like `q.rescued dogs` uses
a certain abstract-bidependent prototype
%-- here the double-refinement of a noun and adjective
grounding each other %-- to trigger
the listener's awareness that the speaker's
discourse is centered on one specific aspect of the
dogs (that they were at some point rescued)
with its conceptual corrolaries and unstated assumptions
(that, being in need of rescue, they were abandoned,
in danger, and so on).  Similarly
the further link to the definite article
%-- `q.`i.the` dogs` %-- evokes the prototype of a
definite article grounding a noun, which
in turn communicates the speaker's beliefs
about the current state of the discourse.
`p`

`p.
This last `q.bi-dependency` deserves further comments,
because nouns more often than not reveal some
dependency on an article: `i.some dog(s)`/,
`i.the dog(s)`/, `i.a dog`/, `i.many dogs`/.
These pairings paint a picture both through the
choice of article and the presence or absence of a
plural.  This picture is partly situational
%-- obviously whether the speaker is talking
about one or multiple dogs is situationally
important %-- but it is also meta-discursive:
selecting the definite article indicates the
speaker's belief that the listeners know which
dogs are on topic.  The lexeme `q.the` thereby
signifies a meta-discursive stance as well
as a cognitive framing %-- both that the
collection has enough coherence to function as
a conceptual unit in context, as `i.the` dogs
(and not something less specific like
`q.`i.some` dogs`/) `i.and also` that the
speaker and listeners share compatible
cognitive pictures as a result of the prior
course of the discourse.  This also introduces
several avenues for future discursive evolution
%-- the listeners can respond to the
speaker on both cognitive and meta-discursive
levels.  A direct question like `i.which dogs?`
signifies that the first speaker's meta-discursive
presuppositions were flawed %-- the referent of `q.the dogs` has
not been properly settled by the discourse to that
point.  Or questions for clarification
%-- like `i.how many dogs are there?` %-- indicate
the listeners' sense that all parties' respective
construal of the situation needs to be more neatly
aligned for the discourse to proceed optimally.
`p`

`p.
The point I particularly want to emphasize here,
though, is that these discursive/cognitive effects
inhere not only in the word `q.the` but in its pairing
with other words, like `i.the dogs`/.  We tend to see
the lexical substratum of a language as the ground
level of its signifying potential, but we should
perhaps recognize bidependent prototypes as
equally originating.  The communicative
content of `i.the dogs` is ultimately
traced not only to the lexical potentialities
of `i.the` and `i.dogs` as word-senses, but to the
abstract prototype of the definite-article
bidependence, which becomes concretized in the
`i.the dogs` pairing at the same time as the
individual words do.
`p`

`p.
In order to bring this account full-circle, I
would then add that lexical units mutually
completed by an instantiated bidependence
can also be seen as a tie-in between
interpretive procedures.  Lexical interpretive
scripts %-- the cognitive processing solicited
by `i.the` and `i.dogs` in isolation
%-- are themselves open-ended and ungrounded or
incompletely grounded.
`p`

`p.
We can speak metaphorically
of `q.words` being incomplete, or carrying expectations,
but it is really the cognitive scripts associated
with words that are lacking in detail.  The resolution
of a merely schematic cognitive outline to a
reasonably complete situational construal
can be likened to a rough sketch filled out in color
%-- but we have to imagine that a sketch can be
completed by pairing it with a second sketch, and
the content in each one, crossing over, allows a
completed picture to coalesce.
`p`

`p.
So in the current running example, `i.the` in itself
evokes an interpretive process that on its own logic
cannot be completed, and likewise `i.dogs`/; but
each script supplies the content missing from
the other.  In this sense the bidependent form
concretized by the pair is actualy evoking
an interpretive phenomenon of mutual completion
%-- the language structure here is guiding us
toward an interpretive interpenetration where
the two scripts tie each other's open ends.
Whereas lexical items can be associated with
single `q.scripts`/, prototypes of mutual
dependency model patterns in how script-pairs
can become mutually complete.
`p`

`p.
But unlike
lexemes, which are notated directly by
language, the instantiation of
bidependent script-pairs occurs indirectly.
Some paired-up words are of course adjacent,
but adjacency between words does not have the same
binding determinacy as sequencing among morphemes
in `i.one` word.  Instead, word adjaceny
is only one signal among many others suggesting
that some prototypical inter-word relation
applies between two words (which might be
widely separated in a sentence).  Morphology
and syntax also point towards the pairings operative
in a sentence %-- they are to bidependency prototypes
what word-choice is to the lexicon.
`p`

`spsubsectiontwoline.Link Grammar as the Syntax of
Procedural-Network Semantics`
`p.
Thus far I have made an admittedly
`i.philosophical` and speculative case for
`q.interpretive mutual dependence` as
a constituent building block of linguistic
understanding.  This theory will remain troublingly
incomplete if the more philosophical presentation
cannot be wed to a more rigorous formal
methodology.  True, an essential core of this theory
is that interpretive `q.scripts` are largely
prelinguistic and so not covered by linguistic
formalisms in themselves.  However, I have also
argued that formal linguistic structures `i.do`
govern how we identify which links apply to
which word-pairs, and the general outlines of
how word-pairing coordinates cognitive processes associated
with single words %-- the fully contextualized
synthesis of lexically triggered cognitive procedures
may involve extra-linguistic grounding, but
abstract prototypes of bidependnet relations
are also prototypes of a synthesis between
cognitive/interpretive functions.  It would
accordingly be reassuring if notions like
`q.bidependency` and `q.mutual completion`
could be employed as foundations for a formal
theory of grammar and/or semantics with a
degree of rigor comparable to, say, Link
Grammar in its computational form, or type-Thoretic
Semantics in the sense of Zhaohui Luo or
James Pustejovsky.
Such a theory %-- and potentially concrete
technologies associated with it %-- would also
then have a reasonable ground of comparison
to the Semantic Web and, in the context of
phenomenology, to the formalizing influence
which Semantic Web paradigms have exerted on
projects to unite phenomenology with science
and with Analytic Philosophy.
`p`

`p.
Given these considerations, I propose that
formal grammars with the same underlying
structure as Natural-Language Link Grammars
can indeed be used as a foundation for type-theoretic
and programming-language-design methodology.  The
key step here is to generalize Link Grammar's
notion of a `q.connector` %-- the aspect of a
word or lexeme that allows (or requires) compltion
via another word %-- to a gneric data structure
where connections can be made between different
parts of a system on the basis of double
potentials that must be in some sense `q.compatible`
for the connection to be valid.  One way to
visualize such a system is via graph theory:
imagine a form of graphs where nodes are
annotated with `q.potentials` or `q.half-edges`/;
a complete edge is then a union of two half-edges.
Half edges are also classified into different
families, and there are rules governing when a
half-edge of one family may link with a half-edge
of another.  In the case of Link Grammar, these
classifications are based on surface language structure
%-- head/dependent and left-to-right relations %--
from which a suite of links and connection rules
are defined (for instance, abstractly a head/right
word must link to a dependent/left word, a rule that
then becomes manifest in specific syntactic rules,
like how a verb links to its subject).  For a
more generic model, however, we can stipulate only
that there is `i.some` classification of connectors
governed by `i.some` linkage rules, to be specified
in different details for different modeling domains.
`p`

`p.
Such a graph model expands upon the notion
of `i.labeled` graphs, where edges are annotated
with labels that characterize the kind of relation
modeled via the edge itself.  A canonical example
is Semantic Web graphs: the edges in any
Semantic Web structure are labeled with `q.predicates`/,
defined in different Ontologies, specifying what sort
of relation exists between its adjacent nodes.
That is, in the Semantic Web, nodes are not
`q.abstractly` linked but rather exhibit
concrete relations: a person is a citizen
of a country, two persons are married, and so forth.
These structures are then concrete instances of Labeled
Graphs as abstract mathematical structures.  Based on
Link Grammar, we can then refine this model
be splitting labels into two parts, and
allowing edges to be incomplete: a fully
formed edge is possible when the label-parts
on one side are compatible with the label-parts
on another.  One valid class of graph transforms
is then a mapping where a graph is altered by unifying
two half-edges into a complete edge, subject to
the relevant linkage rules.
`p`

`p.
Another way of modeling this kind of structure is via
edge-annotations and a rule for unifying two edges into
an edge-annotation pairing.  For sake of
discussion, I will express this in terms of
Directed Hypergraphs: assume that edges are
`q.hyperedges`/, connecting `i.sets` of nodes.
In Hypergraph theory, the nodes incident to a hyperedge
are divided into a `q.head set` and `q.tail set`/;
these sts can then aggregate as `q.hypernodes`/.
We can then define a kind of unification where
the `q.tail hypernode` of one hyperedges joins
with the `q.head hypernode` of another, producing
a new hypredge whose head comes from the
first former hyperdge and whose tail comes from the
second.  The merged hypernodes, in turn, form a new
hypernode which `q.annotates` the new hyperedge
(this new hypernode is not connected to the graph
via other nodes, but is indirectly `q.part` of
the graph through the hyperedge it annotates).
`i.Annotated` hyperedges therefore differ from
`q.non-annotated` hyperdges in that the former are the
result of a merger between two of the latter.
The rules governing when such merger is possible
%-- and how to map a pair of hypernodes into a
single `q.annotative` hypernode (which
belongs to the graph through the aegis of
its annotated hyperdge) are not internal
to the graph theory, but presumed to be
specified by the modeling environment where
implemementations of such graphs are
technologically applied.  Annotated Directed
Hypergraphs are then `q.complete` in a sense
if every `q.un-annotated` hyperedge has been
subsumed into an annotated hyperedge, via a
fusional process we can call a
`q.annotative-fusional transform`/.
`p`

`p.
Extending this model further, we can say that the
tail of an `i.un-annotated` hyperedge is a
`q.tail pre-annotation`/, since it is poised to
be merged into an annotation.  Analogously,
the head of an un-annotated hyperedge is a
`q.head pre-annotation`/, and `q.annotative fusion`
is the synthesis of a head and a
tail pre-annotation (triggering a synthesis of
their incident hyperedges).  Correlate to annotative
`i.fusion` we can define a notion of annotative
`i.partiality`/, referring to the `q.incompleteness`
of pre-annotations which leaves room for their fusion.
`p`

`p.
It turns out that annotative fusion and partiality
in this sense is a non-trivial
model for computation in general, and can be extended
to a form of type theory and process calculus.
The idea is that computational procedures can be
modeled as hypergaphs (computer source code
can certainly be modeled as hypergraphs which
are productions of a certain class of parsing
engines).  Each `q.value` affected by a
procedure %-- or more technically the source code symbols
and memory addresses that `q.carry` a value %-- is
then modeled as a hypernode that can link with
other hypernodes in the scenario where one procedure
calls a different one.  Annotative fusion is then
a phenomenon of values being transferred from
one execution environment (associated with
the caller procedure) to a second one (associated with
the callee).  The `q.annotations` themselves
are then in this context the full set of type
coercions, type checks, synchronization
(e.g. resource locks or thread blocks depending on
whether or not the caller waits for the callee to
finish), and any other validations to ensure that
the procedure call is appropriate.  Annotative fusion
also provides a formal basis for developing the
intuition that `q.procedural networks` are rigorous
representations of information spaces %--
annotative fusions capture the precise details
of procedures linking (via caller/callee relations)
with other procedures.
`p`

`p.
The constituent units of procedural networks are
inter-procedure calls %-- but procedural networks
also reveal dimensions of connectivity and
and clustering characteristic of large, complex
networks (and the graphs that represent them) in general.
What appears as one function-call in source code can
actually represent many different inter-procedure connections,
a phenomenon reflecting `q.overloading` and `q.genericity`
in programming language theory.  Functions are generic
in the sense that any one of their arguments can take
multiple types %-- either because the function is
explicitly declared to take a `q.typeclass` or a single type
for that argument, or because an instance of a given
type may actually be at runtime an instance of some subtype.
The engine which actually implements inter-procedure calls
%-- i.e., the programming-language implementation %--
needs to factor this genericity into runtime decisions, so
a single expression in one function body can branch
to many different called procedures.  This is the
essential core of the `q.semantics` of programming
languages: data structures manipulated by computer code do
not `i.intrinsically` represent real-world,
non-digital phenomena, though they
are enginered to model external data when used properly.
However a code base does `i.internally` posess a space
of implemented functions, and a symbol at one
place in source code can match to some set of other
functions so as to effect a procedure call.  This
`q.matching`/, and the rules governing how
`q.overload resolution` occurs %-- `q.overload`
meaning that a given notated procedure call can actually
branch to multiple functions, so runtime
information is needed to select the right one %-- are
the essential formal principles governing the
semantics of computer code.
`p`

`p.
From this basis, all the same, computer code can model
a wide range of empirical phenomena.  Generic code
represents generic patterns of functional organization,
allowing models to be built from varying layers of
abstraction.  From this perspective, to
describe an empirical system it is neessary to
identify important behaviors and functional patterns
via which the system's observed behavior can be
notated and/or simulated.  To the degree that
systems take on functional organizations that can be
abstractly described, similar to the functional
dynamics of other systems, their behavior
can be represented and/or simulated via generic
code.  To the degree that there are particular
details of a system's behavior that are more
idiosyncratic to that system, and need to be modeled
precisely, procedures can be crafted specifically
for observing and emulating that exact behavior.
More generic and more exact procedural implementations
can coexist in a single code base, with generic
functions calling granular functions narrowed to
precise types, and vice-versa.  The coexistence of
generalization and specificity is an essential feature
of code bases and, by extension, of procedural networks,
ensuring the flexibility of these networks
as tools to model information spaces.
`p`

`p.
Unfortunately, this kind of `q.procedural` modeling
is hard to intgrate with the more static techniques
representd by the Semantic Web and the current
`q.Big Data` fad.  The latter paradigms tend to treat
data as a static repository to be mined for patterns
and insights, rather than a digital
simulation or encoding of dynamic real-world systems.
The Semantic Web, as a large, collaborative modeling
project, evolved largely apart from the technological
community concerned with computer simulations and
the programming techniques which emerged from there,
like Object-Orientation.  This divergence is relevant
for linguistics and cognitive science, because I would
argue that the more `q.dynamic` paradigm is actually
more `q.Semantic` in a Natural Language sense.
In other words, our cognitive dispositions
when interpreting empirical phenomena %-- and
matching these interpretations to linguistic
cues %-- are more like procedural networks
capturing functional patterns and layers of genericity
in observed phenomena, rather than an accretion of
static data.  The techniques of procedural data
modeling may therefore be relevant for Cognitive
Linguistics and Cognitive Phenomenology
because they aspire to something which, arguably,
the mind does instinctively: build
cognitive or computational models of
dynamic, functionally organized phenomena.
`p`

`p.
As a corrolary, the theoretical building-blocks
of Procedural Data Modeling %-- how it leverages
type theory, programming language semantics, and
so forth %-- can provide at least
analogs or case-studies for corresponding cognitive
phenomena.  Here I would argue that generalizing
Link Grammar from Natural Language to formal
languages, type systems, and lambda calculi
yields added structures to type theory that
are useful toward a more rigorous `q.theory` of
Procedural Data Modeling %-- a theory of natural
linguistics generalized to a theory of general data
representation which, in turn, may offer
insights onto the cognitive dynamics underlying
(prelinguistic) situational/perceptual
comportments and interpretation.
`p`

`p.
Type-theoretically, annotative partiality %-- which
recall is my terminology for the abstract
generalization of the mutual `q.incompleteness` in
Link Grammar connctors, driving their
link-fusion %-- extends conventional applied type theory
(as in the Typed Lambda Calculus) in parallel
to partial-labels extending labeled graph theory.
It is paradigmatic in the theory of typed procedures
and of `q.effect systems` that the type of a procedure
is determined (up to certain equivalences that may discard
overly granular type distinctions) by the types of
all values affected by the procedure (including but
not necessarily limited to the types of input
and output parameters).  We can then superimpose
on this model an account of annotative partiality.
Specifically, on the paradigm that procedure-calls
are structurally represented as annotative fusions over
Directed Hypergraphs, the values manipulated by
a procedure are pre-annotations: they are not
(in the `i.implementation` of a procedure,
as a formal object) single values but rather
typed spaces that can take on a spectrum of
possible values depending on the inhabitants of
their types.  When a procedure is `i.called` these values
become concretized, but as a formal system procedural
networks model software in terms of its capabilities
and expected behavior, rather than the state at
any moment when the software is actually running.
Partiality therefore models how precedures
(as formal objects) deal with potentialities
%-- we do not know what values will `i.actually`
be present at runtime (e.g. what specific
values passed to a procedure as arguments),
so procedural analysis is essentially characterized
by a partiality of information.
`p`

`p.
When one procedure
calls another, the caller must build an
`i.expression` %-- a gathering of values that provide all
the information the callee requires %-- thereby
creating a case of mutual-completion: the caller has
values but not an algorithm
to operate on them; the callee has an algorithm
(that's what it implements) but needs concrete
values so to produce concrete values.
This dual partiality allows the caller to
call the callee, via an `i.expression` which is part
of the callee's implementation (represented as a
hypergraph) which must in turn match the
callee's signature %-- epigrammatically, we
can say that `q.expressions are annotative-fusional
duals of signatures`/.  The point here is that whereas
signatures are conventionally understood to be
type-declarations assigning types to procedures,
with annotative partially we can more
precisely recognize signatures as stipulating
`i.pre-annotative` types.  The values carried
within expressions also have pre-annotative types,
but there is a distinction between types in the context
of expressions and types in the context of signatures
%-- and moreover this distinction is precisely the
manifestation of the abstract head-pre-annotation
and tail-pre-annotation contrast in the specific
context of procedural networks.  Just as
signatures unify multiple types into one
profile, we can analogously define
`q.expression types` as the aggregate of all types
from values affecting the expression %-- note that
this is different from the type of the expression's
calculated `i.result`/, just as the type of
a function is different from the type of its
return value.  Expression-types and signature-types
are almost exact duals (the complication being
default values for optional parameters, which
are not directly represented in exprssions
%-- obviously, since then they would not be
missing).  The `q.duality` involved here
derives from partitioning a type system into
`q.expression annotative partials` and
`q.signature annotative partials`/, a projection
of head/tail duality in an abstract theory
of Annotated Directed Hypergraphs
(and analogous to head/dependent
and left/right partiality in Link Grammar).
`p`


