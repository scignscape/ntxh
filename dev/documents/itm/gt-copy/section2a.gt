
`subsection.The Chinese Room Revisited`
`p.
John Searle's `q.Chinese Room` argument %-- about someone who 
behaves like he understands Chinse by matching characters to responses 
from a vast table %-- is often understood as claiming that 
`q.symbol processing` by itself can never produce real 
understanding, which is `i.semantic` and `i.conceptual`/.  
Modern technology makes this thought-experiment less 
hypothetical: automated telephone systems often use a template 
mechanism that is practically like Searle's Chinese Room, 
understanding a limited range of sentences and producing a limited 
range of responses.  But there are two different kinds of questions we 
can ask in relation to Searle's argument: some 
more philosophical and some more practical.
`p`

`p.
On the philosophical side, we should properly assess the 
important questions as being qualitative and not 
quantitative: it's not as if a synthesized phone system 
is just not a very `i.good` conversationalist; it's that 
a software machine simply isn't the `i.kind of thing` that 
we can say actually understands language.  This is 
plausible if we say that emotions and empathy are 
intrinsic to language; that we can't properly understand 
language if we do not grasp the emotions residing 
behind expressions.  Indeed, as the case of Grandma's window 
shows (which I analyzed in Part I, where we 
should try to satisfy her request to close a window which 
in fact is already closed, perhaps finding a window which `i.is` 
open and causing a draft), our 
status as competent interlopers depends on reading intentions 
behind expressions, and it seems hard to do this if we can't experientially 
empathize with our linguistic partners.
`p`

`p.
Maybe we are now just pushing the important questions back 
to reappear: Ok, can computers be programmed to feel emotions?  
Is there a meaningful distinction between meaningfully, 
experientially having emotions and just behaving as if 
you have them?  Are emotions themselves somehow 
functionalizable apart from their 
chemical/hormonal substrate so that systems with very 
different physical realization than ours can be said 
to have emotions?  I can see how such a debate can go 
different ways.  But I'd also argue that 
any well-organized dialog about these questions will be 
only tangentially about language %-- in which case, 
neither linguistics nor philosophy 
of language themselves can answer questions about 
what kind of systems (on metaphysical criteria) 
actually `q.do` language.  That would imply 
that affirming a computer's linguistic capabilities 
as `i.real` linguistic understanding is a 
disciplinary non-sequitor for linguistics proper.  
Nothing in the linguist's arsenal either 
demonstrates or depends on AI agents actually
`i.being` part of our linguistic community or just 
mimicking language-use to some (sometimes helpful) degree.
`p`

`p.
The more practical questions raised by Searle's 
Chinese Room come into play to the 
degree that the philosophical trail I just sketched 
turns many analyses into a non-starter.  
Consider these two questions to a hypothetical 
automated telephone service: |+|

`sentenceList,
`sentenceItem; `swl -> --- -> What time does the office open? -> log ;;
`sentenceItem; `swl -> itm:phone -> What time does train 100 depart from Newark? -> log ;;
`sentenceList`

|.|

While we can see a template holding canned responses for both 
cases, (`ref<itm:phone>;) needs to do more than just 
fit the input to the nearest pattern; it has 
to pull out the dynamically variant details 
(train `i.100` from `i.Newark`/) and use those to 
fill in details in the response.  This is something 
like `i.parsing` the original question.  So we can add 
bits and pieces of genuine linguistic processing 
to a minimal response-template system %-- a real version of 
what Searle appeared to imagine in the Room.  With enough 
added features the primitive template-driven 
kernel can evolve into a complex AI-powered 
Natural Language Processor.  
`p`

`p.
In that case we may imagine that `q.language 
understanding` exists on a spectrum.  The primitive 
telephone service and an erudite bard may lie on 
opposite ends of a spectrum, but they share a spectrum 
between them.  In this case, their differences are 
quantitative more than qualitative.  The bard just 
has more `i.features` we associate with total 
linguistic behavior.
`p`

`p.
However, this quantitative view still leaves open the question 
of where among the `q.features` do we have something 
that actually drives language competence?  Searle's 
Chinese Room helps point out these questions: 
it is reasonable to say that the simplest template-response 
system does not really understand language at all, since it 
is a pattern-matching system that does not 
have any structural relation to language itself.  
Analogous capabilities can be developed for a system 
which matches any kind of input to a pattern directing 
an output, based on any metric of similarity.  
The patterning reflects an actual `i.linguistic` 
parse only insofar as it selects elements 
via syntactic criteria, like grasping the 
non-template variables as `i.100` and `i.Newark`/.  
So, even if the holistic behavior different systems 
lies on a linguistic-competence scale, not 
all `i.parts` of the system seem to bear the weight 
of actually `i.realizing` linguistic competence equally.
`p`

`p.
One reading of the Chinese Room is that `i.no` 
part of a system is truly 
linguistic.  This includes the argument that 
holistically the Chinese Room `i.does` 
speak Chinese: Searle's discussion 
suggests that no `i.part` understands Chinese, but 
if we can imagine the entire room as a single 
system this `q.entity` can be treated as a fluent 
Chinese speaker.  Even if we reject that analysis, 
we could agree that, even among humans, `i.parts` of 
our language system arguably do not understand language: 
not nerve cells, not neural clusters for auditory 
processing, or syntax, or conceptualization, etc.  
It is us, the whole system, that 
uses language.  The reason why `q.holistic` claims 
that `q.the entire room` speaks Chinese sound dubious 
may not be because something is 
`i.structurally` lacking in that whole
system, but because it's not the kind of whole 
system %-- with one body, one consciousness, one 
personhood %-- that we think of as a conversant. 
`p`

`p.
Those who find Searle's analysis compelling probably believe 
that there `i.is` some meaningful difference between us 
(or at least people fluent in Chinese) and 
the Chinese Room.  A further alternative, however, 
is that `i.we` are not language-usrs, at least not 
in the way we think we are.  This 
claim can be expounded as follows: the philosophy 
of language, interactively with linguistics, seems 
to be looking for some essential kernel of linguistic 
capability that distinguishes us from AI engines or 
template-response system.  That is, AI-skeptics want to 
sift through all the models of processes within 
languages, the central domains in linguistics, 
and find the few genres of linguistic processing that are 
unique to human language %-- and computationally 
intractable.  These would be the smoking gun evidence that no 
artificial system can equate to human language-use because 
there is some essential stage in the linguistic 
pipeline that computers computationally 
can't realize.
`p`

`p.
However, even if we accept as premises first that the Chinese Room case 
suggests this analysis %-- and second that it agrees with our 
underlying intuitions %-- there remains 
the possibility that computers are indeed lacking 
some stage associated with language %-- but it 
is not a `i.linguistic` stage.  If something like 
an Interface Theory of Meaning is correct, all linguistic 
processing is intermediary to some other 
cognitive layer: and perhaps the human quintessence 
lies on the far side, so it both limits what computers can 
linguistically achieve and lies outside of linguistics 
proper.
`p`


