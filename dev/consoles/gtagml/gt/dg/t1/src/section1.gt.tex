

\section{Cognitive Transform Grammar and Transform-Pairs}
\label{s1}

\p{The idea that inter-word pairs are a foundational linguistic
unit \mdash{} from which larger aggregates can be built up recursively
\mdash{} is an central tenet of Dependency Grammar.  Here I will
generalize this perspective outside (but not excluding)
grammar, to overall semantic, pragmatic, and even
extralinguistic relations indicated via interword relations.
}

\p{In some cases word-relations can still be theorized mostly via
syntax.  Consider hypothetical, example sentences like

\begin{sentenceList}

\sentenceItem{} \swl{itm:having}{His having lied in the past damages his credibilty in the present.}{syn}
\sentenceItem{} \swl{itm:whether}{Voters question whether he is truthful this time around.}{syn}
\end{sentenceList}

In (\ref{itm:having}), \i{having} is necessary to syntactically transform its ground
\i{lied} from a verb-form to a noun (something which can be inserted into a
possessive clause).  Analogously, in (\ref{itm:whether}) \i{whether} modifies
\i{is} (since this is the head of a subordinate clause), wrapping a propositional
clause into a noun so that it furnishes a direct object to the verb
\i{question}.  The essential transformation in these cases is
motivated by grammatic considerations, particularly part-of-speech:
a verb and a subordinate, propositionally complete clause (in (\ref{itm:having}) and
(\ref{itm:whether}), respectively) need (for syntactic propriety) to be
modified so as to play a role in a site where a noun is expected
(in effect, they need to be bundled into a noun-phrase).
}

\p{The relevant transforms here \mdash{} signified by \i{having} and \i{whether} \mdash{} have a
semantic dimension also, and we can speculate that the syntactic
rules (requiring a verb or propositional-clause to be transformed into a
noun) are actually driven by semantic considerations.  Conceptually,
for example, \i{his having lied} packages a verb into a possessive
context because the sentence is not foregrounding a specific lying-event
but rather the fact of the existence of such occasions.  We cannot perhaps
\q{possess} an event, but we possess (as part of our nature or history)
the fact of past occurrences, viz., events in the form of things we
have done.  In this sense \i{his having lied} marks a conceptual transformation,
from events qua occurrents to events (as factical givens) qua states or
possessions, and the grammatical norm \mdash{} how we cannot just say
\q{his lied} \mdash{} is epiphenomenal to the conceptual logic here;
the erroneous \q{his lied} sounds flawed because it does not
match a coherent conceptual pattern in how events and states fit together.
But, still, the syntactic requirement \mdash{} the expectation
that a noun or noun-phrase serve as the ground of a possessive
adjective, or the direct object of a verb \mdash{} manifests these
underlying conceptual patterns in the order of everyday language.
Syntactic patterns become entrenched \i{because} they are comfortable
translations of conceptual schema, but \i{as} entrenched we hear
these patterns as grammatically correct, not just as
conceptually well-formed.  Likewise, we hear errata like
\q{his lied} as \i{ungrammatical}, not as conceptually incongruous.
}

\p{I contend, therefore, that many conceptually-motivated word-pairing
patterns become syntactically entrenched and thus engender a class
of transform-pairs where the crucial, surface-level transformation
is syntactic, often in the form of translations between parts of speech,
or between morphological classes (singular\plural, object\location, etc.).
Consider locative constructions like

\begin{sentenceList}

\sentenceItem{} \swl{itm:grandma}{Let's go to Grandma.}{syn}
\sentenceItem{} \swl{itm:lawyers}{Let's go to the lawyers.}{syn}
\sentenceItem{} \swl{itm:press}{Let's go to the press.}{syn}
\end{sentenceList}

Here nouns like \i{Grandma}, \i{the lawyers}, and \i{the press} are
used at sites in the surrounding sentence-forms that call for a designation
of place \mdash{} this compels us to read the nouns as describing a place,
even while they are not intrinsically spatial or geographical
(e.g., Grandma is associated with the place where she lives).
}

\p{In (\ref{itm:press}) and perhaps (\ref{itm:lawyers}), this locative
figuring may be metaphorical: going \i{to the press} does not necessarily
mean going to the newspaper's offices.  Indeed, each of these
usages are to some degree conventionalized: going \i{to Grandma}
is subtler than going \i{to Grandmas house}, because the former
construction implies that you are going to a \i{place}
(\q{Grandma} is proxy for her house, say), but also that Grandma is actually
there, and that seeing her is the purpose of your visit.  In other words,
the specific \i{go to Grandma} formation carries a supply of
situational expectations.  There are analogous implications in
(\ref{itm:lawyers}) and (\ref{itm:press}) \mdash{} going \i{to the press}
means trying to get some news story or information published.
But the underlying manipulation of concepts, which structures the
canonical situations implicated in (\ref{itm:grandma})-(\ref{itm:press}),
is organized around the locative grammatical form as binding noun-concepts to a
locative interpretation.  However metaphorical or imbued with additional
situational implications, a person-to-location or institution-to-location
mapping is the kernel conceptual operation around which the further
expectations are organized.  Accordingly, the locative case qua
grammatic phenomenon signals the operation of these situational
conventions, and the syntactic norms in turn are manifest via
word-pairings, such as \i{to Grandma}.
}

\p{In short, a transform-pair like \i{to Grandma} can be analyzed in
several registers; we can see it as the straightforward
syntactic rendering of a locative construction (via inter-word morphology,
insofar as English has no locative case-markers) or explore further
situational implications.  In these examples, though, there is an
obvious grammatic account of pairs' transformations, notwithstanding
that there are also more semantic and conceptual accounts.
Part-of-speech transforms (like \i{having lied}) and
case transforms (like \i{to Grandma}) are mandated by
syntactic norms and therefore can be absorbed into conventional
grammatic models, such as Dependency Grammar: the head\dependent
pairings in \i{having lied} and \i{to Grandma} are each
covered by specific relations within the theory's inventory
of possible inter-word connections.  So a subset of
transform-pairs overlaps with (or can be associated with)
corresponding Dependency Grammar pairings.
}

\p{Another potential embedding of transform-pairs into
formal models can be motivated by Type Theory.
Such analysis may proceed on several levels, but
in general terms we can assume that parts of speech
form a functional type system (as elucidated, say, in
Combinatory Categorial Grammar; e.g.,
\cite{BiskriDescles}).
For instance, we can recognize
nouns and propositions (sentences or sentence-parts forming
logically complete clauses) as primitive types, and
treat other parts of speech as akin to \q{functions} between
other types.  A verb, say, combines with a noun to
form a proposition, or complete idea: \i{go} acts on
\i{We} to yield the proposition \i{We go}.  Schematically,
then, verbs are akin to functions that map nouns to propositions.
Similarly, adjectives map nouns to nouns, and
adverbs map verbs to other verbs (here I use \q{noun} or \q{verb} to
mean a linguistic unit which functions
(conceptually and\or \visavis{} syntactic propriety) as a noun, or verb;
in this sense a noun-phrase is a kind of noun \mdash{} i.e.,
a linguistic unit whose \i{type} is nominal).
This provides a type-theoretic architecture through
which transform-pairs can be analyzed.
An adverb modifies a verb; so an adverb in a transform
pair must have a verb as a ground.  Moreover, the \q{product}
of that transform is also a verb, in the sense that the
adverb-verb pair, parsed as a phrase, can only be
situated in grammatic contexts where a verb is expected.
}

\p{In effect, we can apply type-theoretic models to both
parts of a transform-pair and to the pair as a whole,
producing structural requirements on how words link up
into transform-pairs.  We can then see an entire sentence
as built up from a chain of such pairs, with the rules
of this construction expressed type-theoretically.
Given, say, \i{his having lied flagrantly} we can identify a
chain of pairs \i{flagrantly}-\i{lied},
\i{having}-\i{flagrantly}, and \i{his}-\i{having}, where
the \q{outcome} of one transform becomes subject to a
subsequent transform.  So \i{flagrantly} modifies \i{lied}
by expressing measure and emphasis, adding conceptual detail;
grammatically the outcome is still a verb.  Then \i{having},
as I argued earlier, applies a transform that maps this
verb-outcome to a noun, which is then transformed by
the possessive \i{his}.  Each step in the chain is
governed by type-related requirements: the output of one
transform must be type-compatible with the modifier for
the next transform.  This induces a notion of \i{type-checking}
transform-chains, which is analogous to how type-checking
works in formal settings like computer programming languages,
Typed Lambda Calculus, and Dimensional Analysis.
}

\p{This gloss actually understates the explanatory power of
type-theoretic models for linguistics, since I have
mentioned only very coarse-grained type classifications
(noun, proposition, verb, adjective, adverb); more
complex type-theoretic constructions come into play
when this framework is refined to consider plural\singular,
classes of nouns, and so forth, establishing a basis for
more sophisticated structures adapted to language from
formal type theory, like type-coercions and
dependent types (I will revisit these theories
in a later section).  Here, though, I will just point out
that Dependency Grammar and Type-Theoretic Semantics can
often overlap in their analysis of word-pairs (inter-word
relations is not centralized in type-oriented methodology as much
as in Dependency Grammar, but type concepts can certainly
be marshaled toward word-pair analysis).
}

\p{Even though Dependency and Type-Theoretic analyses will often reinforce
one another, they can offer distinct perspectives on
how pairs aggregate to form complete phrases and sentences.
In the transform-pair \i{having lied}, \i{lied} is clearly the
more significant word semantically.  This is reflected in
\i{having} being annotated (at least according to the Universal Dependency
framework) as auxiliary, and the dependent element of the pair, while
\i{lied} is the \q{head}.  Then \i{lied} is also connected to
\i{his}, establishing a verb-subject relation.  So \i{lied} becomes
the nexus around which other, supporting sentence elements are
connected.  This is a typical pattern in Dependency Grammar parses, where
the most semantically significant sentence elements also tend to be
the most densely connected (if we treat the parse-diagram as a
graph, these nodes tend to have the highest \q{degree}, a measure of
nodes' importance at least as this is reflected in how many
other nodes connect to it).  Indeed, by counting word connections we
can get a rough estimation of semantic importance, distinguishing
\q{central} and \q{peripheral} elements.  These are not
standard terms, but they suggest a norm in Dependency Grammar that the
structure of parse-graphs generally reflects semantic priority:
the central \q{spine} of a graph, so to speak, captures the
primary signifying intentions of the original sentence, while the
more peripheral areas capture finer details or syntactic auxiliaries
whose role is for grammatical propriety more than meaningful content.
}

\p{Conversely, a type-theoretic analysis might incline us to question this
sense of semantic core versus periphery: in the case of
\i{his having lied}, the transform \i{having} supplies the outcome
which is content for the possessive \i{has}.  If we see the sentence
as a cognitive unfolding, a series of mental adjustments toward an
ever-more-precise reading of speaker intent, then each step in the
transformation contributes consequential details to the final
understanding.  Moreover, \i{lied} is only present in the transformation
signified by \i{his} insofar as it has in turn been transformed by
\i{having}: each modifier in a transform-pair has a degree of
temporal priority because \i{its} effects are directly present
in the context of the following transformation.  This motivates a
flavor of Dependency Grammar where the head\dependent ordering is
inverted: a seemingly auxiliary component (like the function-word
to a content-word) can be notated as the head because its
output serves as \q{input} to a subsequent transform.  In the analogy
to Lambda Calculus, \i{his having lied} would be graphed with
\i{having} being the head for \i{lied}, and \i{his} the head for
\i{having}, reflecting the relation of functions to their arguments.
In lisp-like code, this could be written functionally as
(his (having lied)), showing \i{having} as one function, and
\i{his} as a second one, the former's output being the latter's input.
(Later I will include diagrams contrasting these different
styles of parse-representation.)
}

\p{Implicitly, then, Type-Theoretic Semantics and Dependency Grammar
can connote different perspectives on semantic importance and
the unfolding of linguistic understanding.  I will explore
this distinction further below, with explicit juxtaposition of
parse graphs using the two methods.  I contend, however, that the
distinction reflects a manifest duality in linguistic meaning:
we can treat a linguistic artifact as an unfolding process or
as a static signification with more central and more peripheral
parts.  Both of these aspects coexist: on the one hand, we
understanding sentences via an unfolding cognitive process;
on the other hand, this cognition includes forming a mental
review of the essential points of the sentence, a collation of
key ideas such as (for \ref{itm:having})
\i{his}, \i{lied}, \i{damages}, and \i{credibility}.
Given this two-toned cognitive status \mdash{} part dynamic process,
part static outline \mdash{} it is perhaps understandable that
different methodologies for deconstructing a sentence into
word-pair aggregates would converge on different structural
norms for how the pairs are interrelated, internally and to one another.
}

\p{This analysis, which I will extend later, has considered transform-pairs
from a syntactic angle \mdash{} in the sense that I have highlighted pairs
which obviously come to the fore via grammatic principles.  As I indicated,
I believe the notion of transform-pairs cuts across both syntax
and semantics, so I will pivot to some analyses which attend more
to the semantic dimension.
}

\spsubsectiontwoline{Semantic Analyses of Transform-Pairs}
\p{In the simplest cases, a transform-pair represents a modifier
adding conceptual detail to a ground, like \i{black dogs}
from \i{dogs}.  But the nature of this added detail
\mdash{} and its evident relation to surface language \mdash{} can be highly
varied, even among similar sentences at the surface level.
Compare between examples like:

\begin{sentenceList}

\sentenceItem{} \swl{itm:black}{I saw my neighbor's two black dogs.}{sem}
\sentenceItem{} \swl{itm:rescued}{I saw my neighbor's two rescued dogs.}{sem}
\sentenceItem{} \swl{itm:latest}{I saw my neighbor's two latest dogs.}{sem}
\end{sentenceList}

Whereas (\ref{itm:black}) presents a fairly straightforward conceptual
transformation, the detailing in (\ref{itm:rescued}) is a lot subtler;
mentioning \i{rescued} dogs makes no reference to perceptual qualities,
but rather implies intricate situational background.  The term
\i{rescued dogs} strongly suggests that the dogs were adopted by their
current owner, probably after an animal-welfare organization
found them abandoned, or removed them from a prior abusive owner.
This kind of backstory is packaged up, as a kind of
situational prototype, in the conventionalized phrase
\i{rescued dogs}, implying a level of specificity more
precise than the ajective \i{rescued} alone implies.
Correspondingly, the verb \i{to rescue} when applied to
dogs suggest more information than in more
generic contexts.
}

\p{The phrase \i{latest dogs} carries implications in its own
right; we assume the neighbor had owned other dogs before.
Of course \q{latest} implies some temporal order, but the
understood time-scale depends on context.
If we hear talk about a \i{vet}'s two latest dogs, we would presumably
interpret this in terms of patients the vet has seen over the course of
a day:

\begin{sentenceList}

\sentenceItem{} \swl{itm:vet}{We have to wait until after the vet's two latest dogs.}{sem}
\sentenceItem{} \swl{itm:organization}{I'm concerned for the rescue organization's
two latest dogs.}{sem}
\end{sentenceList}

Understanding the relevant time-frame depends on understanding the relation
between the dogs and the possessive antecedent.  In (\ref{itm:latest})
the neighbor (in a typical case) actually owns the dogs, so the
situational context grounding the modifier \i{latest} would be understood
against the normal time-scale for dog ownership (at least several years).
In (\ref{itm:vet}), the vet only \q{possesses} the dogs in the sense
of endeavoring to examine them, a process of minutes or hours.
In (\ref{itm:organization}), the implication of the \i{organization's}
possessive \visavis{} rescued dogs is that the group endeavors
to rehabilitate and find permanent homes for the rescuees.  So in each
case \i{latest} implies a succession of dogs, leading over time to
two most recent ones, but the implied time-frame for our conceptualizing
this sequence can be minutes-to-hours, or days-to-months, or years.
}

\p{We should also observe that the implied time-frames and backstories in
(\ref{itm:rescued})-(\ref{itm:organization}) are not directly signified via
morphosyntax or lexical resources alone.  The word \i{rescued} only
carries the \i{rescued dog} backstory when used in a context
involving the dogs' eventual owners; in some context the more
generic meaning of \i{rescue} could supersede:

\begin{sentenceList}

\sentenceItem{} \swl{itm:boatmen}{Boatmen rescued dogs from the flooded streets.}{sem}
\sentenceItem{} \swl{itm:firemen}{Firemen rescued dogs from the burning building.}{sem}
\end{sentenceList}

Neither (\ref{itm:boatmen}) nor (\ref{itm:firemen}) imply that the dogs were abandoned, or
will have new owners, or be sent to a shelter, or that their rescuers are
members of an animal-welfare organization \mdash{} in short, no element of
the conventionalized backstory usually invoked by \i{rescued dogs} is present.
Analogously, there is no lexical subdivision for \i{latest} which regulates
the variance in time-frames among (\ref{itm:latest})-(\ref{itm:organization}).
It is only by inferring a likely situational background that
conversants will make time-scale assumptions based on one situation
involving dog ownership, another involving veterinary exams, and a
third involving animal-welfare rehabilitation.
}

\p{That is to say, the time-scale inference I have analyzed is essentially
\i{extralinguistic}: there is no specific \i{linguistic} knowledge
(lexical or grammatical, or even pragmatic inferences in the sense of
deictic or anaphora resolution) which warrants the situational classification
of (\ref{itm:latest})-(\ref{itm:organization}) into different time scales.
Instead, the inference is driven by (to some degree socially or culturally
specific) background-knowledge about phenomena like veterinary clinics
or animal rescue groups.  Whether or not the nuances in \i{rescued dogs}
are similarly extra-linguistic is an interesting question \mdash{} we can
argue that the phrase is now entrenched as a \i{de facto} lexical
entrant in its own right, so the role of \i{rescued} is not only to
lend adjectival detail but to construct a recurring phrase with a
distinct meaning, like \i{red card} (in football) or \i{stolen base}
(in baseball).  Lexical entrenchment is, I would argue, an
intra-linguistic phenomenon, in the sense that understanding entrenched
phrases is akin to familiarity with specific word-senses, which is a
properly linguistic kind of knowledge.  But even in that case, entrenchment
is only possible because the phrase has a signifying precision more
rigorous than its purely linguistic composition would imply.
There are, in short, extralinguistic considerations governing \i{when}
phrases are candidates for entrenchment, and a language-user's
ability to learn the conventionalized meaning (which I believe
is an intra-linguistic cognitive development) depends on their
having the relevant (extra-linguistic) background knowledge.
}

\p{If we consider then the contrast between transform-pairs like
\i{black dogs}, \i{rescued dogs}, and \i{latest dogs},
the similar grammatic constructions \mdash{} indeed similar semantic
constructions, in that each pair has an adjective modifying a
straightforward plural noun (\i{dogs} designates a similar
concept in each case; this is not a case of surface grammar
hiding semantic diversity, like \i{strong wine} vs.
\i{strong opinion} vs. \i{strong leash} or
\i{long afternoon} vs. \i{long history} vs. \i{long leash})
\mdash{} package transforms whose cognitive resolution spans a
range of linguistic and extralinguistic considerations.
Straightforward adjectival modification in \i{black dogs} gives
way to lexical entrenchment in \i{rescued dogs} which, as
I argued, carries significant extra-linguistic background knowledge
even though possession of this knowledge is packaged into basic
linguistic familiarity with \i{rescued dogs} as a signifying unit;
and in the case of \i{latest dogs} the morphosyntactic evocation of
temporal precedence and two different multiplicities (the latest
dogs and earlier ones) is fleshed out by
extra-linguistic estimations of time scale.
The same surface-level linguistic structures, in short, can
(or so such examples argue for) lead conversants on a
cognitive trajectory in which linguistic and extra-linguistic factors
interoperate in many different ways.
}

\p{This diversity should call into question the ability of conventional
syntactic and semantic analysis to elucidate sentence-meanings with
any precision or granularity.  Lexical and morphosyntactic
observations may certainly reflect details which \i{contribute} to
sentence-meanings, but the overall understanding of each sentence in
context depends on holistic, interpretive acts by competent
language users in light of extra-linguistic, socially mediated
background knowledge and situational understanding.  Contextuality
applies here not only in the pragmatic sense that pronoun resolution,
say, depends on discursive context (who is \i{her} in \i{her dogs});
more broadly, transcending even pragmatics, context describes
presumptive familiarity with conceptual structures like veterinary
clinics, animal shelters, and any other real-world domain which
provides an overall system wherein particular lexical significations
can be standardized.  Without the requisite conceptual background it
is hard to analyze how speakers can make sense even of
well-established variations in word-sense, like \i{treat} as in
a veterinarian treating a dog, a doctor treating a wound, a carpenter
treating a piece of wood, or how an actor treats a part.
These senses have lexical specificity only in the domain-specific
contexts of medicine, carpentry, theater, and so forth.
}

\p{The problem of holistic cognitive interpretation (as requisite for
sentence-meanings) can be seen even more baldly in examples where
semantic readings bifurcate in ways wholly dependent on
extra-linguitic conceptualization. Consider for instance:

\begin{sentenceList}

\sentenceItem{} \swl{itm:boroughs}{All New Yorkers live in one of five boroughs.}{sem}
\sentenceItem{} \swl{itm:commute}{All New Yorkers complain about how long it
takes to commute to New York City.}{sem}
\sentenceItem{} \swl{itm:Cambridge}{The south side of Cambridge voted Conservative.}{ref}
\sentenceItem{} \swl{itm:lower}{Lower Manhattan voted Republican.}{ref}
\sentenceItem{} \swl{itm:si}{Staten Island voted Republican.}{ref}
\end{sentenceList}

Sentence (\ref{itm:Cambridge}) is taken from the \i{Handbook of Pragmantics}
(example 40, page 379, chapter 15) which borrows in turn from Ann Copestake and
Ted Briscoe.  In the \i{Handbook} analysis (chapter by Geoffrey Nunberg),
(\ref{itm:Cambridge}) is seen as ambiguous between
reading \q{The south side of Cambridge} as an oblique description of
the \i{voters} in that territory or as topicalizing the territory
itself as a civic entity:

\begin{quote}On the face of things, we might analyze (40) in either of two ways: either the
description within the subject NP has a transferred meaning that describes a
group of people, or the VP has a transferred meaning in which it conveys the
property that jurisdictions acquire in virtue of the voting behavior of their
residents.
\end{quote}

The duality is significant because the designation in \q{south side of Cambridge}
would be more informal in the prior reading, both geographically
and in the how the implied collective of people is figured.  The prior
reading accommodates a hearing wherein the speaker construes
\q{south side} not as a precise electoral district (or districts) but as a
vaguely defined part of the city.  That imprecision also allows the claim
\q{voted Conservative} to be only loosely committal, implying that
some majority of voters appeared to vote Conservative but not that
this tendency is directly manifest in election results.  In short,
we can interpret (\ref{itm:Cambridge}) as making epistemically more
rigorous or more noncommittal claims depending on how we read
the geographical reference \q{south side of Cambridge} (as crisp
or fuzzy), the group of people selected via that reference
(mapping the region to its inhabitants, a kind of \q{type coercion}
since places do not vote), and the assertive force of the
speech-act: how precisely the speaker intends her claim to be
understood.  Each of these \q{axes} contribute to the sentence's
meaning insofar as they constrain what would be dialogically
appropriate responses.
}

\p{Meanwhile, in (\ref{itm:boroughs}), \i{New Yorkers} refers specifically to everyone who lives
in the City of New York, since the five boroughs collectively span the
whole of city.  In (\ref{itm:commute}), by contrast, we should understand
\i{New Yorkers} as referring to residents of the metropolitan area \i{outside}
the city itself (who commute \i{to} the city); and moreover \i{All} should
be read less then literally: we do not hear the speaker in (\ref{itm:commute})
committing to the proposition that \i{every single} New Yorker complains.
So both \i{All} and \i{New Yorkers} have noticeably different meanings in the
two sentences.
}

\p{And yet, I cannot find any purely linguistic mechanism
(lexical, semantic, syntactic, morphological) which would account for
these difference as linguistic signifieds \i{per se}: the actual differences
depend on conversants knowing some details about New York
(or, respectively, Cambridge) geography, and
also general cultural background.  It does not make too much sense to
commute to a place where you already live, so our conventional picture of
the word \i{commute} constrains our interpretation of (\ref{itm:commute}) \mdash{}
but this depends on \i{commute} having a specific meaning, of traveling
in to a city, usually from a suburban home, on a regular basis; a meaning
in turn indebted to the norms of the modern urban lifestyle (it would be
hard derive an analogous word-sense in the language spoken by a nomadic
tribe, or a pre-industrial agrarian community).  Likewise,
reading \i{All} in (\ref{itm:boroughs}) as \i{literally} \q{all}
depends on knowing that the five boroughs are in fact the whole of
the city's territory.  I am from New York, not Cambridge;
perhaps residents of the latter city would clearly read
\q{south side} as referencing a fixed civic\electoral area
(like \i{Staten Island}) or as only vaguely defined (like
\i{Lower Manhattan}).  For New Yorkers,
(\ref{itm:lower}) would be read as fuzzy and
(\ref{itm:si}) as fixed: the latter sentence has a clearly
prescribed fact-check (since Staten Island is a distinct
electoral district) which the former lacks.
}

\p{Given that in everyday speech quantifiers like \i{all} or \i{every}
are often only approximate \mdash{} and that designations like
\i{New Yorker} are often used imprecisely, with not-identical
alternative meanings intended on a case-by-case basis \mdash{}
these kind of examples point to signifying ambiguities that
can easily arise as a consequence.  Often extra-linguistic
considerations resolve the ambiguity by rejecting one or another
(otherwise linguistically plausible) reading as non-sensical.
Consider:

\begin{sentenceList}

\sentenceItem{} \swl{itm:beat}{The Leafs failed to beat the Habs for the first time this year.}{amb}
\sentenceItem{} \swl{itm:consecutive}{The Leafs failed to win two consecutive games for the
first time this year.}{amb}
\sentenceItem{} \swl{itm:goal}{The Leafs failed to score a goal for the
first time this year.}{amb}
\end{sentenceList}

Sentence (\ref{itm:beat}) has two competing readings: either the Toronto Maple Leafs
won \i{all} or \i{none} of their previous games, in the relevant year, against the
Montreal Canadiens.  The difference is whether \i{for the first time this year}
attaches to \i{beat} or to \i{fail}.  In (\ref{itm:consecutive}),
on the other hand, the only sensible interpretation is that the Leafs had
not yet won two games: while it is logically accurate to describe a
team on a long winning streak as repeatedly winning two consecutive games, it would
be very unexpected for (\ref{itm:consecutive}) to be used in a case where the Leafs
lost for the first time, after a three-plus-game winning streak.
And in (\ref{itm:goal}) any hockey fan would hear that the Leafs had scored
at least one goal in all prior games; even though there is no linguistic rule
foreclosing the reading such that the Leafs have not scored a goal in \i{any} game.
}

\p{These variations \mdash{} the degree to which superficial ambiguity is actually
perceived by competent language-users as presenting competing plausible
meanings \mdash{} depend on background factors; the contingencies of
hockey fix how potential ambiguities resolve out because one or another
alternative is extralinguistically incoherent.  But these cases point to
how linguistic criteria alone, no matter how broadly understood,
cannot necessarily predict in what sense linguistic structurations
have empirically plausible meanings \mdash{} or whether they have
sensible meanings at all.
}

\p{Notice however that all these examples have alternate versions which are less
subtle or ambiguous, which shows that the complications are not
localized in the communicated ideas themselves, but in their
typical linguistic encoding:

\begin{sentenceList}

\sentenceItem{} \swl{itm:allresidents}{All residents of the city of New York live in one of five boroughs.}{log}
\sentenceItem{} \swl{itm:manyresidents}{Many residents of the New York metropolitan area
complain about how long it takes to commute to New York City.}{log}
\sentenceItem{} \swl{itm:SouthCambridge}{All districts on the south side of Cambridge voted Conservative.}{log}
\sentenceItem{} \swl{itm:leafshabs}{For the first time this year, the Leafs failed to beat
the Habs.}{log}
\end{sentenceList}

These versions are more logically transparent, in that their propositional
content is more directly modeled by the structure of the sentences.
Indeed, hearers unfamiliar with New York (respectively Cambridge)
or with hockey might find these
versions easier to understand; more context-neutral and journalistic.
But perhaps for this reason the \q{journalistic} versions actually
sound stilted or non-idiomatic for everyday discourse.
}
