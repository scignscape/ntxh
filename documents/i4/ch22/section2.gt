
`section.Integrating Virtual Machines with Image-Processing Operations`
`p.
This chapter's discussion has thus far focused on topics, 
such as two-coordinate data types and enumerations, which 
are only tangentially related to image-processing.  
The current section will focus on images more specifically.  
When considering `VM; `q.integration` with image-processing, 
I refer primarily to setting up Virtual Machines with 
inherent functionality related to invoking Computer Vision 
algorithms.  Presumably many `VM;s will implement 
a general-purpose Foreign Function Interface would would 
support calls to native-implemented graphics-related 
procedures (along with any other high-throughput 
domain, where highly optimized code in languages such as 
`C; or `Cpp; is preferable for performance reasons).  
In the current context I intend to focus however 
in `VM; which are explicitly oriented toward 
image-procesing, where this specific problem-area 
is a central design emphasis instead of one group 
of foreign-function capabilities amongst others.  
This means that the `VM; model will incorporate features 
specifically relevant to image.  For example 
%-- as already discussed last section %-- representations 
of image locations and intra-image lengths, in 
the form of distinct numeric-pair types, could 
be incorporated as `q.kernel` data types for 
the relevant `VM;.  This is a straightforward example 
of how basic choices for `VM; design could be 
influenced by image-related use cases.  I will present 
other examples below.
`p`

`subsection.Exposing GUI Functionality`
`p.
As outlined in Chapter 20, a central focus of 
`VM; functionality in these chapters involves 
applications `q.exposing` features for 
third-party components or plugins; within this 
context an important area of functionality 
attends to `GUI; state.  In computer vision, a 
straightforward example of `GUI; interop would 
be showing intermediate stages of an 
image-processing pipeline.  The `OpenCV; library, 
for example, provides an `b.imshow` procedure 
that displays an image in its `GUI; window.  
Typically this procedure is called in conjunction 
with specific `OpenCV; algorithms in order 
to visualize those algorithms' outputs, 
particularly while the pipeline is 
initially being formulated and fine-tuned.  
For example, the algorithm in question 
might be a feature-detector which looks 
for `q.keypoint` locations (that match a 
specific point in an image), straight lines 
(matching extended, relatively crisp edges or 
visible lines), contours (matching 
closed curves, considered to enclose a 
`TwoD; area) or `q.superpixels` (relatively 
small image-regions of large homogeneous 
interior color, also `TwoD;).  These alternatives 
correspond to zero-dimensional, one-dimensional, 
and two-dimensional features, respectively.    
`p`


`p.
In most scenarios feature-detection is 
not intended to transform an image or 
to produce intrinsically visual data; 
instead a set of keypoints, lines, contours, 
or segments/superpixels is subject to 
further statistical analysis.  For 
example, `q.landmark` keypoints 
may be used for image registration 
%-- aligning multiple images 
(`MRI; scans, say, or photos taken 
with a head-mounted camera that 
are stiched together to form `ThreeSixty; 
`q.panoramic` tours) by locating 
pre-identified keypoints common to 
all images in a series.  Landmark 
keypoints may also be used for 
segmentation (by connecting certain 
keypoints to form polygons outlining 
or containing a Region of Interst). 
Whether via this technique or 
others (e.g., superpixels or closed contours) 
segmentation can isolate a desired 
foreground region and analyze its 
color and/or shape (metrics such as 
its color average, or the eccentricity 
of an ellipse containing the region) 
which in turn may yield 
object-classification or other 
`q.semantic` interpretations of 
image-data %-- consider software 
which records traffic patterns by 
isolating visuals of cars in the 
context of street cameras. 
`p`


`p.
Image-data generated via this kind of 
pipeline is typically entered into a 
database or in some other manner 
tracked analytically, so insofar 
as human users access these results it 
would be in the form of spreadsheets 
or other structured, largely textual 
formats.  However, when initially 
formulating a Computer Vision pipeline 
engineers often need feedback concerning 
intermediate processing stages, so 
algorithms typically feature-detectors 
can be paired with operations 
to produce secondary images visually 
sumarizing the algorithm's performance.  
Keypoints, feature-lines, contours, and 
superpixels may be visualized by 
drawing markers at key-point locations 
or highlighting lines and superpixel/contour 
boundaries with pronounced colors 
(distinct reds, yellows, light blues, 
and so forth, selected to avoid confusion 
with actual colors in the image).  These 
graphics are then overlaid on the 
original image to represent an 
algorithm's results pictorially.  Such 
intermediate images are not analyzed 
themselves (they are not technically 
part of a pipeline) but provide feedback 
(many segmentation and feature-detection 
algorithms, for example, have multiple 
parameters that can be adjusted, and 
programmers will often experiment 
with different settings in the context 
of a sample image for which the 
desired analysis is known ahead of 
time, so summary visuals signal 
how well the algorithm matches 
that goal).  In other contexts, 
intermediate graphics that `i.are` 
part of a workflow proper 
%-- such as blurred/simplified or grayscale  
images analyzed in lieu of an 
initial picture, so as to 
reduce noise or allow for 
restricted-channel analysis %-- might 
likewise be previewed during 
development, to help programmers 
conceptualize how the pipeline operates.  
`p`


`p.
In short, an intrinsic demand of 
Computer Vision components involves 
rendering intermediate images 
visually even if those visuals 
are not analytically part of 
a workflow.  Insofar as image-processing 
capabilities are part of a full-fledged 
desktop-style application, Computer Vision 
routines can leverage `GUI; features 
to show intemediate images, potentially 
with interactive capabilities allowing
users to examine results in greater
detail.  For example, keypoints might 
be represented on images via boxes that receive 
mouse-events independently from the 
larger picture (recall the `b.QGraphicsScene` 
comments from Chapter 20), providing 
for instance context-menu options 
specific to one keypoint.  In `OpenCV;, 
each keypoint detected via `SIFT; 
(Scale-Invariant Feature Transform) 
and similar computations have 
multiple corresponding parameters 
reflecting their surrounding 
image-context (these data structure 
allow keypoints occupying the same 
ground-truth location to be tracked 
among multiple images, e.g. for 
registration: we can 
compile key-point sets for two 
images and look for points with 
similar statistical profiles, 
as if matching fingerprints; 
scale- and rotation-invariant keypoints 
in particular will have analogous 
profiles in multiple images whose 
contents overlap in some area of 
physical space).  Keypoint 
attributes are an example of how 
visualizing Computer Vision 
workflows may involve interactive 
displays with more functionality  
than merely graphings images 
decorated with feature-depicting 
annotations (like highlights 
for feature-points and 
superpixel-boundaries).   
`p`


`p.
This example also points to the 
potential complications involved in 
applications `q.exposing` 
`GUI; functionality for image-processing.  
In the case if `i.previewing` 
an intermediate-stage graphic 
%-- suppose an algorithm 
draws feature-annotations on a 
copy of the original image and 
saves that version to a temporary 
file %-- a host application might 
need to offer only a relatively 
simply function to show the 
temp file in a picture-view window.  
Data for that operation may involve 
nothing more than a file path 
for the desired image.  Once the 
presentation inolves context menus 
and intra-image interactions, however, 
the interop between a `GUI; host 
and an image-analysis (semi-autonomous) 
component becomes more elaborate.  
In a context such as examining 
keypoints, the front-end would need 
access to a data set of keypoint 
attributes alongside the underlying 
image, plus instructions on how 
to populate context menus over 
keypoints and over generic image 
points, respectively.  Context menu 
options might furthermore be 
linked with procedures available 
in the `i.analytic` component, 
so the front-end would need 
to register `q.callbacks` 
deferring functionality back to 
the client; there would 
be a bidirectional conduit for 
metaprocedure invocations 
(recalling terminology from 
Chapter 20) instead of a 
client requesting one host 
feature (as would be the 
case with showing an 
intermediate image 
non-interactively).  In short, 
both components would need 
to expose respective 
functionality according to a 
mutually-implementable protocol.  
For reasons I have discussed, situations 
where components 
`q.expose capabilities` along these 
lines are a good example of 
leveraging Virtual Machines 
to model, fine-tune, or dynamically 
extend data-sharing protocols. 
`p`

`p.
As a general rule, Computer Vision 
algorithms are largely self-contained, 
in that code needed to run the actual 
mathematical analyses %-- once an 
image for processing is loaded into 
memory %-- may be grouped together into a 
single library without often 
calling external procedures.  On the 
other hand, image-processing workflows 
will typically need to defer to 
host applications for file-paths 
to load images in the first place 
(or some other resource-reference, such as 
handles into an image database) as well 
as (discussed in the prior paragraphs) 
for visualizing analytic results.  
Because of the sheer diversity of 
image-analysis methods, applications 
should incorporate Computer Vision in a 
flexible manner, with the option of 
adding new kinds of analysis over 
time (partly because different methods 
are better suited for different 
image series, depending on their 
coloration, occlusion effects %-- and 
in general how crisply pictures 
show foreground/background boundaries 
%-- textural complexity, and similar 
variables that can affect the 
accuracy of a pipeline).  In this sense 
we can speak of `q.exposing` functionality
in two directions; image-processing 
components share their analytic capabilities, 
while host applications expose `GUI; and 
filesystem routines, and so forth. 
`p`

`p.
The prior discussion has considered interop from 
the host application's point of view, but I will 
now transition to image-processing components 
themselves.  As a case-study I will consider 
the `qXCSD; format which was formulated 
partly in conjuction with developing material 
for these chapters.  This discussion will 
indicate the rationale for `XCSD;'s approach to 
color-processing, memory-layout, and other 
implementation details, but this is not a technical 
documentation of `XCSD; intimating that this is 
necessarily a superior representation or paradigm.  
Instead, this chapter employs `XCSD; mostly as a 
case-study for implementing new image-processing 
functionality in a pre-existing host-application 
environment.   
`p`

`subsection.Extensing Host Applications with Image-Processing Workflows`
`p.
As is implicit in this discussion, the form of connections
between applications and image-processing components which I will 
emphasize here involve hosts `i.calling` Computer Vision 
algorithms provided by external libraries, perhaps 
mediated through Virtual Machines.   
A different notion of `q.overlap` between `VM; and 
image-based engineering would be dedicated `VM;s 
for `i.implementing` Computer Vision algorithms, 
analogous to how Graphics Processing Units 
provide computing environments tailored to 
concerns such as ray-tracing and other 
graphics-rendering calculations.  Designs for 
compiling image-processing code `i.to` special-purpose 
`VM;s, which are optimized for running such 
algorithms very efficiently, constitutes a 
separate topic, which is outside the 
focus of this chapter (except indirectly 
since calling conventions within such 
algorithms might overlap with calling conventions 
for initiating image-processing workflows or 
workflow steps, the latter topic fitting more 
within the bounds of invoking Computer Vision 
capabilities externally from a `VM; context). 
`p`

`p.
Image-analysis functionality can be grouped into 
several areas, such as contour-detection, region morphology, 
texture-description, diffusion analysis, and 
methods related to color statistics (e.g., color histograms). 
For purposes of exposition, this section will discuss 
algorithms related to the `XCSD; format mentioned 
earlier.  The basic focus of `XCSD; concerns how 
pixel-data is stored in computer memory; in particular, 
pixels are lined up into groups at different `q.tiers`/, 
or nested levels of detail.  The `XCSD; code 
accompanying this chapter supports three tiers, based on 
powers of three; so `threebythree; pixel-boxes are 
grouped together, as are `ninebynine; and 
`twentysevenbytwentyseven;.  This structure is 
noticeably different from conventional `q.matrix`/-based 
image formats, where one entire row of pixels (spanning 
the whole image-width) would typically be coded 
in memory as one pixel-run (sometimes called a 
`q.scanline`/), followed by each successive row 
of pixels in turn.  This representation is of little 
intrinsic value, however (apart from simply recording pixel-data; 
that is, the `i,organization` of this value in memory 
adds no further benefit) unless one is working 
with sub-images spanning the whole distance from left to 
right margins.  In `XCSD;, pixels are stored in memory 
such that `threebythree; boxes represent one contiguous 
chunk, and can be accessed via raw pointers (rather than 
copying from scanline-based data to temporary buffers); 
then, on higher tiers, the same applies to 
`ninebynine; and `twentysevenbytwentyseven; groupings.      
`p`

`p.
At the uppermost scale on this `q.tiered` system, the 
`twentysevenbytwentyseven; boxes are called `q.tierboxes`/, 
which are aligned in memory from the center of the 
image outward.  The `XCSD; code uses calculations based 
on the Manhattan-Chebychev hybrid distance representation 
(mentioned earlier) to arrange tierboxes such that 
contiguous pixel-runs correspond to meaningful 
image regions, specifically, diamonds, octagons, 
squares, and rectangles centered on the overall 
image-center.  This provides a rationale 
for tierbox memory-layout, clarified below in more 
detail.  `i.Within` each tierbox, pixel-runs are 
stored on `ninebynine; and `threebythree; scales.  
The goal is to increase the likelihood that a 
particular cluster of pixels of analytic interest 
can be accessed as a raw pixel-run (via pointers 
to start and end memory) without needed multiple 
copy operations.  Also, `XCSD; allots memory 
space for data associated specifically with 
boxes on each level, from `threebythree; to 
`twentysevenbytwentyseven;.  This allows 
algorithms to consider the image on coarser-scales, 
avoiding features on the individual-pixel level 
that could be statistical `q.noise`/, as desired.  
Pre-computed values such as color-averaged 
can be recorded in the image data along side 
pixel-level information itself.       
`p`

`p.
The multi-level representation just described 
gives rise to the notion of `q.subdivision` indexing, 
wherein individual pixels can be access via their 
location in nested three-by-three groupings rather 
than absolute coordinates.  Such indexing is useful 
in contexts where we may want to examine 
image-features present at different level of detail; 
shifting focus from the pixel-tier to the 
`threebythree;, `ninebynine;, or tierbox levels 
simply involves dropping subdivision-indexing coordinate.  
Also, `XCSD; allots space for encoding 
image-data of varying provenance (e.g., texture codes 
or region-of-interest masks) alongside channels, 
both at the pixel level and higher tiers (referred to 
as `q.extension` channels).  The `XCSD; format can therefore 
record analytic data internally, such as masks 
on different tiers placing units on that tier 
within (or on the border of) one or more regions-of-interest.
`p`

`p.
The last two paragraphs have explained some featuers 
and rationaled for `XCSD;, and mentioned the 
origin of concepts like `q.extensible` channel-systems 
and subdivision indexing, reflected in the format's 
name.  The purpose of this chapter is not to 
analyze `XCSD; itself in detail, but rather to 
adopt it for case-studies in `VM; integration 
with image-processing.  I will do this through 
the lens of algorithms associated with `XCSD;, 
which hopefully serve as representative examples 
of how `VM;s can interface with algorithms 
related to image-formats in general.  
`p`

`subsection.Manhattan/Chebychev distances and `q.Black-Grey` grids`
`p.
Earlier I mentioned that `XCSD; uses a `q.hybrid` combining Manhattan 
and Chebychev distance metrics.  For the sake of discussion, I'll 
call this combination an `MCH; distance-representation 
(not a metric, `i.per se`/, because the result is a two-valued 
pair rather than a scalar).  Formally, the `MCH; pair between 
`xypair; and `abpair; is `xyabMCH;, where `mathM; and `mathC;  
are the Manhattan and Chebychev distances (see the formulas 
from last section).`footnote.
Actually, `XCSD; uses a slightly different pair I'll 
call `MCHprime; equaling `MCHprimepair;, but this 
text's version of `smMCH; is more intuitive for exposition.  
They are readily interconvertible: `MCHprimecovert;.
`footnote`  Visually, the `MCH; merges how the 
two component metrics `q.count steps`/: whereas the Manhattan 
counts only orthogonal steps, and the Chebychev freely counts 
`i.either` orthoginal and diagonal steps, the `MCH; 
represents a path `i.first` along a diagnoal and then, 
`q.turning` by 135 degrees, orthogonal steps outward.  
The raw-number pairs can be supplemented with 
one of eight directional indications as needed 
(between two points the diagonal-then-orthogonal 
path can have four initial direcitons and two 
possible further directions for the orthogonal 
component).  Picture `MCH; pairs as the 
sum of a diagonal vector (sloping 
at 45 degress from the relevant line) and 
end-to-end with an orthogonal vector.      
`p`


`p.
In order to document the relevant mathematical 
properties of `MCH; I will introduce a couple 
of auxiliary constructions related to discrete 
geometry.  The main results I am leading uo 
to are smmarized in Table 1 with respect to 
finding locations equidistant from some point.  
The math involved here is barely above grade-school 
level, but there is some combinatorial trickiness 
in fully enumerating all possibilities for 
distance comparisons, so this  
presentation hopefully does not seem more technical 
than the subject warrants.  
In particular, it is significant to contrast 
locations (especially in image-processing) which 
represent `i.pixels` themselves and which 
represent boundary-points `i.between` pixels; 
this comment would then generalize to overall 
image-regions.  If a picture has odd pixel-sizes  
both horizontally and vertically, the exact center 
is one pixel; otherwise, it is the gap `i.between` 
two pixels (for an even/odd mixture) or 
four (in the both-even case).  This motivates 
the following nodification to the notion 
of discrete geometry on an integer grid or lattice:

`anondefin.
Call a `q.black-gray` grid a lattace with lines 
grouped into two classes, `i.black` and `i.gray`/, 
where each horiztonal (respectively, vertical) 
black line is surrounded by two gray lines, and 
vice versa (i.e., the two colors are interspersed).  
The `q.points` on such a grid would lie at 
intersections between lines, and given the 
color-differences there are four kinds of 
points: double-black (intersections of two 
black lines), gray-black, black-gray, and 
double-gray (reading the horizontal color 
first, then vertical).  Unless 
stated otherwise, I will refer to `MCH; distances 
as comparisons between two intersection-points on a 
black-gray grid.  In this context `q.points` discussed 
without further qualification mean `q.grid points`/, 
i.e., intersections between grid lines.
`anondefin`

The purpose of this kind of grid is to address 
certain mereogeometric or meretopological situations 
where distances can combine entities of differing 
dimensions.  Consider a checkboard pattern; imagine 
placing chess pieces either `i.inside` board squares 
(corresponding to double-black grid points), 
or along edges `i.between` squares (gray-black mixed) or 
at corners where four squares intersect (double-grays).  
The black-gray construction  distorts actual geometry 
(if we picture gray and black as evenly spaced lines then 
colors do not affect the dimensions of points or 
lines involved, so the mereogeometric interpretation 
is not visually implicit, but the point 
of black-gray grids is for discrete geometric properties, 
such as distance, which do not correspond to Euclidian 
space anyhow).  I am particularly focused on the following:

`anondefin.
Call an `MCH; `i.cycle` on a black-gray grid to be a collection 
of double-black points equidistant, by some specific value, from a central 
point, within the context of a rectangle (possibly a square) 
centered on that point.  If the stipulated center-point 
is not double-black, calculate the shared `MCH; distance 
according to the double-black point adjacent to  
the center (either two points alongside, for black-gray or vice-versa, 
or four points around, for double-gray) which is nearest to 
each candidate double-black point.    
`anondefin`

Note that two `MCH; pairs are `q.equal` if both components 
are equal (they may differ in the vector-directions).  
If two `MCH; pairs are `i.not` equal, there is no 
way for them to represent the same `i.Euclidean` 
distance (assuming we treat the diagonal  
and orthogonal vectors as immersed on a 
Euclidean plane) so `MCH; provides a 
reasonable alternative to Euclidean distances 
with respect to ordering point-pairs in terms 
of their respect distances, as mentioned 
last section, or grouping those representing 
the same Euclidean (and therefore `MCH;) distance.


`anonobservation.
The `MCH; and Euclidean distances are similar with respect 
to ordering point-pairs into lesser, equal, or greater 
internal distances.   In particular, (i) for two `MCH; distances 
which internally sum to the same number of steps, the 
`MCH; and Euclidean orderings coincide.  Also, (ii) strict 
equality between distances matches between both representations, 
and (iii) if one `MCH; coordinate is held constant, or if both 
either simultaneously increase or descrease, the 
changing coordinates will increase or decrease with Euclidean distance.
`anonobservation`

`observationproof.
Suppose we have a normal (one-color) grid of evenly-spaced 
orthogonal lines separated by a fixed unit, 
so we can assign them integer coordinates.  `input<mcheuclidproof>;
`observationproof`

`noindent;In short, `MCH; is a reasonable replacement for Euclidean distance 
in many contexts because there are few situations where point-pairs 
have greater `MCH; but less Euclidean distance, or vice-versa 
(as shown by the above proof, the only scenario where 
the two representations differ in this sense is that 
an `MCH; pair which sums to fewer steps, but has 
greater diagonal component %-- i.e., `kgtzero; from (5) 
in the proof %-- may yield a greater Euclidean length 
than an alternative `MCH; pair with more steps but less diagonal).
Calculations such as those related to `MCH; cycles 
are not affected by those discrepancies.

`anonobservation.
An `MCH; cycle can have 1, 2, 4, or 8 points. 
`anonobservation`

`observationproof.
Every point in an `MCH; cycle will have some pair 
`diagorthopair; shared by all points, which will differ 
by direction.  Since there are eight possible 
directions attributable to each pair, a cycle 
can have eight different points.  We then have 
to identify cases where cycles will have 
`i.fewer` points.  Note that if either 
or both coordinates in the `MCH; pair are zero, 
then the distinction between diagonal and/or 
orthogonal directions goes away, eliminating 
some points.  The 1-point case corresponds 
to zeros for both diagonal `i.and` orthogonal 
components on a double-black center (so there 
is only one point, the center itself).  
A double-zero on a black-gray or gray-black center  
represents a length-two cycle (because 
we consider the two double-black points around 
the center) while a double-zero `MCH; for 
a double-gray center engenders a length-four 
cycle.  An `MCH; with one zero and one nonzero  
component can have four directions (since both a 
nonzero diagonal and an orthogonal 
with `i.no` diagonal supports four) and therefore 
represent four distinct points in a cycle 
(this is another length-four case).  Moreover, 
for any `MCH; with nonzero `i.orthogonal`/, 
it is possible that half of the points in a full 
cycle (within a sufficiently large rectangle) 
are excluded because they lie outside the 
bounds of the actual rectangle associated with the 
specific cycle.  
`observationproof`

Note that there are several factors influencing 
the length of an `MCH; cycle: the dimensions 
of the bounding rectangle (and the degree to 
which one side is longer than another, causing 
some cycles to `q.lose` points); whether 
the center is double-black, double-gray, or a 
combination; and the presence or absence 
of zeros in the desired `MCH; distance.  An 
orthogonal zero, for example, results 
in four points along two diagonals without an 
additional orthogonal projections which could 
extend either horizontally or verticlly; 
as a result, the cycle can have only four 
(not eight) points, but also it cannot be 
contracted due to width/height discrepancies 
in the bounding rectangle. 
`p`


`p.
The reason why `XCSD; employs `MCH; cycles is to 
itemize tierboxes which are equidistant from 
the image center.  As mentioned earlier, 
`XCSD; computes a memory-layout according to 
which tierboxes closest to the center 
occupy lower memory addresses, with pixel 
data encoded in contiguous raw memory 
expanding outward from that center.  
This `q.expansion` is understood to 
progress through `MCH; cycles.  
The `MCH; format has a intrinsic ordering 
based first on the total length (adding 
orthogonal and then diagonal) and then, 
for paths with the same sum result, 
treating paths with fewer `i.diagonal` steps 
as shorter (given that diagonal steps 
are longer from a Euclidean perspective).  
Given any collection of points around a center, 
then we can partition the set into 
`XCSD; cycles which have a natural 
ordering between one another; moreover, `i.inside` 
each cycle the points are distributed clockwise 
or counter-clockwise (since the basis 
for separating points is alternative 
directions superimpose on a single directionless
`MCH; pair).  The end result is a coherent 
algorithm for ordering points subject 
to constraints that points nearer to the center 
should be placed before those further away.
`p`

`input<table1>;
`p.   
For `XCSD;, the `q.points` are actually 
tierboxes (or gaps between them), but 
this is consistent with black-gray grids 
empbodying mereogemetric relations 
through discrete/integer mathematics, 
without the grid being a faithful 
`i.representation` of the modeled 
space's actual (Euclidean) geometry.  
In short, the algorithms I have described 
in the black-gray context work also 
for the practical task in `XCSD; of 
deriving the proper memory-layout 
for image tierboxes (and, by 
extension, individual pixel runs).  
`p`


`p.
One rationale for memory-layout oriented to an 
image-center is to increase the likelihood 
that the pixel-data for an image-region of 
interest can be obtained simply via pointers 
to the start and end of a memory-block 
(without needing multiple copies into a 
temporary buffer).  Due to the nature of 
`MCH; cycles and ordering, a contiguous  
pixel run (assuming it is aligned to tierbox 
boundaries) would take on a diamond, octagon, 
or square shape around the image-center 
(or potentially a rectangle, if the 
length of one side of the demarcated region 
matches the shorter image dimension).  
While many significant image-regions of 
course will not be centered on the 
full image's center point (or point-gap), 
there are still situations where focusing 
on pixels closer to the overall center 
is desired.  For example, signals 
extracted via image-analysis are more likely 
to be significant for labeling, classifying, or 
matching images when they emerge from an 
image's central region more than its periphery.  
The color spectrum or histogram weighted toward 
the center typically bears more signifance 
than further from the center, for example, 
and similarly for prominent contour-shapes, 
textures, diffusion processes, and so forth. 
`p`


`p.
Also, more simplistically, thumbnail or 
preview image-summaries might need 
to strip away peripheral content, 
even if the image is also scaled 
down.  This points to potential 
benefits of center-orienetd memory layout.  
Copying pixels (even multiple pixel-runs, 
e.g., one per scanline) is not a very 
time-consuming operation, especially in the 
context of full-scale image-analysis, where 
executing Computer Vision algorithms could 
easily take much more time than setting 
up an initial pixel-matrix.  However, 
some image-processing may occur in contexts 
such as image-databases where performance 
delays may noticeably affect usability, 
in context where it could be necessary to 
work through hundreds or thousands of 
images in response to one user query/action.  
In these kinds of contexts, avoiding 
even the relatively minor step of 
pixel-copying can improve performance, 
in situations where one wants to 
construct thumbnails for hundreds 
of pictures (matchng a query, say), 
or perform basic analysis on a 
large image-series.     
`p`


`p.
For these reasons `XCSD; is designed as a format 
particularly suited to hosting images in a database, 
where basic operations such as image-compression, 
calculating dominant colors, and building 
image-thumbnails can be highly optimized.  
It's also true that the `MCH; representation, 
which I have described here in the context of 
memory-layout, can have other benefits.  For 
some analytic algorithms, for example, 
having a near-Euclidean distance formula 
on an integer grid may be useful.  Furthermore, 
memory-organization based on `MCH; cycles 
includes (partly to calculate the layout 
ordering in the first place) numerous 
data-points applicable to each 
tierbox, such as distance and orientation 
against the image center, information 
which may have some meaning in certain 
Computer Vision contexts.  For example, 
the weight given to some color, texture, 
or diffusion scale within the bounds 
of a given tierbox %-- or perhaps as 
compared between tierboxes %-- might be affected 
by the tierboxes' distance from center, or from 
image central-orthogonal or diagonal axes, 
or angular distance from some 
analytically significant line (consider a 
trendline through control points suggesting a 
foreground region).  Some contexts 
may prefer angular distances measured 
via integers (e.g., via `MCH;) rather than 
Euclidean floating-point approximations.     
`p`


`p.
These points are suggestive of potential 
`MCH; use-cases; my goal here is not 
to advocate for `MCH; `i.per se`/, 
but simply to justify the claim that 
this is the `i.sort` of construction 
that we might want to engineer 
in an image-processing context.  
I therefore claim it is a reasonable 
example for discussions within 
image-analysis contexts.  That is, 
I propose to use `MCH;-cycle ordering 
and related algorithms as case-studies 
for `VM; engineering which 
encapsulates Computer Vision capabilities 
(or an interface to them).    
`p`


`subsection.XCSD operators as representative image-processing functions`
`p.
The above discussion has hopefully motivated some details 
concerning algorithms specific to 
`XCSD;.  In particular, the ordering 
of tierboxes around an image-center is 
determined by `MCH; cycles for progressively 
larger `MCH; values.  These and related calculations 
give rise to a series of functions giving 
basic quantitative information about tierboxes, such as:

`description,
`item ->> Translating tierboxes from row/column coordinates 
to `MCH; pairs ;;  This results in a measurement of the 
basic distance of the tierbox from one central tierbox 
%-- 
or one of two or four central tierboxes, depending 
on even or odd tierbox row and column counts 
(the even/odd details correspond to gray/black, 
black/gray, or double-gray cases reviewed above 
for `MCH; cycles on black-gray grids).

`item ->> Mapping index numbers for a desired center-originating 
ordering of tierboxes to `MCH; values ;;  Such an ordering 
can be seeded by expanding outward according to Chebychev 
distance, forming equivalence classes of Chebychev-equidistant 
tierboxes.  Within each such class, `MCH; distances expand 
outward from rows and columns passing through the image 
center (or centers; again, tracking even-odd effects as 
with black-gray grids) toward diagnols.  This algorithm 
has to be adjusted for the case where the Chebychev distance 
is greater than half the shorter image dimension 
(in terms of tierbox-counts) so that the `MCH; cycle 
is restricted to the image's longer axis.

`item ->> Calculating tierbox positioning within each `MCH; cycle ;;  
The choice of how to order tierboxes `i.within` one cylce 
seems mostly arbitrary.  In `XCSD;, I chose to 
order counter-clockwise from the left (for images with 
landscape orientation) or top (for those with portrait 
orientation, i.e., height greater than width).  My 
reasoning was that if an image `i.were` to be chopped from 
only one side, it may be slightly more likely 
to cut from the right or from the bottom, given 
that our vision tends to be oriented left-to-right 
and top-to-bottom; in other words, we are inclined to 
perceive a foreground focus leading from the center top/leftward 
more than the opposite, increasingly the possibility 
that it will be peripheral content toward the bottom/right that 
is deemed expendable, if the image is chopped.  Shrinking an 
image by deleting tierboxes toward the end of contiguous 
memory requires no copying data, of course, which 
provides a rationale for placing more-likely-expandable 
tierboxes later in memory than earlier.   
  
`item ->>  Identify tierbox position within its `MCH; cycle 
in terms of image-directions and octants ;;  In general an 
image can be divided into eight wedge-shape slices 
(i.e., octants) and members of a cycle for `MCH; without zeros 
will lie in one such octant (those `i.with` zeros may 
lie on the lines between octants, e.g., diagonals).  This is one 
example of an algorithm utilizing the `q.direction` enumeration 
I mentioned in the last section.  As explained there, `XCSD;'s 
notion of direction has 24 options, corresponding to 
length-8 and length-4 cycles (the latter representing 
pure diagonals) and then distinct codes for pure orthogonals 
(`MCH;s with diagonal component zero) distinguishing 
even-odd permutations at the center point (as modeled 
by different gray/black grid combinations).  Code 
needs to translate formulae expressing how different 
`MCH; pairs translate to one of these four direction-codes 
(this furnishes an example of where enumerations need 
precise numeric values, mentioned last section, because 
these algorithms make some use of modulo and bitshift 
operations when `q.collapsing` the 24 directions to 
accommodate different `MCH; cycle-lengths).   

`item ->>  Map `q.subdivision indexing` to orthogonal coordinates ;;  
Inside tierboxes, `XCSD; recognizes indexes accross 
`threebythree; and `ninebynine; tiers, mirroring the 
actual memory layout.  For example, the center of each 
tierbox is located via code `q.555`/.  Obviously, sometimes 
it is convenient to access points or box-regions in a more 
conventional x/y or row-column format, so one requisite 
calculation is converting subdivision location codes 
to orthogonal distances against tierbox corners and 
those of the image overall.

`item ->>  Compute color averages within box-regions of different 
tiers ;;  One rationaly for the subdivision system is to 
establish data structures for `threebythree; and `ninebynine; 
boxes (as well as tierboxes) which proxy the collection of 
pixels within them.  If an image is compressed by a factor 
of three, of course, pixels `q.collapse inward` so `threebythree; 
regions `i.become` pixels on the smaller scale.  Given a desired 
color-averaging procedure, `XCSD; precomputes color means 
so that `q.compressed` images are represented within the larger 
image's memory, in case algorithms intend to work with 
the smaller-scale images directly, or to analyze on several 
scales at once.  Apart from color-averages, data from 
different sources (e.g., region masks) can be attached as 
an `q.extension channel` on multiple scales.  For example, a 
numeric code designating one region of interest (in the 
sense that a given pixel or box-region lies on that region's 
interior) can be notated as a channel for individual 
pixels, or boxes on the `threebythree;, `ninebynine;, or 
`twentysevenbytwentyseven; tiers, or any combination 
thereof.  The same applies to codes representing 
textural patterns, annotations, or any other superimposed data.    

`item ->> Applying algorithms to sets of tierboxes 
(or other region-boxes) ;;  There are numerous 
scenarios where subdividing images into smaller regions can produce 
more efficient algorithms.  This includes pixel-based 
computations (e.g., global color histograms) where pixels' contexts 
`visavis; surrounding pixels is not considered, so images 
can be subdivided simply for purposes of parallelization.  
Other analyses %-- such as those involving `i.local` color 
histograms %-- explicitly employ image-subdivisions as a 
unit of computation.  Finally, some forms of image-processing 
are based on a lattice of `q.seed` points, which in a 
`q.subdivision indexing` scenario can readily be 
provided as the centers of tierboxes, or `threebythree;s/`ninebynine;s.
`description`
`p`

`subsection.An example image-processing pipeline`
`p.
Having defined several operations that could be exposed 
by an XCSD-based image-processing component, 
the following discussion will outline how various 
functions could be pieced together into a 
larger workflow.  This summary will point to 
steps in the pipeline wherein data-sharing 
between the XCSD component and a host application 
might be warranted, thereby serving as 
concrete examples of how each side could 
expose feature for interoperation.
`p`


`p.
One premise of `XCSD; is that conventional formulae 
for converting images to grayscale are often 
flawed.  The rationale for analyzing `q.grayed` 
versions of images %-- rather than the full-color 
originals %-- is that most statistical analyses 
rely on color-discontinuities.  Analyses look 
for points in an image where there is an above-average 
gap between two adjacent pixels.  Discontinuity, however, 
in an intrisically `q.scalar` or one-valued concept: 
for example, given a sequence of numbers, a `q.spike` in 
value would correspond to a point where two sequential 
numbers have an unusually large difference, compared 
to typical adjacent number-pairs.  Generalized to two 
dimensions, the same notion of discontinuity 
works across different directions (up, right, down-left, etc.), 
giving rise to a matrix of difference-measures, because 
any pixel if `q.adjecent` to several others (eight 
others, if we include both orthogonal and diagonal 
directions).  However, quantifying discontinuity 
by value-different only makes sense, intrinsically, 
when there are just two values to compare.  Full-color 
spectrums distort such measures; in the typical 
`RGB; format colors can be compared among three 
axes (red, green, and blue).  If adjacent pixels have 
fairly large red differentials but small 
differences in green and blue, should the measure of 
their mutual difference be weighted more toward 
the larger gap (the red) or the smaller gaps 
(blue/green)?    
`p`

`p.
To head off these sorts of questions, Computer Vision 
algorithms usually perform a `q.channel reduction,` 
wherein the three channels of a full-color image 
are reduced to one single channel (which can be 
visualized as an image `q.in gray`/, although 
in principle one-channel graphics can be 
rendered with any one single color, such as 
red or blue, fading to white and darkening to black 
at points of greater or lesser magnitude).  
A simple way to reduce channels is, indeed, just 
to take a grayscale version of the original image, 
as if it were being viewed in an old black-and-white 
movie.  The single channel thereby measures 
color-intensity %-- how dark or light it is 
%-- ignoring color-hue entirely.  This tactic 
can discard potentially useful color information, 
because very different colors can have 
similar grayscale values; for instance, medium-red 
and medium-blue would, if juxtaposed, strongly 
suggest some ground-truth discontinuity, 
but they would both map to almost indistiguishable 
grayscale values if they are similarly mid-range 
in intensity (neither dark nor light).   
For these reasons, some image-processing 
frameworks prefer alternative 
channel-reduction techniques.
`p`


`p.
`lXCSD; is designed to support several 
alternative reductions to one-channel.  
One tactic employed by `XCSD; code involves 
`q.toroidal` color space, which is related 
to `HSV;.  A toroid is a solid torus; topologically, a 
torus is the result of one circle rotating aroung a 
central point, tracing its own circular path in the 
process.  A point on a torus surface can therefore be 
identified by two angles, one measuring how 
far the `q.rotating` circle has gone around 
its anchor point, and one fixing a spot 
on the rotating circle itself (of course, 
we initially have to choose an angle to 
represent `q.zero` both on the surface 
and at the center %-- the latter axis would 
point out in space away from the center of the 
torus `q.hole,` like a rod threaded through 
some bagels on which they hang by their centers).  
In `HSV;, the `i.hue` dimension is typically 
understood as circle-like (embodying the 
`q.color wheel` whose opposing colors are 
green/magenta and blue/yellow).  For any 
given hue, the colors generated by 
varying `q.value` and `q.saturation` in the 
`HSV; model naturally form a triangle, 
with the pure color fading to white along 
one side, and decaying to black on another, 
while the white-to-black side spans 
progressively darker shaded of gray.  Near the 
midpoint of the triangle would be a `q.muddied` 
version of the pure color, appearing as the 
blend of that color with grayish tones.   
`p`

`p.
In real life, most colors do not appear with 
pure hues; instead, because of glare/shadow effects 
and/or material texture (which causes light 
to scatter off images, so that the visible 
color at given points has a grayer appearance 
than the underlying pigmentation would suggest) 
a patch of color (representing one evenly-colored 
surface) would typically span several 
`HSV; triples with varying amounts of white, black, 
or gray mixing with the primary color.  
From this observation we can envision an 
`HSV; triangle being centered at a midpoint 
between pure gray and the pure primary hue, 
a point from which all other parts of the 
triangle can be defined in polar coordinates 
(i.e., as a rotation away from a line 
between the pure hue and pure gray).  This 
rotation is `i.additional` to the hue's own 
rotation in the color-wheel, so the overall 
color space is defined by two angular coordinates, 
which is why the space could be described as a 
toroid.`footnote.
Unlike a toroid proper, such a space has a 
`q.pure` white-to-black line which yields identical 
colors for every hue, which distorts color-distances 
in ways that do not match a toroid shape; however, 
actual color-distance formulae within the toroid 
should be weighted anyhow, so the internal geometry 
does not match a toroid in Euclidean space to begin with.
`footnote`
Given this construction, we can define a color-comparison 
metric in terms of two rotation angles and one 
radius length (the `q.rho` measuring distance 
from a neutral grayish manifestation of the pure color, 
with larger rho values being pure-color, pure-black, 
or pure-whiet).  By weighting these parameters 
the `HSV; triangle could be distorted (into something 
like a disk, more in keeping with the toroid 
shape), with the goal being to derive a color-distance 
metric which accommodates how empirical 
texture and glare/shadow effects can alter 
`i.apparent` color-distance.  
`p`


`p.
An assumption behind this model is that 
even `q.solid` colors (`visavis; an 
object's surface pigmentation) will appear 
sometimes lighter, darker, or grayer, so that 
these textural/lighting effects should be 
compensated for when computing color-distance.  
Exactly how to weight toroidal parameters 
is a matter for future research;  
the existing `XCSD; implementation 
employs a straightforward balancing 
mechanism which tries to compress distances 
for near-black and near-white shades and 
expand distances for purer (high-value, high-saturation) 
tones.    
`p`

`p.
`lXCSD; then applies this color model to derive 
several channel-reduction strategies.  One 
computation involves estimating a good 
reference color typical of an images 
`i.background`/, and another canonical 
color (also called a `q.pole` in `XCSD;'s 
code) for `i.foreground`/.  Via distance 
from these colors, `XCSD; can compute a 
probability that any given color 
belongs to a foreground-object or background-area.  
This probability is a scalar quantity, so it 
serves as a reduction to one-channel, one 
which is typically noticeably different 
from conventional grayscale.`footnote.
The probability 
metric is actually a combination of two 
other values, difference `i.from` foreground 
and from background, with the assumption 
that pixels `i.closer` to the foreground color 
and `i.further` from the background pole are 
the most likely actually to be in the foreground 
%-- but unless the two poles are actually 
at a maximum inter-hue distance, which is 
unlikely for real-world pictures, the two 
distance-from-pole parameters can varying
indepednently, which is why they must be 
averaged together (arithmetically inverting one of the pair) 
to yield a one-channel reduction.
`footnote` 
`p`


`p.
For the case-study workflow discussed in this 
section, `XCSD; estimates foreground/background 
poles by assuming that foreground colors are 
likely to be featured near the center of an 
image, and background colors closer to the 
margins (particularly on top).  This heuristic 
may not work for all images, but it is 
a good place to start.  The workflow then 
leverages `XCSD;'s `q.subdivision` layout to 
grab dominant colors from histograms local 
to tierboxes both near the image-center 
and top-periphery, averaging out these 
two cases to get working foreground/background 
poles.  A variation of this pipeline would 
allow human users to fine-tune this selection 
manually, by selecting specific tier-boxes and 
examining their histograms.  It is helpful 
to support interactive 
image-processing along these lines, even 
if only to fine-tune workflows 
eventually designed to run without 
human input (e.g., users might 
engage interactive methods to 
finalize parameters for an image 
series in the context of one 
or several sample images, then 
fix those parameters for non-guided 
analyses extended to the series as a whole). 
Here, though, the discussion will 
stay focused on fully-automated 
workflows.  
`p`


`p.
Having assigned foreground/background probability 
to all pixels, this workflow then reifies the 
probability scalar as a channel and 
can invoke different feature detectors.  
The illustrations here are based on a 
`BRISK; (Binary Robust Invariant Scalable Keypoints) 
detector, which yields a fairly extensive 
set of keypoints almost all of which are 
positioned within regions of the image 
which, on visual inspection, clearly belong to the 
foreground.  To follow up, a superpixel segmentation 
of the image is performed and superpixels which 
encompass some of the `BRISK; keypoints are 
identified; because the keypoints are localized 
near the foreground, the sum of keypoint-containing 
superpixels then serves as a reasonable 
approximation of the foreground as a whole.  
This is not the only way to use zero-dimensional 
keypoints; a more sophisticated analysis 
might consider keypoint attributes so as 
to more rigorously screen for those in the 
foreground proper, i.e., attempt to 
find statistical signatures strongly 
suggesting that a given keypoint is indeed 
part of the region's foreground 
(or near the foreground/background boundary, 
in which case such boundary-keypoints 
could serve as a scaffolding with which 
to isolate the foreground).  For the 
sake of discussion, however, consider the 
more immediate calculation where the 
overall set of keypoints is treated as a 
point `q.cloud` roughly distributed through 
the foreground. 
`p`


`p.
Visual inspection suggests that the `BRISK; 
results do indeed match the foreground 
well, but the workflow will attempt to 
calculate this result systematically.  
To do so, the code loads a 
data structure (defined in an `XCSD;-specific 
language) which describes a series 
of image-transforms that rotate and slightly 
skew the image, then blur the 
image to solid-color `twentysevenbytwentyseven; 
blocks and cancel (white-out) all blocks 
that are not sufficiently close to the 
foreground color.  These operations 
manipulate the image so that the solid 
blocks can acts as classifying bins, 
where the same transforms are then 
applied to the key-point set, such that 
each keypoint gets mapped to a specific 
box, which may be labeled as either 
foreground or background.  The goal is 
to test, first, that the keypoints are 
indeed mostly found in the foreground 
(i.e., most keypoints map to a foreground 
bin) and also that the keypoints are 
distributed relatively thoroughly 
across the foreground (i.e., most 
foreground bins include at least one 
keypoint).  In combination, these 
two statistics confirm that the 
aggregate of keypoints %-- particularly 
when extended to the containing superpixels 
%-- approximates the foregound, both 
by `i.including` most of the foreground 
and by `i.excluding` most of the background.  
`p`

`p.
As a final detail, a different `XCSD; distance 
metric is employed to calculate 
`q.Hough` lines %-- one form of line-detector 
%-- so as to derive a good angle with which 
to rotate the initial image.  Here `XCSD; 
dispenses with one-channel reduction and 
instead considers `q.local` color distance, 
considering color variation within a relatively 
small (e.g., `sevenbyseven;) aera.  
Unlike converting a full image 
to grayscale (or any other one-channel reduction), 
in which a useful color-distance metric should 
apply across the image, when measuring 
color-distance locally we can focus on how 
far apart two nearby pixel's coloration 
appears in the local context, without 
considering the presence of other pixels elsewhere 
in the image.  Multi-channel color distance 
is complicated by the fact that distance-magnitudes 
may take on different meanings in different 
contexts; we might measure two colors as relatively 
far apart and then find a third color which is 
equally distance from the original two, 
leading to questions of how the respective 
distances should be ordered.  However, consider 
the border between two different image-segments.  
In general, there will be relatively 
little color-variation `i.inside` each segment, 
but a notable differences between sample 
colors in one segment and in the other. 
Near the border, then, color-variation on 
either side will be reduced, but cross-border 
comparisons will be much larger.  So long as 
we only consider the local neighborhood 
where the latter cross-border distance 
clearly outpaces intra-segment color distances, 
we can set aside the question of how 
local distances compare to `q.global` color 
distance `visavis; the image as a whole.  
Applying this metric, then, we can 
isolate pixels that appear to lie near 
segment-boundaries.
`p`

`p.
In the context of line-detection, this latter 
strategy for producing a single image-channel 
%-- measuring local-neighborhood inter-pixel 
color difference %-- can potentially 
enable inter-segment lines to stand out 
clearly.  For the case-study described here, 
all Hough-detected lines have almost identical 
angles of inclination (they appear parallel 
when overlaid on a base image), which is a 
good indicator that the average of the Hough-lines 
gives an angular orientation for the image as a 
whole.  This means that noteworthy boundaries 
or other one-dimensional features tend to be 
aligned along a specific angle (or at right 
angles to that) so that rotating the image by 
that angle allows those prominent features 
to be oriented parallel to the image sides.  
This, in turn, allows the image to be 
partitioned into orthogonal boxes for 
purposes such as keypoint-classification.  
In this example, such classification serves 
to assess the quality of the prior pipeline 
steps (`BRISK; keypoints plus superpixels), 
rather than serving as a further analytic 
step proper, but such summary evaluation 
can play a legitimate functional role 
within a pipeline overall (e.g., 
helping to produce a measure of confidence 
in the pipeline results).
`p`


