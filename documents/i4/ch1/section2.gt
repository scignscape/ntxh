`section.ChasmVM and the Digamma Calculus`
`p.
As a preliminary to analyses later in this 
section, I will make a few comments which might 
situate this discussion in the context of 
topics covered elsewhere in this book.  Specifically, 
I intend to motivate the discussion by appeal 
to technologies related to industrial computing 
and cyber-physical systems (`CPS;).
`p`


`p.
One potential use-case for Virtual Machines is encapsulating 
cyber-physical networks: we assume that `CPS; devices are 
sensors and/or actuators managed remotely 
by more-conventional computers.  In general, sensors measure 
physical quantities (air quality in a room, say) whereas 
actuators have tangible physical effects (optimizing air 
quality by adjusting vents, say).  Neither kind of 
device typically has extensive computational capabilities; 
instead, data is routed from the devices to central 
location which use software to process sensor data 
and convey instructions to actuators.  These networks 
can, in turn, cover multiple intermediate points 
(consider `CPS; data warehoused in the cloud) but, 
for exposition, we can focus on the essential end-points, 
on one hand the devices themselves and on the other 
software applications through which humans 
monitor and, if desired, manipulate cyber-physical systems.  
Focusing for discussion on sensors, the actual devices typically 
do not perform calculations (they do not have the means 
to execute computer code) but they `i.are` capable of 
converting some physical quantity to digital 
signals which computers proper can interpret.  
Programmers developing `CPS; applications will rarely 
interact with devices independently; instead, 
the `q.physical` links with `CPS; networks`footnote.Which 
of course can be physical only in an ethereal 
sense, e.g., passing data via wireless services` will typically 
be handled by low-level driver programs, which might 
expose capabilities to access sensor readings through, 
for instance, `C;-language functions.     
`p`


`p.
Developers may employ Virtual Machines in the 
`CPS; arena to streamline this process %-- instead 
of programmers needing to write code which 
interfaces with `C; drivers directly, 
the low-level function calls can be managed 
via virtual machines that interoperate 
with higher-level applications in a language-agnostic 
manner.  In this sense `CPS; `VM;s can be analogous 
to database query languages, in their design and 
rationales: just as query formats give 
engineers the option of communicating with a database 
from a diversity of application-environments, the 
protocols for accessing `CPS; data could similarly 
be encapsulated in `VM; operations.  Moreover, 
as touched on above query engines can ideally 
function either as standalone languages of their 
own (processing query-code outside an application 
context, e.g. on a command-line for debugging and 
maintenance) or via query-factories programmable 
from general-purpose languages; the situation is 
comparable for `CPS;, and one could pursue the 
same duality in how `CPS; network capabilities 
are exposed (either through host languages or 
special extra-application code).   
`p`


`p.
If a `VM; is indeed devoted to specific Cyber-Physical 
network (or group thereof) then a logical first step 
when implementing the `VM; would be to articulate the 
collection of procedures (through driver libraries, say) 
to be encapsulated.  More likely, a more-generic 
`VM; would be deployed in different contexts, each of 
which (by targeting a specific group of `CPS; devices) 
would work against particular groupings of driver code, 
on a case-by-case basis.  The overall `VM; would thereby 
introduce capabilities to interface with some 
collection of kernel functions exposed by driver libraries, 
with the actual integration to those procedures 
finalized during deployment.  In any case, though, 
the general pattern is that any particular deployment of a 
(`CPS;-context) `VM; would be `q.seeded` with some 
preliminary set of functions (i.e., built-in `VM; operations, 
some perhaps added on in deployment).         
`p`


`p.
At a minimum, in short, `VM;s should furnish functionality 
imitating `q.kernel` driver or operating-system calls, 
initializing input arguments with value passed from applications 
and/or sending back to calling application procedures' results.  
The `VM; accordingly bridges application-level and low-level 
kernel functions, which involves not only encapsulating access 
to low-level functionality but also negotiating the 
discrepancy between relatively high-level and low-level 
programming environments.  Most application programming 
languages, for example, recognize constructs (e.g., 
Object-Orientation, or functional programming via lambda 
closures/call-continuations, plus exceptions, mutable references, 
and so on) outside the scope of low-level code (even `C;).  Effective 
`q.bridge` protocols allow applications to access `CPS; data 
(as one category of low-level resources) within familiar 
higher-level paradigms.  For example, programmers already 
working in an Object-Oriented environment might reasonably 
find it intuitive to apply object models to `CPS; elements, such 
as individual devices.  Driver code most likely would not 
recognize Object-Oriented calling conventions directly, so 
a `VM; could be pressed into service to translate application-level 
descriptions of procedure calls and their input parameters into 
simpler binary packages which drivers can handle.   
`p`


`p.
In this sense, a `VM; fitting these requirements would 
simultaneously model `q.high level` calling conventions 
and translate procedure-call requests between higher and 
lower levels.  I make this point in the context of 
`CPS;, but the idea would hold for many cases wherein 
`VM;s encapsulate access to some integral set of 
low-level functions (at least low-level relative to 
applications that would benefit from the requisite 
functionality).  Consider image-analysis: procedures 
exposed via Computer Vision libraries are not directly 
comparable to low-level driver code; they may themselves 
be implemented in high-level languages like `Cpp;); nevertheless, 
libraries such as `OpenCV; or `ITK; tend to demand a 
rather complex scaffolding set up to enable analytic 
procedures being called.  We could imagine circumstances 
where wrapping complex image-processing pipelines in 
a simpler `API; would be convenient for 
code libraries managing image series.  For instance, 
a database exposing image-processing operations through 
query expressions would ideally support query-evaluation 
covering Computer Vision capabilities 
(going beyond obvious information one may want 
to query from an image, such as dimensions and 
color depth).
`p`


`p.
Indeed, database queries, image-processing, and `CPS; network 
administration each represent plausible 
use-cases wherein `VM;s can serve as adapters 
between application-style coding environments and 
procedure-collections that are too specialized or low-level 
to fit comfortably in application-development 
norms.  These cases can overlap, of course; database 
can track or warehouse `CPS; data such that queries monitoring 
real-time device state would logically coexist with 
queries against database content (presumably representing 
temporally prior device info), or image databases can 
support Computer Vision based queries.  The database-query 
perspective points to an interesting heuristic 
analogy, insofar as packaging procedure-calls from an 
application environment to a lower-level context 
resembles the task of packaging application-level 
datatypes and updates into records and fields natively 
recognized by a persistence engine.  Similar to how 
live-memory data structures (objects, say, in 
Object-Oriented environments where the relevant data 
is interpreted in light of objects' polymorphic type, 
which could be subclasses of their declared type) 
are destructured for insertion into database sites, 
high-level calling-conventions (again, Object-Orientation 
provides useful examples insofar as `thisSlashSelf; 
values are represented apart from other input parameters) 
need to be restructured into the (generally simpler) 
forms suitable for low-level functions (e.g., 
moving `thisSlashSelf; to be an ordinary 
parameter, which may require truncating 
to base-class binary layouts, and some level of type-erasure).   
`p`


`p.
Continuing this analogy, flexible database architectures 
bridge application-level data types with persistent 
data structures/records so that application code 
can work with back-end values according 
to the norms of application-context programming; 
equivalently, `VM;s can wrap low-level 
functions such that they fit the profile of 
high-level procedures called according to 
high-level conventions (with objects, exceptions, 
and so forth).  A flexible `VM; would play 
this bridge role in multiple contexts, perhaps 
allowing native functions to be registered as 
kernel operations and striving to be usable 
from multiple host languages: that is, from one 
`VM; we can envision encapsulating access 
to a variety of procedure-collections 
(examples I've cited here include functions 
exposed `visavis; database queries, 
image-processing, and `CPS; networks) and 
routing descriptions of procedure-calls 
from a variety of programming languages 
(which might have object-oriented or 
functional characteristics, or some combination).
To the degree that such generality 
is desired, `VM; should anticipate 
integration with diverse application-level languages 
preferring different calling-conventions and 
procedural contracts. 
`p`

`p.
One maxim for `VM; design, then, at least in 
this sort of use-case context, is to 
prioritize capabilities to model 
and carry out procedure-calls described 
through diverse calling-conventions, rather 
than narrowing in on specific calling-conventions 
which derive from a preferred programming 
model (functional or Object-Oriented, say).  
Object-Oriented conventions such as 
method overrides/polymorphism and exceptions/exception-handling 
might be natively expressed via the `VM;, but likewise 
functional idioms such as lazy evaluation and overloading 
based on type-state.  To clarify the last example:  
procedures can potentially be given different implementations 
by virtue of values' type-state at the moment of 
call (essentially a more granular classification than 
type-attribution itself), often mixing type-state with 
value-destructuring.  A canonical example 
would be procedures operating on list-style collections, 
which in one overload would accept only `i.non-empty` 
collections where the last (or first) element is passed 
separately, alongside a structure representing 
all other elements (this convention is ubiquitous 
in recursive algorithms, since the `q.tail` can then 
be passed to the same procedure recursively, 
becoming destructured by the calling mechanism 
into its own head-plus-tail pair; of course, procedures 
implemented via this strategy also need an 
overload taking an empty list, which serves as a 
halting-point).  
`p`

`p.
In short, an ambitious `VM; can ideally work with 
a broad set of calling styles embraced by 
diverse programming styles, e.g. object-methods, 
exceptions, lazy evaluation, typestates, and 
parameter-destructuring.  To this list 
we might add dependent types and functional-reactive 
idioms (e.g., so-called `q.signal/slot` conventions).  
`p`


`p.
As might be obvious by this point, the kinds of `VM;s 
I am envisioning bound to such requirements would 
be relatively `q.high level`/, closer in spirit 
to Interface (or Service) Description Languages than 
low-level emulators of actual machine code.  This is consistent 
with the spectrum of `VM; technology; while certainly 
some `VM;s embrace the use-case of enabling 
virtual `q.operating systems` or similar low-level 
environments for portable code %-- where the 
execution and runtime of `VM; instructions should 
mimic machine-language steps %-- other flavors of 
`VM;s are more concerned with engineering 
language-neutral environments that interoperate 
fluently with different kinds of programming front-ends.  
The priority in such a case may still be cross-platform 
flexibility, but the design goals emphasize 
a desire for multiple origination-languages to 
emit `VM; code (through factories if not text streams) 
according to protocols which are in sync with 
host-language conventions (`q.host` language in the 
sense that `VM; capabilities can be embedded 
via static or dynamic libraries linked against 
software components, which in turn may 
utilize such capabilities in different ways %-- via 
scripts, queries, workflow descriptions, etc.). 
`p`


`p.
Since not all `VM; actually aspire to multi-language 
support to the open-ended degree implied here, 
theories informing `VM; implementation do 
not necessarily analyze data models or design 
patterns targeted specifically at language 
`q.agnosticity` (so to speak); a more common 
scenario is that theoretical perspectives 
emanate from coding paradigms which give 
rise to distinct flavors of programming languages, 
potentially at some level prosthelytizing for 
favored paradigms rather than aiming for 
broad generality (in the sense that `VM;s 
`i.for functional languages`/, say, reflect a 
general assumption that functional methodology 
is in the general case a better coding style 
than alternatives).  By contrast, I 
am interested here in describing theoretical 
`VM; models that remain nonjudgmental 
as to which conventions are better in which 
context (or to take the view that multiple 
coding styles each have their own use-case 
so should be supported as such).  A rigorous 
`VM; `q.model` should have multiple 
dimensions (addressing types and data-encoding, 
for example), but of course an essential 
concern is modeling procedure-calls and 
calling-conventions, so I will sue 
this topic as a starting-point for a 
(relatively informal) system to 
encapsulate `VM; details in a schematic fashion.     
`p`

`subsection.Applicative Structures and Mathematical Foundations`
`p.
Theoretical (and applied) computer science often 
approaches procedures from the viewpoint of mathematical 
functions, essentially mapping that transform 
inputs to outputs.  Codifying the principles of 
`q.functionhood` in general forms the central project 
of analyses that bridge math and computers 
(e.g., lambda calculus).  In terms of mathematical 
`q.foundations`/, these various formulations 
(including lambda calculus and its derivatives 
and, for instance, Combinatory Logic) entail 
strategies for clarifying what are 
sometimes called `q.applicative structures`/, 
or generic patterns involving the `i.application` 
of a function/procedure to one or more 
arguments/parameters.  Since this is such a 
basic phenomenon in the mathematical 
realm, it is understandable that some 
researchers in `q.philosophy` of mathematics 
and its foundations would focus on applicative 
structures (perhaps indirectly via, say, lambda 
calculus) %-- even though mathematical 
expressions are written down according to 
a wide variety of conventions (consider formulae 
for integration, for ratios, for polynomials, and so forth) 
we can imaging a system which translates 
mathematical terms to a more systematic logical 
representation (which would presumably 
resemble something like `lisp; code). 
`p`


`p.
The concerns evoked by applicative structures are 
not only orthographic, however, because there is 
also a `i.semantic` dimension in the sense of 
spaces of functional values qua semantic entities.  
The problem of semantically characterizing 
`i.a` function or procedure (e.g., as a calculational 
process, or a %-- maybe time/context-dependent %-- 
input-to-output mapping, or the like) is one 
matter, but whatever our foundational semantic 
theory in this sense it is natural to extend 
it via functional composition (analogous to 
how linguistic semantics involves the semantics 
of nouns, and verbs, but more thoroughly also 
the compositional principles of verbs together 
with nouns yielding sentences/propositions).  
The set of possible applicative structures 
`q.generated` by some collection of functions 
(or function-symbols) is analogous to the 
set of discrete functional values that can be 
defined by the combination of multiple functions 
wherein the results of one function applied to 
its arguments becomes in turn (one of the) 
parameters of a different (or, recursively, the same) 
function.  Systematically, an `q.applicative system` 
would be a set of function-symbols alongside 
`q.variable` symbols with a rule that applicative 
`i.structures` include expressions of the form 
`fxoneetc; where `xs; are variables, 
and that for any applicative structure the modification 
formed by replacing a variable-symbol with another 
applicative structure is also an applicative structure.  
Defined in terms of the close of such substitution operations. 
applicative systems can be seen to follow a generative  
pattern very similar to labeled trees %-- each 
node is either a symbol-node or a branch with 
its set of child-nodes %-- with the added detail 
that labels are partitioned into two sets 
(function and variable symbols, respectively) 
such that left-most child nodes always have function-labels 
and other non-branch nodes always have variable labels.  
`p`


`p.
I'll make a couple of technical points about applicative 
structures here, not so much because the mathematics 
is particularly sophisticated or consequential but 
so as to establish a baseline of comparison for 
the graph-theoretic encoding of (generalizations of) 
applicative structures I will discuss subsequently.

`defin -> Applicative Systems ->> for any fixed 
set of `i.function symbols` and `i.variable symbols` 
an `i.applicative structure` is any element of a 
set which is the closure of the set of 
primitive expressions (consisting of a function 
symbol followed by a number of variable-symbols 
which matches its arity, assuming we assign a 
specific arity to each function, or else to any 
number of variable-symbols) under substitution 
operations wherein an applicative structure 
is inserted in an enclosing structure taking the 
place of a variable.  Applicative Systems 
are then sets of function and variable 
symbols (and possibly declarations of function-arity) 
together the full set of possible applicative 
structures generated on their basis.  For generality, 
we can allow functions `f; to have dynamic range 
arity (multiple integers `n; such that 
`f; followed by `n; variables is a valid expression); 
in this sense systems which do not recognize arity 
limitations at all would implicitly allow arbitrary 
range arity for all function symbols.`footnote.
A `i.partial` applicative structure would be one 
that belongs to an applicative system no subject to 
arity restrictions but which is excluded when 
arities are recognized, due to one or more function 
symbols lacking a sufficient number of following 
symbols or nested structures; partial applicative 
structures in this sense can be semantically 
interpreted as designating `q.meta-functions` which, 
when the variables are replaced by fixed values, 
reduce to actual functions whose parameters are 
the missing symbols implied by insufficient arity 
thresholds.  Of course, this discussion assumes 
we also have a notion of `q.fixed values` being 
assigned to functions as parameters).
`footnote`  Note that the generative 
rules allow for zero-arity functions, whose 
semantics would be procedures that yield results 
even without inputs (nested structures can be 
wholly comprised of one single function symbol). ;;

`anondefin.
A given symbol may appear multiple times in an applicative 
structure; the above definitions were formulated with the 
idea that `q.the same` symbol in different positions 
is, according to the generative rules, a different 
symbol, but for clarity we can say that one 
symbol can have multiple `i.tokens` in a given 
structure.  Each symbol-token has a 
`i.nesting level` such that the nesting level of a 
function-symbol matches that of variable-symbols 
following it (that are not themselves part of a 
further nested structure) and replacing a variable 
with a nested structure forces the function symbol 
at the left of the latter structure to have a 
nesting level on greater than the replaced variable.  
By the construction/generation process, there will always 
be exactly one function-symbol with least nesting 
level, which we can stipulate to be zero.  Symbol-tokens 
also have `i.positional indices` defined  such that 
function symbols are assigned index zero and variable-symbols 
following them (incrementing across but skipping over nested structures) are 
assigned successively greater index numbers.  Tokens are 
uniquely identified by 
a positional-index list whose length is 
determined by (viz., one greater than) the token's 
nesting-level, notating the tokens own positional-index 
and also those that would be assigned to its 
parent nodes (treating the structure as a tree) were 
they tokens rather than nested structures).  Positional-index 
lists induce an ordering on all tokens in an applicative 
structure (comparing the first number in two respective 
lists, then the second as a tie-breaker, and so forth; 
akin to ordering leaf nodes on a tree where 
left-ward and higher nodes are prior to those 
below and/or to their right).  Applicative 
structures are `i.non-recursive` if each function-symbol 
has only one token.  It is helpful to further 
define non-`i.root`/-recursive structures as those 
where the function-symbol whose token has 
zero nesting level appears only once.
`anondefin`

`anondefin.
Applicative structures can be grouped into equivalence 
classes wherein any structure in the class would 
map onto a peer structure under a permutation of 
the applicative system's variable symbol-set.  Since 
we can give an ordering to the variable-set, assume 
each variable is assigned a number code, which 
in turn allows us to define a `i.canonical presentation` 
of an applicative structure by permuting its 
variable-symbols so that tokens which are 
prior (in the ordering based on positional indices 
just described) are assigned symbols with lesser 
codes, choosing each new symbol to be the one 
with  lowest code value not yet used.  Each 
applicative-structure equivalence-class is thereby 
represented by one structure with 
such canonical presentation, and we can restrict 
attention to these structures in particular.  
Note that the generative rules for applicative 
structures have to be separated into two 
groups, because (in the general case) we 
have to avoid unrelated symbols with the same 
`q.label` colliding according to the generative 
step whereby a nested structure is substituted 
for a variable.  For a nested structure with its 
own collection of variables, the structure may 
need to be rewritten (cf. lambda-calculus alpha-conversion) 
as an equivalent structure 
(but with symbol-permutation) %-- not necessarily 
(indeed not usually) one in canonical-presentation %-- 
so that each nested symbol is different 
from all symbols in the enclosing structure 
(unless the symbol-representation is explicit, 
i.e., the point is to model a function-application 
where the same input value is copied in multiple places).
`p`

`p.
Representing function-arguments via `q.variable` symbols 
(which are understood to be substituted with values from 
some domain in the context of `i.calling` functions) 
is consistent with lambda calculus, and with the 
idea that applicative structures describe valid strings 
in a language codifying notions of function-application.  
However, it is possible to develop alternative representations 
of the same structures, such as De Bruijn indices 
(due to Nicolas de Bruijn) which in effect model argument-places 
by numeric indices rather than symbols.  I propose 
to use a similar convention based on the idea of 
forming symbols which have numeric values but also 
`q.colors`/.  Specifically, consider, first, characterizations 
of the set of functions which may be identified 
on the basis of one function-symbol %-- say, `fRed;, 
which assume has a fixed arity `arity;, say, three.  
Then `fRed; itself could be notated `fOneTwoThree; 
(using the color red to signify function-symbols 
and blue numbers for variable-symbols).  Given 
`f; we implicitly also have a family of related functions 
which are identical to `f; but operate on different domains, 
taking argument-lists longer than `f;s arity but 
ignoring the extra values: I propose denoting these 
via `fOneTwoThreeGFour;, `fOneTwoThreeGFourFive;, etc. 
using `i.gray`/-color numbers for discarded 
extra elements.  Conversely, we can also form 
functions with arity `i.less` than `f;s by 
repeating numbers for the arguments; e.g., 
`fOneTwoOne; or `fOneTwoTwo; where note that 
numbers occurring multiple times are shaded in 
light blue.  I'll refer to this as 
`q.red-blue-gray` notation.  Schematically, 
one can treat this as equivalent to 
notation with symbols, each colored number 
being just a symbol, except that the 
colors allow grouping into ordered sets which 
may be subject to further constraints.  
Likewise, the function-symbols themselves 
can be replaced by numbers encoding 
any ordering of the list of available 
function-symbols which form the core of 
our applicative system.
`p`


`p.
Suppose we start with any function-symbol list 
%-- for instance, as earlier in this chapter 
I alluded to collections of kernel 
functions which `q.seed` a Virtual Machine 
wrapping access to `CPS; networks.  
Intuitively, it would seem to be natural 
property of any computational or mathematical 
(or even linguistic environment 
for which a notation of `f; itself would 
be meaningful %-- e.g., denotes a 
possible computation, or a mathematical 
function which yields a value when parameters 
are fully specified %-- then so too 
would be variants of `f; with greater 
or lesser arity as notated (here) through 
red-blue-gray strings.  In other words, 
if we take some semantic predicate %-- say, 
`i.describes a computation` %-- then we may 
want to consider the full set of functions 
which meet this criterion that can be 
generated from some initial `q.seed` collection; 
given `f; as a seed, notate the set of 
functions (describable solely via `f;) 
which describe a computation, for instance 
(or replace `q.describe a computation` with 
some other predicate).  We seem to have:

`lemma-statement.
Assuming `f; designates a function 
with arity `arity;, there is a 
unique list of red-blue-gray 
strings which each describes 
a function, mutually structurally 
distinct, enumerating all functions 
that can be described on the basis 
of `f; alone (including `f; itself). 
(Note that we are considering functions 
generated only by extending or contracting 
`f;s arity; a different 
enumeration would involve recursive 
descriptions where a call involving 
`f; yields a value which is input 
to another call involving `f;.) 
`lemma-statement`

`lemma-proof.
Consider strings with exactly `arity; 
(not necessarily distinct) variable-symbols.  
Without generality, we can assume that 
symbols (blue-colored numbers) will 
lesser numeric value appear to the 
left in their first token.  Let 
`arityprimelessthanarity; be 
the number of `i.distinct` variable-symbols 
in a given string; i.e., the arity 
of the function being described as a 
variant of `f;, not `f; itself.  
Then any permutation of `arity;-length 
`arityprime; numbers (in the range 
`onearityprime;) %-- restricted to cases 
where the first occurrence of smaller numbers 
is always before the first occurrence of 
any larger number %-- describes a 
structurally unique variant of `f;.  
Analogously (distinguishing blue and 
light-blue) the darker-blue-only 
substrings are increasing by 
starting at one, and light-blue 
symbols can be freely interspersed 
except that any light-blue `nlightblue; 
has to follow the corresponding darker `ndarkblue;. 
For each of these strings, consider the 
strings themselves and then modifications 
which add the gray-color 
numbers starting with `arityprimeplusone;, and 
so forth, where the gray-colored numbers 
can only occur in an arithmetically (-by-one) 
increasingly list.  Each string generated in 
this fashion represents a structurally 
distinct variation on `f;, suggesting 
that the full set of strings generated 
accordingly embodies all `f; variations, 
at least those which could be notated 
by a language that encodes function-calls 
through symbols representing input parameters.   
`lemma-proof`

`anondefin.
I will call a red-blue-gray string 
`q.lambda feasible` if it satisfies the 
restrictions employed above `visavis; 
darker-blue and gray numbers.   
`anondefin`
`p`

`p.
Generalizing this analysis to something like lambda 
calculus requires notating function-composition, 
by substituting nested applicative 
structures for variable-symbols.  For this 
context I propose extending `q.red-blue-gray` 
notation to include `q.black` numbers that stand in for 
nested structures.  Call a `q.red-blue-gray-black matrix` 
a set of rows formed from red, blue, gray, or black-colored 
numbers (unlike normal mathematical matrices the rows 
need not have equal size, though if desired we 
can always introduce a `q.dummy` symbol, e.g. a black 
zero, to pad shorter rows on the left, yielding 
something that looks like a normal matrix; likewise, 
colored numbers can always be mapped on to disjoint 
integer sub-sets, so we can if desired 
treat red-blue-gray-black matrices as isomorphic 
to normal integer-matrices). 

`lemma-statement.
For any fixed function-symbol and (ordered) variable-symbol 
set there is exactly one (countably infinite) set of 
finite applicative structures in canonical presentation 
that can be generated from those symbols.
`lemma-statement`

`lemma-proof.
I'll proceed by representing each applicative structure 
via a red-blue-gray-black matrix.  The simplest applicative 
structures have no nesting, just a function-symbol 
followed by a list of variable-symbols.  
Since we are attending strictly to canonical presentation, 
we can permute the latter list so that 
lower-numbered symbols appear to the left, and distinct 
symbol-numbers are allocated incrementally.  
Symbol-numbers in this context function similarly 
to `q.De Bruijn indices` in the lambda calculus 
(when formalized through de Bruijn's nonstandard notation).`footnote.
See, e.g., `cite<x>;
`footnote`  Lemma l1 argued that we can describe all 
variations on an `f; `i.without` composition as the 
set of `q.lambda-feasible` red-blue-gray strings with 
`f; (which can be denoted by a red number as well as a 
symbol) as the sole function notated.  According 
to the construction rules generating applicative 
structures, we then have to consider 
substituting nested structures for variable 
symbols.  Using red-blue-gray-black matrices, 
encode such nesting by inserting a black-colored 
number instead of a blue one, selecting 
black numbers according to the rule 
that lower numbers occur before (to the left 
of and above) higher ones, and that the 
black-number values map the index of subsequent 
rows in the matrix; each row, then, is its 
own red-blue-gray-black string.  Re-encode 
blue numbers in nested structures by adding 
to their value the smallest offset possible 
without conflicting with blue numbers in earlier rows.  Call 
red-blue-gray-black matrices subject to 
these restrictions (on the black numbers 
as well as lamdba-feasible restrictions for each row) 
`q.lambda-feasible` matrices.  Based on their 
construction, I claim the set of such 
feasible matrices is isomorphic to the 
set of applicative structures, so that 
the lambda-feasible red-blue-gray-black matrices 
offer a systematic enumeration of all 
applicative structures. 
`lemma-proof`
`p`



`p.
There is a certain amount of mathematical bookkeeping 
implicit in the above presentation, which might 
obscure the fact that applicative structure are 
very basic; they should indeed by seen as rudimentary 
to the point of being rather uninteresting in themselves.  
More involved systems however emerge by 
relaxing certain conditions; for example, we 
might consider allowing self-referential applicative 
structures that express infinite recursion.  
The `q.De Bruijn` matrix form points to one 
plausible avenue for modeling this process, because 
negative matrix entries could be allowed 
to `q.refer back` to prior rows, notating the 
idea of instruction-sequences in a computing machine 
looping back to prior instructions.  Other 
classical developments (often phrased via 
lambda calculus rather than applicative structures 
`i.per se`/) involves encoding natural numbers 
via repeated iterations of a single `q.successor` 
function (one being the successor to zero, etc.), 
such that %-- again appealing to matrix-notion 
%-- there is a sequence of matrices encoding 
the sequence of natural numbers.  Any application 
of functions `i.to` natural numbers can then 
be encoded alternately as a `i.substitution` 
of these special matrices for the relevant variables.
`p`


`p.
Additional applications of applicative-structure 
theory turn on the notion that 
applicative structures model function-composition 
semantics.  To the degree that we can 
(with suitable semantics) treat functions 
as `i.values` %-- as points in an ambient 
function `q.space` %-- then function can be 
composed in various ways to generate new 
such values.  In the simplest case (functions 
of arity one), at least without recursion, 
composition can be 
analogous to an algebraic operation %-- 
from `f; and `g; get `fofg; (and `goff;, which 
is generally different).  Once one or more 
functions has arity two or greater, however, we 
have a set of multiple composition-options 
%-- `fxgx;, `fxgy;, `fgxy;, `fgxgx;, `fgxgy;, 
for example, are all potential compositions 
of a two-arity `f; with unary `g;, listing 
only non-root-recursive structures where `f; 
takes priority over `g; (the full list 
as such is larger, and, if we allow unrestricted 
recursion, infinite).  We can therefore 
describe each form of composition via 
applicative structures, yielding a more 
complete description of function values' semantic 
terrain than is designated via individual 
function-symbols themselves (this discussion 
leaves unaddressed the question of 
whether there are function values that 
can `i.not` be encoded via applicative structures).
`p`


`p.
Depending on one's perspective, applicative structures 
can be seen as either essentially semantic or 
syntactic phenomena.  Syntactically, we can 
treat these structures as characterizing 
valid strings in a language comprised of 
function and variable symbols, and one 
single grouping construct (via nested 
structures, which syntactically take 
the form of sub-terms that can be grouped 
into quasi-atomic units, substituting 
for the actual atoms, viz., variables).  Certain 
syntactic formations, however, also seem to have 
semantic interpretations: as already observed, 
the `fofg; (and `goff;) compositions correspond 
to what are implicitly semantic relations, that 
is, the composition of two functions to yield a 
new function.  Moreover, mappings 
`i.between` applicative structures sometimes 
appear to express semantic relations: `goff; is 
the compositional `i.inverse to` `fofg;, and 
the inverse of one (say, binary) function `fxy; 
can be notated as `fyx; %-- in short, we 
can extend applicative systems to treat certain non-canonical 
structures as notations for variation forms of 
functions derived from their `q.base` forms by 
substituting which argument is placed in which 
position, the simplest example being `fxytofyx; 
(note the similar point mentioned earlier, 
that partial applicative structures can be 
read as designating `q.meta`/-functions).
The fact that some applicative structures 
thereby have semantic interpretations 
%-- even if we consider applicative 
systems as essential syntactic constructions 
(enumerations of valid expressions in a 
certain simple class of formal languages) 
%-- has led some researchers to 
consider applicative structures 
(particularly in the guise of Combinatory 
Logic) within fields as diverse as 
linguistics and psychology.  Combinatory 
Logic essentially uses a set of combinator 
symbols (external to both function and variable 
symbols) and string-reduction rules 
to enumerate all applicative structures 
generated by a system's intrinsic symbol-lists: 
for any structure formed from symbols 
`fetcxetc; (`f;s and `x;s being functions 
and variables respectively) there is a 
combinator `Ccomb; such that the string 
`Cfetcxetc; reduces to the relevant 
structure (where `Ccomb; can itself be a string 
of other, more primitive combinators).  
In this context combinators play an 
enumerative role analogous 
to (what I am calling) De Bruijn 
matrices (although I personally find   
combinators' reduction rules feel  
more ad-hoc than the state-machine-like 
interpretation one can give to a  
De Bruijn matrix; I'll leave the details 
to a footnote).`footnote.
An intuitive way to picture Combinatory 
Logic is as follows: disregard the interpretation 
of function/variable symbols as functions and 
arguments, respective, and consider merely 
symbol-strings as expressions in a formal 
language.  Extend this language 
with combinator-symbols that are associated 
with reduction rules that impose a 
partial order on the set of permissible 
strings (such strings are differentiated 
by a grouping operation, potentially nested, 
as well as symbol-lists).  A canonical 
example is the `Scomb; combinator such 
that `Sfgx; `reduces; `fxparengx; (where `reduces; notates 
reducing to) or the `Bcomb; that appears to connote 
composition (seen as semantic): `Bfgx; `reduces; 
`fparengx; (or the famous `q.Y` combinator, 
which can be described by the rule `YgreducgYg;).  
Note that `q.reduction` here, however, at least 
in the mathematical presentation of Combinatory 
Logic, does not have an explicit semantic 
interpretation, but merely notes that 
some strings come before others in the 
`reduces; partial order; moreover, 
strings in the language contain 
free-form admixtures of combinators, functions, and 
variables, which have no apparent semantic 
interpretation (the general premise being 
that only `q.maximally` reduced strings should 
be approached semantically).  I find these 
details to render Combinatory Logic proper 
somewhat counter-intuitive, at least on 
its own terms (to be fair, though, 
although some papers systematize Combinatory Logic 
in isolation, it is more common to adopt 
combinators as merely notational conveniences 
for certain constructions in lambda calculus).      
`footnote`  The linguistics-based interest 
in Combinators evidently reflects the 
idea that certain applicative structures 
(and intra-structure morphisms) codify 
compositions or modifications which express 
semantic operators, and, in general, 
certain applicative structure embody syntactic 
constructions that are sufficiently common  
or entrenched as to emerge as semantic 
conventions, not just syntactic forms 
(the underlying principle, articulated 
for example in Construction Grammar, being that semantic 
constructions originate at least in some 
cases from recurring syntactic formations, 
such that `i.syntax` `i.per se` %-- novel 
phrases, say, which rely on grammar rather 
than idiomatics to signal the intended 
composition %-- represent forms that 
are `i.not` sufficiently entrenched as to 
have `q.automatic` meanings (like those 
of single words, say) but instead need to be 
parsed.  In this context Combinators 
are at least intuitive emblems suggesting 
how syntactic entrenchment yields semantic 
conventions.  More generally, we can 
also observe in the linguistic context 
that the overall space of applicative structures 
(over all words in a sentence, say) 
can be partitioned into subspaces (semantic 
entrenchment being one example, but 
we can also analyze different applicative 
structures as having more or less linguistic 
coherence, based on criteria such as 
verb-to-noun relations).  
`p`


`p.
Apart from `i.syntactic` interpretations, 
Applicative Structure can also be seen from 
semantic angles in more narrowly 
mathematical contexts, for example when 
we consider properties `i.of` functions, 
such as those derived from algorithm-theory.  
For example, if `f; denotes a function which 
can be evaluated through a calculation implemented 
with guaranteed termination, the composing 
`f; with another function with the same 
property yields (or describes) a different 
function which also by guarantee terminates 
(simply allow `g;, say, to terminate, 
then feed its value to `f;).  More generally, 
we can take the applicative system 
over a set of functions sharing properties 
such as guaranteed-termination to be a 
larger set with the same property.  In this 
sense we can regard applicative structures 
(perhaps via lambda calculus) as part of 
the backbone for analyzing different kinds 
of function-spaces according to algorithm-theoretic 
properties or profiles (e.g., computability, 
termination, different complexity classes, and the like).  
These potential applications, alongside 
those mentioned above in the context 
of recursive function theory and infinite 
recursion, as well as encoding number theory/arithmetic, 
constitute some of the extensions 
to underlying applicative-structure theory which 
have some mathematical significance.   
`p`

`p.
My concerns here are less mathematical, 
so I will extend applicative 
constructions in a different direction, 
grounded in graph theory.  I will 
suggest that applicative structures 
have natural correlates among 
(directed) graphs, and that this 
setting is in some ways more intuitive 
and less cumbersome than the 
above presentation appealing to 
notions like `q.substitution` and 
function/variable `q.symbols`/.
`p`

`p.
Outside of mathematics proper, there is still 
some value in enumerating the full collection 
of applicative structures (generated by a preliminary 
function-list).  If we treat these structures as 
syntactic phenomena, then intuitively a plausible 
`i.semantics` for notions of function-application 
would consider formally distinct applicative 
structures as semantically distinct entities.  
The mechanisms for enumerating distinct 
structures may be less important than the 
mere fact of having a well-defined algorithm for 
producing and screening for all valid constructions.  
I use (what I call) red-blue-gray-black matrices 
to provide these criteria: intuitively, a reasonable 
semantics for applicative structures would 
recognize every formation described by a distinct 
such matrix as a semantically distinct, and recognize 
the set of lambda-feasible red-blue-gray-black matrices as a 
complete listing of all valid strings in a language 
that describes functions (for some meaning of `q.function`/) 
derived from kernel `q.seed` functions by arity-expansion (via 
unused arguments), projection (arity-reduction via 
reusing parameters), and composition.  Later in the 
section I will discuss more specific semantics, 
but the enumeration through red-blue-gray-black matrices 
yields an initial picture of the space 
over which such a semantics should quantify.  
`p`

`subsection.Hypergraph Models of Calling Conventions`
`p.
The definition of function-application modeled via 
applicative structures (and similarly the lambda 
calculus) should be deemed insufficiently 
expressive for analyses related to modern programming 
languages.  To be sure, in the above discussion 
I was evasive about function's `q.semantics`/, 
such that an incomplete account of function 
`q.application` `i.per se` is arguably warranted 
by generality.  When turning to programming 
languages in particular, however, we have more 
detailed semantic notions to work with; for 
instance, `i.functions` can be considered as 
computational `i.processes` which follow 
so `i,procedure` so as to (in general) derive 
from output in the presence of given inputs.  
Such an overview is still mostly intuitive 
and informal, but it begins concretizing 
the notion of `q.function` in `i.procedures` 
which. presumably, in the general case 
execute series of smaller steps in sequence 
(contrast, say, to 
regarding functions set-theoretically 
as mathematical relations between inputs 
and outputs, each function being a subset of 
the total space of mappings conceivable 
between its domain and codomain).  
`p`


`p.
Given the functions-as-procedures paradigm, a first 
step is to broaden the notion of applicative 
structure appropriately to accommodate how 
programming languages (via which procedures 
are implemented) recognize multiple 
`i.forms` of inputs and outputs (objects vs. 
ordinary parameters, say, or thrown exceptions 
as opposed to ordinary return values).  To designate 
these variations in forms of inputs/outputs, 
I will describe procedures parameters as being 
grouped into `q.channels`/, which can be interpreted 
as gathering a node's adjacency set into 
(potentially) two or more partitions.  I will 
introduce some nonstandard terminology, 
which is hopefully justified by the 
Virtual Machine model which `q.drops out` of 
the theory I'm sketching here, as a practical 
use-case.  The central underlying notion 
is to designate certain graphs as `q.syntagmatic` 
insofar as they model procedures call with 
(in general) multiple parameters grouped 
into multiple `q.channels` (an explanation for the 
choice of the terminology `i.syntagm` and `i.syntagmatic`/, 
usually encountered in linguistics, is 
offered in Chapter 6 of `cite<CovidCancerCardiac>;).   

`anondefin.
A `i.channelized in-neighborhood` (we can 
drop the `q.in-` when it is clear that out-neighborhoods 
are no relevant) of a directed 
graph is the in-neighborhood of one vertex `v; where 
the edges incident to `v; are ordered and grouped into (typically) 
one or more `i.channels`/.  A `q.channel` in this context 
can be considered most generally any collection of edges 
(focusing on the edges themselves, not their incident 
nodes; in the case of labeled graphs channels 
would then be, in effect, sets of label-tokens) but 
define a `i.syntagmatic channel` as a channel all of 
whose directed edges share the same target node; 
by default `i.channel` and `i.syntagmatic channel` 
can be used interchangeably.  The central 
vertex `v; can be labeled with a string taken 
from a prior collection of names (intuitively, 
name of procedures insofar as graphs 
diagram procedure-calls).  Channels 
may also have `q.descriptive` labels (which serve 
to associate channels with channel `q.kinds`/).
`anondefin`

`anondefin.
A `i.channel system` 
on a directed graph is a set of criteria constraining 
the legal channelized neighborhoods around any vertex; 
example restrictions would be stipulations 
that vertices cannot have multiple channels 
with the same kind, or (in some context) 
restricted to specific possible channel-kinds, 
or that channels need some fixed number 
or range of edges (e.g., a channel embodying 
procedural `i.outputs` may be restricted to 
have at most one edge).  More detailed 
restrictions can apply to multiple 
channels in interaction: consider modeling 
the alternative between normal procedure `i.outputs` 
and `i.exceptions`/: throwing an exception 
precludes returning from a procedure normally, 
and vice-versa.  In terms of channels this 
means that a non-empty channel representing 
(normal) outputs precludes a non-empty 
channel representing exceptions, and vice-versa 
(more precisely, as clarified below, the two 
channels cannot simultaneously have 
edges out-incident to nodes with `i.non-void state`/).
`anondefin`

`anondefin.
A `q.lamdba-restricted` channel system is one 
specific system which it is convenient 
to name ahead of time, intended to 
mimic a minimal lambda calculus.  See the following lemma.
`anondefin`
`p`

`p.
Instead of an open-ended notion of `q.applicative 
structures` we can speak of applicative 
systems `i.constructed relative to` channel systems.  
This more general sense of applicative 
structures builds up incrementally from 
channelized neighborhoods conformant to the 
relevant channel system.  By way of 
illustration, consider a graph-theoretic 
encoding of `q.classical` applicative 
structures (the form embodied in lambda calculus) 
as defined earlier.  

`lemma-statement.
`q.Classical` applicative structures can 
be modeled via channel systems with the 
following characteristics: there are two 
channel kinds %-- inputs and outputs %-- 
and output channels must have exactly one edge.  
Call channel systems subject to 
these limitations `q.lambda-restricted` as 
anticipated above.
`lemma-statement`


`lemma-proof.
Proceeding by induction on the generative rules 
for (the kind of) applicative structures 
defined above.  The simplest case is 
one function-symbol following by some number 
of variable-symbols.  Express this via 
a neighborhood with one `q.procedure` node 
(standing for a procedure to be called) and a 
collection of `q.argument-nodes` with edges 
pointing in to the procedure-node (all these 
edges grouped into one channel).  Next, we 
expand outward to encode nesting (substituting 
structures for single variables).  Consider 
two channelized neighborhoods `None; and `Ntwo; restricted 
to the channel system described in the lemma 
statement (inputs plus exactly one output).  
We can form a connection from `Ntwo; to 
`None; by inserting an edge between 
the vertex in `Ntwo; which is adjacent 
(in the outgoing sense) to the single 
output-channel edge in `Ntwo; and 
one of the input-channel edges in `None;.  
That is, the out-adjacent vertex in `Ntwo; 
will now have two out-edges, one targeting 
the function node in `Ntwo; and one 
targeting an argument node in `None;.  
Introducing these connecting edges 
is structurally analogous to `q.substituting` 
nested structures for variables.  
I'll clarify that claim with a further definition.   
`lemma-proof`

`anondefin.
The `i.syntagmatic neighborhood-set` of a 
vertex is an expansion of channelized in-neighborhoods 
by connecting a different neighborhood 
to an initial neighborhood via an edge between 
a `q.peripheral` node in each neighborhood 
(i.e., excluding the vertex in-adjacent to 
other nodes in the neighborhood).  Because 
each channelized neighborhood can have its own syntagmatic neighborhood-set 
these structures can model nesting.  Stipulate 
that any syntagmatic neighborhood-set proper has exactly 
one channelized neighborhood which does `i.not` have a 
connection to another neighborhood; i.e., all of its 
vertices have at most one out-edge.  Call the central 
node of this neighborhood central for the neighborhood-set 
overall, and stipulate that syntagmatic neighborhood-sets 
must be `i.connected` in the sense that the central 
node of each component channelized neighborhood must 
be connected to the central node (via paths 
allowing either in- or out-adjacency). 
`anondefin`

`anondefin.
An `i.input ring` around a syntagmatic neighborhood-set 
is a collection of labeled nodes (each with distinct 
labels) that are not otherwise parts of the construction.  
Consider the ring nodes to be connected with (non-procedural) 
nodes via a `q.supplication` edge (intuitively, to 
represent the idea that the node is associated with a 
value based on its supplier-node's label); two nodes 
adjacent to the same ring node via such edges 
have `q.shared supplication`/.
`anondefin`

`lemma-statement.
Syntagmatic neighborhood-sets within a 
channel-system restricted to output/input kinds 
(and max-one output channels) can be unambiguously 
encoded via matrices equivalent to `q.De Bruijn` 
matrices represented earlier for applicative 
structures in canonical presentation. 
`lemma-statement`


`lemma-proof.
Label procedure-nodes (i.e., center-nodes) with 
numbers based on their string labels (assigning 
like numbers to like strings).  
Edges in channelized neighborhoods 
are ordered, so form matrix rows by notating the 
procedure-number followed by numbers assigned 
to argument-nodes; if an argument-node 
shares supplication with a different node already 
numerically labeled, adopt that number, otherwise 
adopt the least available numeric label.  This 
applies only to argument-nodes which are not 
connected to other nodes across syntagmatic 
neighborhood-set connections.  In the latter 
case, label the `q.nested` channelized neighborhood 
with numbers (starting at one for the central node 
overall and incrementing as needed) and, 
for argument-nodes connected to other neighborhoods, 
insert a negative integer whose absolute 
value is the external neighborhood's index number.  
Create a matrix row for each channelized 
neighborhood, in order of their index numbers.  
The resulting matrix (zero-padding on the left as 
needed) will structurally mimic `q.De Bruijn` 
matrices in the context of applicative 
structures outlined above.
`lemma-proof`

`theorem-statement.
Applicative structures as presented earlier and syntagmatic 
neighborhood-sets within a lambda-restricted channel 
system are isomorphic to the same set of lambda-feasible 
De Bruijn matrices, assuming they share the same 
set of function-symbols/procedure-labels.  
`theorem-statement`

`theorem-proof.
Lemmas () and () detail the construction of 
De Bruijn matrices from applicative structures 
and syntagmatic neighborhood-sets restrictively.  
I claim that comparing the two constructions 
documents that each step in the construction 
would modify the resulting matrices in 
equivalent ways, and so the set of 
matrices generated from applicative structures 
will be identical to that generated from 
syntagmatic neighborhood-sets %-- both are 
precisely the lambda-feasible matrices 
according to the earlier definition of 
lambda-feasibility which precludes infinite 
recursion.  
`theorem-proof`

In other words, lambda-restricted syntagmatic 
neighborhood-sets are a graph-theoretic encoding 
of applicative structures in the classical 
lambda-calculus sense.
`p`


`p.
I have formally reviewed `q.classical` applicative structures, 
however, primarily to demonstrate that 
within the context of channel systems these 
structures represent only one (relatively restricted) 
model of function/procedure-application, characterized 
by a simplified (input/output) channel semantics 
(plus at most one output per function).  The constructions 
for channelized neighborhoods and syntagmatic neighborhood-sets 
can be naturally extended to more expressive channel systems, 
yielding (so I claim) graph representations which are 
more consistent with actual programming languages 
(whereas lambda calculus models a kind of abstract 
programming language for purposes of mathematical 
treatment).    
`p`


`p.
There are at least two significant extensions which can be 
made when transitioning from `q.lambda-restricted` channel 
systems to ones that are more free-form: first, 
channels can have multiple kinds (beyond just 
input and output) and, second, it is possible 
for the output to one procedure to be a 
`i.function value` which in turn is applied to 
other arguments.  The latter scenario implies 
that an output node (in one neighborhood) 
can be linked (via a directed edge) to 
the `i.central` node of a different neighborhood, 
not just to one of its argument nodes 
(in this case the central node would not be 
labeled with a string denoting a function-name, 
but rather would receive a function-value 
from that output edge).   

`defin -> Syntagmatic Graphs ->> Consider syntagmatic 
neighborhood-sets as above, with the following 
generalizations: each component syntagmatic neighborhood-set 
may have channels of multiple kinds (though we can 
maintain the stipulation that each neighborhood has at 
most one channel of each kind), and the central/procedural 
node of a neighborhood may have an incoming edge that 
passes a function value assigned to that node.  
This latter possibility can be accommodated by defining a 
special `q.function value` channel kind which can be 
occupied by at most one edge; the non-central node 
incident to that edge can be considered a special 
form of argument node, one which has an incoming 
edge from another neighborhood (representing a 
passed function-value), but instead of this 
argument node being an `i.input` parameter to 
the central-node's procedure, it is a function 
value representing the actual procedure which 
that node iconifies.  This formation would thereby 
model situations where nodes encapsulate numerous 
possible procedures and the actual procedure 
designated is dynamically calculated (presumably 
just prior to the function-call which is 
notated via the syntagmatic graph). ;;
`p`


`subsection.Semantic Interpretation of Syntagmatic Graphs`
`p.
Intuitively, each Syntagmatic Graph describes a structure connecting 
one or more procedure-calls, interconnected by parameter 
passing %-- outputs from one procedure become inputs to another.  
Working in a more flexible `q.channel system`/, however, 
the connections between such calls may have more 
subtle relationships than output-to-input chaining.  
For example, multiple nodes in a syntagmatic 
graph may represent the same `q.variable` 
(cf. `q.shared supplication` nodes from earlier) which in turn 
might be modified by one procedure, so an input given a 
new value by a procedure and then passed to another 
procedure is a form of inter-procedure precedence %-- the 
effects of the first are consequential to the second %-- 
even if the two are not linked by an explicit 
output-to-input handoff.  There are the kinds 
of scenarios that a procedure-call semantics 
reflection actual programming languages (as opposed 
more strictly mathematical function theories) should address.
`p`


`p.
Assuming we remain within the context of applicative structures 
proper, the point of these formations is to represent 
the idea of functions (in some sense) which take input 
values.  Notating this process via variable-symbols 
allows function composition (and arity-reducing projections) 
to be described; thus we have `q.substitution`/, 
replacing variable-symbols with concrete values.  When 
all free variables in an expression describing 
(potentially nested) function-applications are 
thus substituted, we have sufficient information to 
evaluate the function, or %-- in the sense of 
lambda-calculus `q.beta` reduction %-- reduce 
(or `q.collapse`/) the 
aggregate expression to a single resulting value.  
This is the central dynamic figures by applicative 
structures in their simpler, classical sense 
%-- input parameters yield applications which 
`q.reduce` the inputs to a single output value, 
that may in turn be input to other functions.  
Chaining inputs and outputs in this sense 
engenders a model of computations as graphs, 
where edges encode have values travel 
between applications, with multiple inputs 
potentially each being outputs from 
multiple precursor function-applications.
`p`



`p.
The semantic interpretation I adopt here for `q.syntagmatic` 
graphs is noticeably different than this model 
(I have analyzed the specific differences in `cite<Chapter6>;).  
Rather than taking input/output as a fundamental 
divide within function-parameters, I consider 
more general channel systems here.  Note that in 
contrast to (more typical) graph-representations 
of computation where outputs are marked by 
directed edges `i.away from` procedure nodes, 
in Syntagmatic Graphs (hereafter `SG;s) all edges point `i.into` 
procedure-nodes, including those embodying 
`q.outputs` (`cite<Chapter6>; has some comments 
about why this can make sense).  More elaborate 
rationales for the notational and interpretive 
differences between `SG;s and 
other procedure-models is tangential to the 
current chapter, so I'll focus instead 
on outlining the semantic interpretation itself. 

`anondefin.
Call a `i.marking` of a Syntagmatic 
Graph` to be an association between 
some of its nodes and a collection of typed 
values, against some type system.  Markings 
might be context-dependent; that is, a 
graph could be subject to multiple 
markings concurrently, each restricted 
to one context.  I'll say that values 
are `i.bound to` nodes, but indirectly, 
as explained in the following.
`anondefin`

`anondefin.
Syntagmatic Graph nodes can be associated with 
`i.types` and `i.states`/.  For the present, 
I will not rigorously define `q.types`/, but 
I'll comment that types are introduced 
`visavis; nodes `i.through` states.  
For any type, each possible instance of that type 
is a potential `i.state` for Syntagmatic Graph nodes, 
the state of being `q.occupied` by that specific 
value.  However, not every node-state need correspond 
to a type.  In particular, nodes can have a state 
corresponding to a `q.void` or lack-of-value.
`anondefin`

That is, I approach the notion of `q.void` as a `i.state` 
possibly evinced by nodes, rather than through type 
systems themselves.  That is, we do not need 
a `q.bottom` or `q.nothing` type with some 
special value selected (essentially by fiat) 
to represent non-initialization or pre-initialization.  
I find it more semantically coherent to 
express such `q.nothing` states as the 
state of `i.having no value`/, rather than 
as the presence of a construed `q.nothing` value 
(and nothing-type which it instantiates).
`p`


`p.
Against this background, the semantics of procedure-calls 
can be expressed via before-and-after states 
for each argument-node incident to a procedure-node.  
The procedure's semantics is, as such, 
the cumulative state-changes, or `i.change in marking state`/, 
effectuated by the procedure.  In a general case, 
some of these changes will be void (no-value state) nodes 
transitioning to having a value; these would 
typically correspond to `q.output` nodes in 
classical applicative models.  However 
%-- accommodating scenarios like mutable references 
%-- nodes with the state of binding to some value 
could migrate to the state of binding to a `i.different` 
value.  In general, notions such as input-versus-output 
are expressed in this system `q.semantically` as a 
manifestation of state-changes, rather than 
`q.syntactically` as edge-direction or arrows assigning 
different directions to inputs versus outputs.
`p`


`p.
Roughly as an analog to `q.beta` reduction, I propose 
the term `i.digamma` reduction to express 
marking-state changes due to a procedure-call.  
A digamma reduction is the cumulative state-change 
in all nodes affected by the procedure (or, seen 
syntactically, all nodes adjacent to the procedure-node).  
For multi- (channelized) neighborhood `SG;s, 
digamma reduction happens in multiple stages, each 
contextualized to a single neighborhood.  Note 
that I like the term `q.digamma reduction` partly 
as an oblique reference to `q.sigma` calculus 
(an object-oriented extension to lambda calculus, 
and the Greek digamma numeric symbol looks like a enlarged 
lower-case sigma) and partly because `q.gamma` is a common 
symbol for graphs, so `q.digamma` suggests `q.two graphs`/, 
or a computational interaction between one graph 
assembling a procedure-call and one graph implementing it.      
`p`


`p.
Semantically, then, `SG;s model digamma 
reductions `i.in sequence`/, each localized to 
individual channelized neighborhoods.  This 
idea can then be extended to sequences of 
`SG;s in turn.  We are getting closer 
to representing actual computer code; one further 
step might then be incorporating something like 
`q.stack frames`/.  Assuming that `SG;s 
are represented `i.in sequence` and moreover 
that such sequences occur in the context 
of a `i.symbol scope`/, or an environment 
where string labels can serve as `q.carriers` 
for typed values; I'll sometimes refer to `q.`SG;-scopes`/, 
scopes contextualizing `SG; sequences wherein symbols 
designated carriers which (when in an initialized 
state) hold concrete values.  In particular, assume that 
carriers can reveal `q.carrier states` 
reciprocating the states defined 
on nodes earlier: for any type there is a 
spectrum of states equivalent to each type-instance, 
but there are also states involving 
`i.lack` of typed values (the generic example 
of such a state I'll call `i.pre-initialized`/).  
The states available for carriers can be called 
`q.type-based` in that they are organized around 
typed values but include non-type states as well.
`SG; nodes can then take on states 
by associating with carriers such that 
carrier-state propagates to the nodes.  
Nodes are in `q.shared supplication` if 
their carriers are syncronized to be 
perpetually in the same state (this can be 
considered a refinement of the earlier definition).
Symbols in `SG;-scopes can derive from multiple 
sources: assume that scopes' smybol-list can be 
grouped into `i.declared` symbols internal to the 
scope, `i.argument` symbols bound to 
procedures' signatures, and (potentially) `i.global`.
or perhaps `q.ambient` symbols 
shared among multiple scopes (I'll use 
`q.ambient` for the general case and `q.global` 
for analogs to `q.global variables` in 
typical programming language).  

`defin -> Tripartite Scope ->>  I'll call `SG; scopes 
`i.tripartite` in that they are mappings from symbol-names 
to carriers (or groups of state-syncronized carriers) 
with type-based states, and symbols can be 
classified as `i.declared`/, `i.argument`/, 
or `i.ambient`/.  The semantics of these groupings 
will be clarified below. ;;

`anondefin.
I will say an `q.`SG;-described procedure` 
(or just `q.procedure` when the `SG; topic is obvious) 
is an `SG;-sequence unfolding in a tripartite scope 
where argument-symbols are bound to values 
held by carriers in a `i.different` procedure (or, 
recursively, the same procedure in a different context, 
viz., different symbol-bindings).  Such 
interactions between `SG;-described procedures 
models the semantics of procedure calls.  
Of course, procedure-calls in turn are modeled by 
channelized neighborhoods in individual `SG;s.  
Specifically, channelized neighborhoods embody 
procedure-calls wherein peripheral nodes' 
carrier-states in the `i.calling` neighbothood 
become the basis for initializing carriers 
in the called procedure; specifically, 
the carriers bound to argument symbols in the 
latter's tripartite scope.  The switch 
in execution context from the calling to the 
called graph is analogous to binding symbols 
to values in process calculii, or beta-substitution 
in lambda calculus, except that carriers' 
connections (via their argument nodes) to 
procedure-nodes is organized via channels, 
which can affect procedure-call semantics.  
For example, it may be stipulated that 
carriers in channels modeling `q.outputs` are 
understood to be in pre-initialized state 
for the duration of called procedures' 
execution, and attempting to utilize 
(e.g., read values from) such carriers 
`i.within` the procedure is a logical error.  
There are various ways of enforcing this 
kind of restriction, of course, 
but channels offer a convenient 
representation of the relevant 
constraints viewed under a semantic interpretation.
`anondefin`
`p`


`p.
We now have a semantic interpretation of channelized 
neighborhoods: each such neighborhood embodies a 
procedure-call, which entails binding call-site 
carriers to procedures' argument symbols and 
then carrying through other procedure 
calls notated through `SG;s associated with 
the `i.called` procedures.  When they are 
finished, the original carriers will (potentially) 
be in a modified state, so the called 
procedure effectuates a `q.digamma reduction` 
on carriers around the calling site.  
State-changes then propagate across 
channelized neighborhoods insofar as 
values from output-like channels 
in one neighborhood are handed off 
to input-like channels in later-executed 
neighborhoods.  Each `SG; has a 
`q.top-level` neighborhood which is the 
last to `q.execute` (viz., compel a 
procedure-call) or, from a different 
perspective, the root of a directed acyclic 
graph showing carrier hand-offs and shared 
supplication across channelized neighborhoods.  
Note that output channels on the 
top-level neighborhood cannot be 
connected to other neighborhoods 
in the form of procedure outputs become 
inputs to a subsequent procedure 
(there is none) %-- so either 
top-level procedure-calls are 
useful solely for their side-effects 
or the carriers in their output-like 
channels are bound to symbols in the 
current scope.  In the general case, 
I'll say that an `SG; is `i.anchored` 
by one or more symbols if those 
symbols (strictly, the carriers 
associated with them) acquire values 
from the `SG;s top-level neighborhood 
(particularly from the channels 
there which obey output-like semantics).  
`p`

`p.
The mechanism just described constitutes 
how `i.declared` symbols in an 
`SG; tripartite scope can acquire values; 
they become initialized from procedure-calls 
enacted `i.in` the procedure, rather than 
from arguments passed `i.to` the procedure 
(or from an ambient space of values 
visible from multiple scopes).  Once initialized, 
declared symbols can then supply values 
for procedures called `i.inside` a procedure 
%--that is, an argument-symbol in one 
procedure can derived from a declared symbol 
in a calling procedure.  It is reasonable 
to assume in the typical case that all procedure-symbols 
(whether declared, argument, or ambient) hold 
values which originate from a `i.declared` symbol 
somewhere, so the anchoring-points 
where declared symbols are first initialized 
represent precisely the points in any holistic 
collection of `SG;-described procedures where 
new values are `q.intoduced` into the system.
`p`


`p.
This last point has implications for how we 
theorize `i.typed values` as well.  Implicitly 
here I have assumed that we are operating in a 
strongly-typed system where types and 
values are interconnected: any value is a 
`i.typed` value (the instantiation of a type) and 
any type exists by virtue of its possible values.  
I assume that in the general case types are 
not equivalent to their `i.extensions`/, 
that is, to any fixed set of values.  
There are indeed some types whose extensions 
are fully `q.enumerable`/, so we can specify 
exactly how many possible instances a type has: 
the type corresponding to signed one-byte 
integers, for example, denotes precisely 
the set of numbers at least -128 and at most 127 
(the analogous range for `i.unsigned` bytes 
is 0-255).  However, for types such as 
`i.lists` of integers, the extension cannot 
be specified `i.a priori`/, in the sense that 
it is impossible to know from a specific 
computing environment (at a specific moment 
in time) whether or not a particular 
value (here, a particular list of integers) 
which logically fits the type's criteria 
can in fact be represented.  The size of a list 
which may be instantiated is limited by 
factors such as the computer's available 
memory; in theory, the extent of a collection-type 
like `q.list of integers` is infinitely large, 
because there is no rule to group 
all potential such lists into a finite set 
(if there were, take the longest list, an 
an integer to the end, yielding a new 
list which has no reason not to be included in the 
first place).  I assume moreover that 
we are working in a `i.non-constructive` type 
environment where we cannot consider types' 
extensions as a logical construction abstracted 
from computational feasibility in concrete 
computing environments.  We can discuss 
types' `i.hypothetical` extension, which may indeed 
be infinite, but I assume we need some 
other mechanism to manage uncertainties 
regarding types `i.actual` extension (in 
some contexts, such as lazy-evaluated lists 
or ranges, working directly with a logical 
model abstracted from actual realizability 
is appropriate, but in contrast to 
functional-programming theory, for example, 
I do not take such `q.constructive` formalizations 
as an essential aspect to the relevant 
type systems).`footnote.
See `cite<CCC>;, chapter 4, for a more 
extensive analysis of non-constructive 
type systems and why assuming 
that types are non-constructive in general 
(rather than the opposite) is practically appropriate. 
`footnote`
`p`


`p.
So as to give a rigorous semantics to `i.non-constructive` 
types, instead, I leverage the interconnections 
between types, symbols, and `SG;-nodes insofar as 
we are working in an `SG; context.  Recall the 
heuristic that all symbols acquire values that 
originate from declared symbols, which in turn 
have a specific initialization-point 
(although some symbols might be initialized 
by non-constant reference, passed into a 
procedure for initialization rather than 
being bound to a procedure-result, the 
values they are bound `i.to` in this 
case will itself be determined 
by an anchoring-initialization, either 
in that procedure itself or some 
other procedure called in turn; so, 
even if not all declared symbols 
receive their values via anchoring, 
all declared symbols receive values 
which `i.originate` from an anchoring 
somewhere).  In other words, when we say 
that a symbol has some type value, 
we mean that a symbol's value is 
derived from an initialization from anchoring 
point, where a declared symbol's value 
was set via digamma reduction at an anchoring 
point %-- the symbol was associated with a node 
which transitioned from a pre-initialized to a 
type-bound state as a result of binding to nodes 
in an output-like channel.  Symbols may be 
`q.re-anchored`/, initialized to new values 
due to anchoring points, but (assume we 
work in a fixed-type environment) symbols 
and carriers retain the same types 
once they are initialized, so long as they 
remain in an initialized state.  Types 
themselves, then, can be construed 
in terms of continuities in carrier-state: once 
a carrier is initialized to be in a 
state determined by a type-instance, it will 
remain in states determined by the same type 
so long as it is initialized at all; and 
all initializations can be traced to 
anchoring-points, together with 
synchronization among states of distinct 
carriers.  We do not need to theorize types' logical 
extensions in general, because we aren't concerned 
with the totality of a type's possible 
values, only with the states directly 
associated with the type at specific 
anchoring-initialization sites. 
`p`



`p.
Many programming languages distinguish between `i.constructors`/, 
which are intrinsic to a type's definition, from other 
procedures that return values of a given type.  
To the degree that this distinction is in effect, 
any value can be traced back specifically to a 
`i.constructor` for its type.  To be sure, 
a value might be bound to the output of a 
`i.non`/-constructor procedure, but in 
that case this return value will have 
been formed initially by a constructor 
proper called in that procedure, or perhaps 
in a procedure it in turn calls, iteratively.  
In a channel system we can leverage a comparable 
distinction be defining a special kind of 
channel, which functions like a normal 
output except that procedures whose signature 
includes such a channel are considered 
to be constructors, or analogous to constructors 
(in `cite<x>; I used the term `q.co-constructor` 
to suggest allowing procedures being analyzed as if 
they were constructors even in programming 
contexts where, for technical reasons, they 
would not be classified as such according to 
a programming language whose code is being analyzed).  
For generality I'll call procedures with 
constructor channels `q.co-constructors` with the 
assumption that different environments may be more 
or less lenient in conditions where co-constructors 
may be declared (only in code specifically 
defining a type, say, in contexts where a narrower 
policy is warranted, but without imposing restrictions 
along these lines `i.a priori`/).
`p`


`p.
The significance of co-constructor channels 
is that they allow us to pinpoint the 
specific anchorings where new typed values 
are assigned `i.ab initio`/.  Assuming an 
`SG; system uses co-constructors, 
for any carrier in a type-bound state 
we can be sure that the value traces back 
to a digamma reduction on a carrier 
specifically situated in a co-constructor 
channel.  As such, our semantics 
for types may not give a set-theoretic 
account of types' extensions, but we 
do have a mechanism for locating the 
origination point for all 
typed values.  In particular, values 
are acquired via carrier state 
changes localized to co-constructor 
channels; such a change only occurs 
when a procedure has completed with an 
initialized value in that channel 
available to be bound to its 
eventual symbol.  Any value 
therefore witnesses the fact that 
a procedure with a co-constructor 
channel completed and 
triggered a state-change accordingly.  
If a type system is designed to 
recognize types as (associated with) 
certain contracts, such that 
a type being instantiated 
confirms certain properties 
about the value thereby 
manifest, we can recognize 
such contracts in an `SG;-style 
system so long as co-constructors 
adhere to type-specific contracts 
in the course of depositing 
values in a constructor channel.
`p`

`p.
Once a carrier with a given type is initialized, 
its value can of course be handed off to other 
carriers assigned the same type (or potentially 
a supertype thereof, or some other 
related type wherein casting is possible).  
Channel systems and `SG; representations 
do not preclude polymorphism %-- 
specifically, the labels assigned to procedure-nodes 
need not uniquely identify one single 
procedure available to be called, but may 
instead provide a premise from which to 
select one of numerous available 
procedures, each with the same name %-- 
so type resolution may come into effect: 
if there are multiple candidate procedures, 
the one whose signature best matches 
the procedure-calls argument types is 
selected.  In the context of channels, 
such disambiguation extends to recognize 
different channel-kinds, which become 
part of a function's signature.  In 
effect, each node in a channelized 
neighborhood has a type (based on 
its carrier-state) that might be 
matched against candidate procedures' 
signatures.  Types therefore a 
consequential primarily in 
the context of overload-resolution.  
The issue here is not types' extension 
per se (two types having the same extension 
would have no bearing on their 
appropriateness for matching 
call-site neighborhoods to procedure 
signatures; and co-extensive types 
are no interconvertible unless such 
casts are implicitly declared according 
to the same conventions as casts between 
types with different extensions).  However, 
since successfully matching call-sites 
to signatures implies that all 
affected carriers' types are suitably 
aligned, whatever guarantees on 
carrier-states are implicit in 
type-bound states becomes transferred from 
the calling to the called procedure, insofar as 
the formers' carrier-states carry over to 
the latter's argument nodes.  Therefore, 
carrier-states associated with being initialized 
according to a given type propagate 
from co-constructor anchorings across all 
subsequent procedure-calls, with type-specific 
guarantees propagated alongside.
`p`

`p.  
In effect, types' semantics in such a framework 
is based on the premise that once there 
is an `i.originating` initialization 
via digamma reduction in a co-constructor 
channel then there is one specific carrier 
whose state is thereby changed, and 
subsequently (presumably) a series of 
further procedure-calls where that 
originating carrier's state become 
synchronized with subsequent carriers 
in subsequent procedure-calls.  
Each type is essentially a premise 
warranting the propagation of 
this shared carrier-state: the type's 
presence as a condition on procedure-signatures 
or on declared symbols indicates that a 
carrier whose state is synchronized in 
accordance with such propagation would 
be in a valid state according to the 
expectations of the procedure 
which uses the corresponding value.  
For the sake of discussion, I'll 
refer to this approach to the 
issue of type-semantics as a 
`i.propagation semantics`/, in contrast 
to a `i.set-theoretic` semantics 
which would construe types' semantic 
interpretations in terms of their 
extensions, or a `i.constructive` 
semantics which would read type 
semantics through the logic of 
constructive patterns that could give 
rise to values of a given type.   
The essential point about 
propagation semantics is that 
type contracts would be checked 
at origination points (to the degree 
that they are in effect) and must be 
presumed to have been enforced whenever 
there is an originating digamma reduction 
in the first place, and that subsequently 
any use of a type as a polymorhpism-disambiguating 
device should proceed under the assumption that 
any carrier-state inherited as synchronized 
with the state of an originating carrier 
in this sense should be deemed a valid 
state for the called procedure's purposes. 
`p`


`p.
This discussion regarding type semantics has passed 
over some non-trivial details, such as issues 
of type casting and inheritance, which I will 
briefly address in the next subsection.
`p`


`subsection.Syntagmatic Graph Sequences as a Virtual Machine Protocol`
`p.
A plausible objection to the above presentation is 
that it is arguably burdened with a nontrivial list 
of special terminology and representational 
conventions to define constructions which are 
not radically different from traditional 
notions of, say, stack frames and lambda-abstraction.  
Beyond just expanding from alternating inputs and outputs 
to more flexible `q.channel` systems, I have 
constructed a semantic interpretation for 
these system based on formulations such as 
type-based states and channelized neighborhoods, 
without clarifying the theoretical merits of 
this overall semantic presentation.  My contention 
is that the full semantic details (not just, say, 
extending applicative structures to richer 
channel systems) become valuable in the 
context of Virtual Machines.  I will 
attempt to warrant this claim by addressing 
`VM; implementation, in particular, 
to conclude this chapter.
`p`


`p.
I am representing Syntagmatic Graphs as hypergraphs, 
partly because `i.channels` representing a grouping 
operation (gathering multiple edges, by analogous 
to hypernodes being sets of nodes %-- although 
channels are not identical to hyper`i.edges`/, 
wherein `i.one` edge spans multiple nodes), although 
I leave open the possibility of `SG; nodes 
having internal structure (i.e., becoming hypernodes).  
However, such hypergraph representation is 
mostly heuristic; technically, I consider 
hypergraphs to be essentially a visual shorthand 
for `i.virtual machine` constructions.  Here I appeal 
to comments I made in Section 1: a hypergraph 
(or, in essence, any strututred representation) 
essentially iconifies the series of steps 
needed to construct the graph in its precise state.  
For example, focusing on channelized neighborhoods, 
the elements of such structures %-- procedure nodes, 
peripheral nodes, and channels %-- correlate with 
operations setting out these details sequentially 
(identifying a procedure node, e.g., by label; 
identifying a channel-kind to serve as a 
context for subsequent peripheral nodes; initializing 
the latter nodes; switching to a different channel-kind 
as needed; and so on).  This outline could be 
expanded to Syntagmatic Graphs in general by aggregating 
multiple channelized neighborhoods and then 
asserting where carrier handoffs (via cross-neighorhood 
edges) occur, to `SG;-sequences by constructing 
multiple `SG;s along with their respective anchorings; 
and finally to procedure-descriptions by enumerating 
declared, argument, and ambient symbols in a 
tripartite scope.  From this perspective, 
`SG; procedures can be seen as compact representations 
summarizing sequences of `VM; operations, or 
perhaps as configurations which guide the construction of 
`VM; procedures by defining and end-goal toward which 
`VM; operations approach incrementally.        
`p`


`p.
Ordinary `VM;s, of course, depend on such concepts as 
stack frames and procedure calls; one of the fundamental 
operations (or sequences thereof) typical `VM;s 
carry out involves push input values onto a stack 
and then redirecting to the address of a procedure 
which uses them, at least insofar as the `VM; in 
question emulates assembly code to provide a 
runtime for higher-level programming languages.  
Modeling procedure-calls via hypergraphs, and 
in particular `i.channels` (in the sense I propose here) 
adds representational parameters to this model, 
by analogy to how hypergraphs present a richer 
expressive tableau than other metamodels for 
encoding data structures.  The benefits of added 
metamodeling detail ni the procedire-call 
context actually help illustrate why 
expressivity pays dividends with respect to 
data structures (e.g., in query evaluation) as well. 
`p`


`p.
The primary rationale for `VM;s is to execute code, of course 
(whether scripts, queries, or some other programming 
category that does not feat neatly into one or the 
other, such as workflows, or one or both ends of 
remote-service functionality).  Secondarily, `VM; compilation 
can support static code analysis even if the 
intermediate code is not actually run.  Code analysis factors into 
execution as well, at least to the degree that a 
`VM; can internally support runtime checks and 
guarantees %-- insert debugging breaks where certain 
conditions are met, preventing code from running 
in certain circumstances, allowing procedures 
to make contract-like assumptions (stronger than 
type-checks alone could enforce), and so on.  
For example, stipulations that values should fit 
within a fixed range (narrower than theoretical 
posssible via their types) could potentially 
be verified or implemented by weaving 
`q.gatekeeping` code into `VM; operation-sequences.  
In short, `VM;s acquire more flexible capabilities 
%-- they present opportunities to implement Requirements 
Engineering style features, even if only via 
add-ons %-- to the degree that they present 
op-sets and data models configured for static 
and dynamic/runtime analysis.  These 
observations motivate my proposals to augment 
the modeling parameters for `VM;s (in procedure-call 
contexts) via formulations such as 
channels and carrier-state.  
`p`


`p.
To be sure, a suite of static-analysis capabilities 
is endemic to traditional (e.g., stack-based) 
`VM;s as well (reachability, control paths, initialization 
guarantees, etc.).  I would suggest, however, that 
future generations of `VM; technology will seek 
a broader scope `visavis; contexts where cde analysis 
is applied, considering `GUI; programming, 
Cyber-Physical Systems, multi-modal front-ends, 
`AI; integration, and other capabilities that 
may be unified under the rubric of `q.Industry 4.0`/.  
Future `VM; architecture may be optimized 
for the `i.intersection` of such User-Experience and 
multi-modality concerns with traditional 
static analysis and Requirements Engineering. 
`p`


`p.
Consider the case of Channel Systems.  According 
to the `SG; model I have outlined here, 
procedure-calls are represented via channelized 
neighborhoods, and in such neighborhoods 
all edges incident to a central (`q.procedure`/) 
node lie within a channel.  In `VM; translation, 
this configuration implies that procedural 
stack frames are split up into multiple channels, 
so that whenever an argument is `q.pushed` as a 
parameter for some near-future call this 
action occurs in the context of a specific 
channel-kind.  As a result, argument-push 
operations can be analyzed in terms of semantic 
procols specific to currently active 
channel.  For example, `VM;s might want to 
add extra information in the 
context of channels representing `q.message receivers` 
in Object-Oriented contexts (which 
I have elsewhere called `q.sigma` channels, essentially 
the `b.this` or `b.self` of languages like `Cpp;, `Java;, 
and `Ruby;), particularly when such channels 
contain more than one node (insofar as most languages 
providing kernel functions accessed from a `VM; 
do not allow multiple `b.this` objects; the 
demo code for this part's chapters has examples of 
how to emulate multi-sigma calls in `Cpp;, but 
in general it would presumably take extra 
effort to map multi-sigma `VM; calls to the underlying 
native functions).  Moreover, channel-specific 
semantics can be implemented on an extensible 
basis, allowing the `VM;s to be augmented 
with special-purpose channel kinds which supply their 
own functionality for managing (in effect) stack 
frames when such channels are active.
`p`

`p.
Furthermore, channels can act as a grouping mechanism 
tieing together procedure arguments which are logically 
interconnected (to a greater degree than merely co-existing 
as arguments).  An example would be unit/scale-decorated 
types: suppose a function calculates a formula which 
requires two arguments with the same dimension and measurement 
units (kilometers, say) plus a third argument which 
is just a scalar; evidently the first two arguments are 
mutually constrained in a fashion distinct from the third.  
Or consider a calculation with a scalar plus two mathematical vectors 
which should have the same length %-- the latter condition 
might be modeled with a dependent-type construction on the second 
vector, or some runtime check for the two vectors together, 
in either case demarcating the vectors as paired arguments 
distinct from the scalar.  One strategy to satisfy these 
runtime use-cases would be to implement channel semantics
where special guarantees (finer than type-checking alone) 
are enforced while the channels are being populated.`footnote.
In this case the one-channel-kind-per-neighborhood restriction 
might be relaxed, since mutual connections could exist 
between (say) some input parameters and not others; or 
one could adopt something like `q.subchannels` which 
semantically refine the channels around them.
`footnote`
`p`

`p.
Doubtless, some of the features enabled by channels 
could be achieved via other means.`footnote.
For example, 
aggregating input parameters as mentioned last paragraph 
might be achieved alternatively via explicit 
dependent typing or via smashing tuples into 
single (aggregate) arguments.  However, depedent 
types are notoriously difficult to implement 
in the context of Software Language Engineering, 
and the latter alternative could be syntactically 
unwieldy (if done explicitly in source code) 
or complex in its own right to implement 
(if done behind the scenes).
`footnote`  Channels, 
however, present a convenient interface for 
organizing the range of functionality 
entailed by the relevent channel semantics.  
Insofar as every argument-node in an `SG; 
occurs in the context of a channel, before 
any such nodes are set in place the `VM; 
would first construct a channel of the 
relevant kind; that is, there is a specific 
operation to `q.open` a channel given its 
kind.  This operation is therefore an `VM;-site 
(for static or dynamic analysis) that 
can be targeted by extension code enforcing or 
examining channel semantics.  Once channels 
are opened, operations exist to 
indicate the type and value-source for nodes 
added to the channel.  Because these latter 
operations always occur in the context of a 
specific channel-kind, they can be filtered or 
re-implemented based on the channel kind 
in effect, so extensions could modify the 
treatment of certain channels while preserving 
the underlying `VM; implementation in most 
cases.  For example, a `VM; extension could 
modify exception-handling protocols by 
re-implementing operations for inserting 
nodes into (or, on the call side, initializing 
carriers in) special `q.exception` channels, 
which are partitioned from `q.ordinary` 
output channels (given the obvious behavioral contrast 
between exiting via exceptions versus normal 
returns).  Procedures typically involve multiple 
channels, so there are `VM; operations 
for `q.closing` one channel and opening another 
of a different kind, which results in subsequent 
operations occurring in the context of a 
different semantics.  Explicitly introducing 
channels as part of the underlying `VM; machinery 
allows variegated channel-semantic protocols 
to be implemented in an organized and extensible manner. 
`p`


`p.
My earlier comments discussed overlapping concerns 
of code-analysis with `AI;, multi-modal interface, 
and other `q.Industry 4.0` concerns.  Last paragraph's 
discussion regarding (for instance) special-purpose 
channel semantics perhaps does not obviously 
connect the idiosyncratic constructions endemic 
to `SG; representation with concerns in the 
latter sense, so I'll try to present a case 
more concretely.  Consider first the 
issue of `q.reactive programming` in `GUI; 
contexts.  Analysis of procedures as a series 
of calls to other procedures %-- what I am 
calling `SG;-sequence decriptions %-- fails 
to directly address how call-sequences 
in this sense fit into over application execution.  
In general, an application is not a one-dimensional 
`i.program` which simply executes some sequence of 
operations and then terminates.  Instead, applications 
construct a graphical and runtime environment 
and then wait for user-initiated actions, 
responding accordingly, and resetting to a 
passive state awaiting futher user actions.  
Users signal their intention for applications 
to take specific steps via interactions 
which can generically be called 
`i.gestures` (e.g., typing something 
via a keypad, or clicking somewhere via 
a mouse).  Gestures in turn are presented 
to application code as `i.signals` that 
are `i.handled` by implemented procedures.  
The overall programming model entailed 
by composing applications as collections 
of procedures poised to handle signals 
%-- rather than fixed operation-sequences 
designed in advanced %-- is generically 
called `i.reactive` programming.  Such a 
programming model introduces a variety of 
issues for `VM; implementation, to the 
degree that `VM;s would be used to model/analyze 
or execute reactive procedures.
`p`


`p.
In particular, the central idea of reactive 
programming is that certain procedures 
are called (or initiated) in response 
to signals (typically those 
due to user gestures, though certain 
signals may be prompted by non-user 
changes to the current environment 
which might be relevant for applications, 
such as a sudden loss or gain of 
internet connectivity).  Procedures 
in this sense are not called `i.from other 
procedures`/, so the normal analysis of 
procedure-calls in terms of stack frames 
being transferred from one site to another 
has to be modified.  A canonical approach to 
reactive programming involves `q.signals` 
and `q.slots`/, with the idea that 
instead of one procedure directly calling 
calling another, procedures instead emit 
`q.signals` that are `i.connected` to 
other procedures, which in such contexts 
become `q.slots`/.  Unlike hard-coded 
procedure-calls, signal-to-slot connections 
can be dynamically altered, created, or 
suspended.  Such a mechanism depends on a 
centralized routing component 
to observe when a signal has been emitted 
(for instance, added on to an `q.event queue`/) 
and transfer control to one or more 
slots registered as connected to that signal.  
Insofar as a centralized processor in this 
sense is active, it can also (in typical 
application-runtime frameworks) 
receive signals originating `i.outside` 
the application, i.e., resulting from 
external conditions apart from one 
`i.procedure` emitting a signals.  Typically, 
such external signals would result from 
user-generated events, such as clicking a 
mouse button or moving the mouse.      
`p`


`p.
In effect, application code based on signals and 
slots includes some procedures which are 
called because they are registered as `q.slots` 
whose signatures match signals that 
may arise from `i.outide` the application 
propert.  These procedures serve as `q.entry points` 
where operations specific to the application 
originate.  In other words, applications 
are environments which, after an initial setup, 
wait in suspension until external signals 
initiate a chain of actions in response.  
The details of such signals cannot be known 
ahead of time %-- for example, one cannot 
say which user gestures will occur (whether 
the user first types something, or moves 
the mouse, or clicks the mouse, etc.) nor 
their specific details (which keyboard keys 
are pressed, which mouse buttons are clicked, 
where on-screen the mouse-cursor is located 
in the latter event, etc.).  Application 
code therefore needs to prepare for 
multiple forms of external signals, 
and to analyze their properties to respond 
correctly (in accord with user intentions 
and/or design requirements).  A left mouse click 
with the cursor hovering over one `GUI; 
element typically signifies a different user 
intent than right-mouse clicks over a 
different element, for example.
`p`


`p.
The unique characteristics of reactive programming 
have numerous consequences for `VM; design.  
First, note that applications typically 
implement many procedures so as to possess 
requisite capabilities to handle user actions.  
When and whether a given procedure is called 
depends on user pragmatics; for instance, a 
procedure involved in saving a file (at least a 
file users know about, as opposed to, 
e.g., a database-related file storing configuration 
information) would only 
be called in circumstances where users signal 
their desire to save files they are 
currently working on.  A well-organized code base 
will specify which procedures could 
potentially be called as the `i.initial` handler 
responding to external signals.  This information 
makes it possible to factor in external 
signaling conditions during code-analysis.  
For example, suppose file-saving is disabled 
for some reason (e.g., the current user does not 
have permission to modify the local file system).  
In that case, procedures which are `i.only` called 
within a call-chain leading from external 
signals specific to saving files would become 
effectively unreachable.  Notions such as 
reachability and execution paths have 
to be evaluated in the context of 
procedures registered as external signal-handlers.
`p`


`p.
Consider a scenario where an application, being upgraded, 
is redesigned to support two different file-handling 
models: one for local filesystems and one for cloud 
storage.  Certain procedures (e.g., one to check 
whether the file has been modified since the time 
of last save) may be relevant for both scenarios; 
others would only be active in contexts where 
cloud-saving is possible (an open internet connection, say) 
or, respectively, local file-system access.  Introducing 
new procedures to manage the cloud-storage case 
alters the applications inter-procedural `q.connectivity`/, 
potentially requring analyses to be updated 
with respect to conditions wherein a certain 
procedure might be called (or might be 
unreachable).  The fact that such information 
`i.about` applications is subject to change 
(insofar as applications are continually 
refined or redesigned) indicates that information 
`i.about` application code should be managed in a 
systmatic fashion.  This might be 
done through `VM; representations directly, 
or at least source code and/or documentation 
`i.available` to `VM; compilers can draw 
meta-data from such sources.  In other words, 
we can assume that applications are engineered 
in coding environments where information 
(including, for example, which procedures 
play the role of external signal-handlers) 
is detailed with enough rigor to be 
read by `VM; compilers and/or runtimes.  
For example, `VM; code could then internally 
incorporate representations related 
to reactive control flow and `GUI; objects 
which iconify pragmas for initiating 
application actions from users' points of view. 
`p`


`p.
Analogous to `q.constructor channels` as 
special-purpose sites notating the 
origination of typed `i.values`/, a 
channel-based `VM; could similarly support 
special-purpose `i.input` channels 
which carry external-signal data 
(we might call these `q.reactive` channels).  
In the same way that `q.co-constructors` 
are declared by assembling constructor 
channels, external-signal handlers could 
then be identified through the presence 
of reactive channels.  This makes it 
easy to identify all execution points 
where external signals could 
trigger procedure-chains: simply observe 
for whenever a reactive channel is `q.opened` 
during the course of building a 
channelized neighborhood.     
`p`


`p.
External signals (as initiators of procedure-chains 
that disrupt applications' passive `q.event-loop` 
states) offer one example of how reactive programming 
and `VM; design intersect, but there are 
related scenarios that could also be mentioned.  
For example, applications' `GUI;s are typically 
seen as a `TwoD; visual extext populated with 
viewable objects that are simultaneously 
spaces to represent pieces of information to 
users (e.g., text, via readable characters; 
or numbers, via printed characters or indicators 
like dials and sliders; or graphics, via image 
or `ThreeD; displays) and origination-points 
for user gestures (e.g., scrolling on a slider 
to increase/decrease a value).  My above comments 
touched on the latter capabilities, but 
interpreting user gestures depends on the 
information currently presented through the 
relevant `GUI; control.  As such, it is 
useful to track systematically how 
`GUI;s are populated with data.  A useful 
maxim is that each class representing 
distinct `GUI; controls should be 
paired with a separate class representing 
the data which is visible within 
such controls.  This is straightforward 
if a control (or in general a `GUI; `q.gadget`/) 
indicates one simple quantity (consider a slider 
that adjusts the zoom-level for viewing an image), 
but even more complex `GUI; areas whose display 
is spread over multiple subcontrols can be associated 
with a multi-field datatype.  Two datatypes 
are then closely interconnected: one represents 
some data-aggregate from a computational 
perspective (ensuring that all fields are properly 
initialized, packaging the data for persistence 
or serialization, and so forth) while the 
other trasnlates the same information into 
visual indicators for user interaction.  
To the degree that `GUI; and application-level 
datatypes are closely aligned, `VM; code 
can be annotated to mark and leverage such 
alignment.      
`p`


`p.
In general %-- continuing these `GUI;-related 
examples %-- inter-connections between `GUI; 
components, user actions, and the datatypes 
shown and affected by either tend to appear 
in multiple contexts.  Consider (as above) 
an image zoom level, indicated (and adjusted) 
by a slider-contol.  It is not uncommon for 
zooming (in and out) also to be initiated by 
small arrows adjacent to a slider, or by 
context-menu options on image-displays themselves, 
or by keystroke sequences like control-plus either 
plus or minus (holding the `q.control` key and 
tapping the plur or minus keys to increase 
or decrease zoom).  Preumably altering zoom levels 
via these other gestures should cause the 
zoom-slider to be adjusted propertionately.  
There are, then, potentially five different 
gestures which might affect zoom levels, each associated 
with `GUI; controls in different ways: directly 
interacting with either a slider or arrow buttons, 
or via a context menu (within an image-display), plus 
via keyboard actions.  Such interrelationships 
should be tracked with some degree of rigor to 
maintain a consistent `q.User Experience`/.  
For example, a common pattern in `GUI; programming 
is top implement `q.tool tips`/, or floating text 
blurbs that appear when users hover over `GUI; controls, 
explaining the purpose and pragmatics associated 
with the control itself.  Insofar as multiple controls 
can be used for image-zoom, each should be 
provided tool-tip text accordingly; it would 
be useful to confirm that policies along these 
links are sustained in an application code-base, 
ideally via static code-analysis.  Important 
gesture/feature connections need be programmed 
in multiple contexts, apart from the underlying 
signal-handlers themselves, including tool-tips, 
help-menu information, documentation, application 
history and undo/redo capabilities, 
notifications (e.g., small labels sometimes 
displayed near the bottom of application
windows clarifying recent actions, such as a 
string confirming that an image was zoom to a particular 
percentage) and unit or integration testing.     
`p`


`p.
Or, consider again issues with application upgrades.  
In the example I suggested `visavis; cloud versus 
filesystem saves, there may be multiple controls 
and data types associated with the cloud 
functionality, including windows where 
users register credentials to access a cloud 
service and strings giving a remote path 
for files on cloud servers %-- each of the 
`GUI; elements presenting the corresponding strings 
(path names, users names, passwords, etc.) are 
logically interconnected by their common utility 
in the guise of cloud file backup.  Meanwhile, the 
`q.image zoom` case can likewise be extended to 
hypothetical `q.upgrade` examples: consider the 
fact that some image displays use modified 
`i.mouse gestures` for zooms, such as cursor up/down 
with a key pressed, often the shift key (these 
pragmas are borrowed from `ThreeD; displays, which try 
to fit the 6 or 12 degrees of freedom in `ThreeD; graphics 
to conventional mouse and keyboard gestures 
%-- rotations; zoom; and translations, i.e. moving 
parallel to x, y, or z axes; each of which can 
occur within the model or within the `q.camera` %-- using 
the keyboard to compensate for mouse-move gestures 
have only `i.two` degrees of freedom).  Consider an 
application which decides to support this latter 
`ThreeD;-style zoom gesture added on to prior 
functionality; this decision would propagate to 
concerns such as those itemized at the end 
of last paragraph (test suites, documentation, 
help menus, undo/redo, etc.).  Applications
which tend to be intuitive and responsive 
from a User Experience point of view 
%-- where it is easy for users to understand 
how to initiate their desired actions 
by interacting with the software, and 
to learn the requisite steps when they 
do not know the pragmas ahead to time 
%-- consistently model the full 
network of interconnections between 
application-level capabilities, 
`GUI; display elements, and user pragmatics.
`p`


`p.
It is also worth pointing out that such 
goals overlap with database engineering.  
Controls to set zoom levels may be 
factored in to a database profile 
insofar as the optimal (or most recent) 
zoom level for viewing an image might 
be stored as one metadata-point in an 
image database.  Similarly, a file's 
cloud-hosted save-path would be a relevant 
piece of information to track in a database 
for which that file is an external resource-object.  
Cyber-Physical networks belong in the 
discussion as well: `CPS; devices, for 
example, tend to have quantitative 
profiles (data ranges and measurement units) 
which would be relevant both for database 
persistence and for `GUI; admin controls.  
When a `GUI; indicator models 
`CPS; sensor readings or actuator settings 
(a thermostate temperature level, say) the 
relevant value-range and units 
(e.g., Fahrenheit or Celcius) should 
be explicated in `GUI; code (obviously, an 
indicator needs to know how each value to 
be displayed compares to valid minima/maxima, 
and should identify and clarify for 
the user, perhaps supporting alternation between, 
Fahrenheit/Celcius, or metric/imperial, and so forth).
`p`


`p.
User Experience (`UX;) sometimes appears to be treated 
as if it were a stylistic and subjective 
dimension to software engineering more 
than a technical or mathematical problem, but 
effective User Interface design %-- and its 
interconnections with such themes as database 
engineering and `CPS; networks %-- hopefully 
show that rigorous `UX; engineering requires 
highly structured models of software and `GUI; 
components, as well as application-level data types, 
with their interconnections and value-synchronizations.  
There are various ways to enforce disciplined 
engineering standards to sustain quality `UX;, 
but at least one set of tools for this 
purpose can derive from `VM; resources 
%-- whether in the form of script-like layers 
interposed between `GUI; elements and application 
procedures, or runtimes for unit-testing and 
prototyping, or representations of application 
code for static analysis (or some combination).   
`p`


`p.
I contend that trends such as Industry 4.0 
foretell an increasing convergence between 
technologies related to User Experience and 
User Interface, Cyber-Physical Systems, `ThreeD; 
graphics, and Requirements Engineering.  
The forces behind such dynamics reflect 
how digitization and computer analytics 
has the power to increasingly shapes 
industries such as manufacturing, architecture, 
green tech, and scientific research.  Modern graphics 
cards can fluidly display interactive `ThreeD; 
models of industrial parts (on a small scale) 
to building renderings and urban designs 
(on a large scale) and everything in between, allowing 
users to visualize planned or manufactured objects/spaces 
for design, training, or simulations.  Digital `q.twins`/, 
moreover, serve as presentations or anchoring-points for 
data-aggregates specifying objects' or designs' 
requirements and specifications.  The digital twin 
concept implicitly posits relationships 
between an object's or design's `ThreeD; spatial 
form and its functional purpose, material/mechanical 
properties, fault tolerance, energy consumptions, and 
similar usability and quality-assurance metrics.  
Stakeholders can use software both to form 
visual images of materials and spaces as 
physical environments and as functionally 
organized systems, with software being able 
to migrate between more geometric (e.g., `ThreeD; 
rendering) and more data-oriented (e.g., specs-table) 
modes.  Given these application capabilities, digital 
platforms can simulate and analyze physical 
and/or functional systems with an unprecedented 
degree of computational accuracy and, simultaneously, 
user interaction, allowing software ecosystems to be 
a larger presence in industria/architectural 
(and etc.) planning, construction, and maintenance. 
`p`


`p.
I do not sketch this account from the 
point of view of advocacy, but instead 
to point out that maximizing the benefits 
of existing digitization capabilities 
depends on software which systematically 
and interactive unifies visualization 
and data curation/analytic aspects 
(e.g., `ThreeD; graphics alongside 
specs presentations), which in turn 
calls for advanced database and 
application-development capabilities, 
at least if software fitting these 
characterizations is to be developed 
in a cost-effective, extensible/adaptible, 
and widely available manner.  I have emphasized 
how Virtual Machine engineering can fit into 
this overall seachange in application-development 
requirements and standards, with amore 
holistic attention to such details 
as database query-processing and `GUI; 
design than were, arguably, emphasized 
by earlier `VM; paradigms.  
`p`



`p.

`p`



