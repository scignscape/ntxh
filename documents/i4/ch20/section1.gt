`section.Hypergraph Data Modeling`

`p.
I claimed in the introduction that a software 
application's primary role is to await 
user actions and then, in response, 
produce some visual change (in the on-screen display) 
which presents information to the user 
(presumably information relevant to their 
original action), along with possible 
side effects (like saving a file).  
On this account, studies of applications' 
behavior would reasonably focus on 
the starting and ending points of these 
action/reaction cycles: how do we articulate 
the full space of possibilities for users 
to initiate actions within the application?  
For mouse-and-keyboard desktop setups, for instance, 
an action like `q.left click` on the mouse 
can be performed at any point in an application's 
window (or windows), and mouse-events could 
click one of several buttons, or turn the 
scroll wheel, and they can be modified by 
pressing certain keys.  In some contexts, likewise, 
users enter data by typing on a keyboard.
`p`

`p.  
Applications have to systematically model 
each of these possibilities so that they 
can create data structures carrying 
all relevant details about user actions, which 
in turn is needed for responding correctly 
(left-click and right-click usually have 
different `i.meanings`/, for example).  
At the other end, applications need to subdivide 
their display window into parcels of screen-space 
so that data updates are located in useful 
visual contexts (continuing the above 
example, the Montreal score should be placed 
near the Habs logo, and likewise for Toronto).  This, 
of course, means that the computer screen is 
not just a `q.picture`/; it is an organized system 
which can be analyzed through data models and 
types.  And such analyses can potentially be 
extended to robots and/or Virtual Reality, which 
are not just `TwoD; screens but possess 
`ThreeD; physical or visual-immersive 
(and maybe synaesthetic) configurations. 
`p`


`p.
Well-designed applications have multiple features and 
are easy or intuitive to use: this means that 
users have a variety of options for 
leveraging applications' capabilities, and 
that it is not difficult for users to 
learn or infer the steps needed to prompt 
their application toward their desired course 
of action.  Applications can generally become 
`q.better` %-- more featureful and intuitive 
%-- by being flexible and subject to continuous 
improvement: applications should be 
designed to acquire more functionality 
over time, and to update their interactive 
interface based on the concrete experience of 
users who may find different features easier 
or harder to access.  For example, specific 
operations might be exposed in different ways, 
in that the user has multiple options for 
initiating certain responses (for example, the 
effects of clicking on a certain button might 
be duplicated by pressing a specific key on 
the keyboard, or by activating the mouse's scroll wheel 
%-- e.g., scrolling down, hitting an on-screen `q.down` 
button, or pressing the down-arrow key).  Many 
applications also allow plugins or extensions to 
adjust the visual displays in ways not envisioned 
by the program's original designers.`footnote.
I adopt the convention that a `q.program` is a 
self-contained software component, something 
that can be executed, and an `q.application` 
is one kind of program: one with a visible 
user interface.  Most programs are applications, 
on these definitions (when discussing technical 
`VM; matters sometimes however we can use 
the word `q.program` in a different sense, as a 
self-contained sequence of `VM; operations).
`footnote`  
This flexibility helps ensure that applications 
can improve and become more user-friendly, but more specifically 
they are engineering goals: there are steps 
which developers can take to maximize 
applications' ability to adapt and evolve.     
`p`


`p.
Aside from a generic desire to build `q.quality` 
software, engineering applications for 
future adaptation also prompts developpers to 
design software in rigorous, well-documented 
ways.  Applications can be thought of, 
rather metaphorically, as virtual/digital `q.piping` 
linking a tableau of potential user actions (via the 
mouse, keyboard, and etc.) to a space of 
possible screen-configurations.  A software 
engineer's goal is to build up this intermediate 
`q.plumbing` in an orderly rather than haphazard 
manner, so that future programmers have a clear 
picture of how the application's functionality 
can be augmented (without affecting its 
current behavior, unless a conscious decision is 
made to alter the application's pragmatics 
based on user feedback).  Users initiate 
actions so as to invoke application features, 
for example: to organize responses systematically, 
it is important to have an efficient structural 
breakdown of all the functionality which 
the application makes available, by analogy 
to how books might be organized on the 
shelves of a library.  But this structuring 
principle also promotes extensibilty: 
once a system is in place to enumerate 
functionality, it is easier to add 
new features because there is greater 
transparency in how new features can 
`q.slot in` alongside their predecessors.  
Similar points apply to the visual 
display of information: the more 
systematically we model how data types 
recognized by an application are translated 
to visual form, the more readily we can build 
new display units %-- such as isolated 
windows or dialog boxes %-- to display 
`i.new` data types, formulated in the 
course of providing new functionality. 
`p`


`p.
Planning for continuous-adaptation therefore 
implies that application should 
deliberatively model the functionality 
they `i.expose` as features available 
to users: improvements can either 
provide alternative (perhaps more 
user-friendly) ways to invoke these 
same features, or to create 
new features that fit comfortably 
with the existing application 
by honoring its protocols and 
user-pragmatic conventions.  
An important dimension of software design, 
then, is formulating the protocols 
and concrete implementations 
through which applications 
`q.expose` functionality. 
`p`



`p.
Consider an example from the domain of 
image-processing (which will be the focus of 
Chapter 21): Computer Vision software 
will typically have many built-in 
functions for transforming 
and/or extractng data from graphics 
images (and also videos).  Exposing 
these capabilities means offering ways 
for users to initiate an image-processing 
workflow.  The result of the workflow 
might be a modified version of the original 
image, or some extracted data structure 
that can be outlined or visualized.  Because 
workflows are often chained together or, 
in general, aggregated to form more complex 
workflows, `q.exposing` functions also means 
allowing operations to be dynamically 
invoked by software, so that although a user 
will manually initiate an overarching 
functionality there may be many 
behind-the-scenes intermediary stages where the 
most users would not directly perceive.  
In this kind of environment applications 
will typically offer an `API; (Application 
Programmable Interface) and/or an 
`ABI; (Application Binary Interface, which 
is more low-level) such that important 
features can be accessed directly through 
user actions `i.and` indirectly 
as intermediate processes launched in response 
to some higher-level user action.`footnote.
I mention `ABI;s here in the sense that 
application-procedures might be exposed 
to be called directly with raw data values, 
in contrast to what I will later call 
`q.meta-procedures` whose inputs and outputs 
are padded with extra layers of indirection.
`footnote`
Moreover, users need some way to view or study 
the results of Computer Vision 
workflows, so 
applications should expose functionality 
for altering their visual display in 
accord with the details of specific 
pipelines (e.g., presenting a new image 
in a new window).  In short, applications
should expose both analytic/processing 
functionality and display/visualization 
functionality.   
`p`



`p.
To `q.expose` functionality is to 
offer multiple ways that functionality 
may be accessed: by users directly; by 
engines which step through workflows; 
by application `q.macros`/, which 
are (for some programs) programmable 
sequences of user-actions that typically 
are performed together (the macro 
can apply multiple actions to be 
initiated with one single action); 
by `q.updates` which can add new 
features without software needing 
to be re-installed; by `q.personalization` 
features which allow programs to enter 
different states depending on the 
identity of their current user; by 
configuration files; and 
by `q.scripting` environments 
where code written in a programming 
language (typically one with 
an interpreter that can be embedded 
in standalone software, such as Lisp or 
Python) can modify or fine-tune application state 
and performance.  Supporting such 
personalization and fine-tuning is itself a 
feature that makes software more user-friendly, 
but (for reasons I mentioned `visavis; 
rigorous design) also prompts developers 
to architect software in well-organized ways, 
with a logical model of user actions, visual 
displays, and all the coding that connects 
the one to the other. 
`p`


`p.
An important dimension of application development, 
accordingly, is that of modeling and formalizing 
how applications expose functionality and 
User Interface details.  Virtual Machines, 
in turn, are a useful tool for 
analyzing the features exposed by applications 
(and how they are implementationally tied to 
the application as a whole).  This is the 
primary scenario where I will consider 
`VM; designs as theoretical artifacts in their own right.
`p`


`subsection.Hypergraphs as General-Purpose Data Models`
`p.
Ultimately, all of a program's functionality 
is achieved via procedures, which are provided 
by the software code base (perhaps along 
with third-party libraries that get re-used, 
sometimes as high-level source code and sometimes 
as compiled resources).  As such, one way to  
`q.expose` functionality is simply to allow 
external code (that is, code which is not 
explicitly developed as part of an 
application's core code base) to call 
procedures which `i.are` in the code base.  
The vast majority of such procedures, however, 
are `q.internal,` intermediate computations 
that would not be meaningful or appropriate for 
external access.  Accordingly, applications 
need a more overarching model of exposed features 
to ensure that external code utilizes 
internal capabilities in an orderly fashion.  
`p`


`p.
In general, functionality is exposed to 
external components by constructing a specific 
procedure whose role is to be an `q.entry point` 
for externally-invoked features.  That 
is, we assume that any exposed `q.functionality` 
requires multiple procedures to be realized, 
and one entry-point is necessary to 
invoke a larger package of procedures which 
collectively implement an operation that 
we can `i.conceptually` regard as `q.one` 
function.  For example, if an exposed 
capability (in a Computer Vision program) 
is running a superpixel segmentation on a 
`TwoD; image via color-watershed, there will 
be many intermediate procedures needed to 
complete that computation, but we can reason 
about the segmentation as one identifiable 
step in an image-processing pipeline.  
It is a `i.conceptual` function unit even if 
it is not `q.one` function, in the sense of 
one single source-code procedure.  I propose 
to use the term `q.meta-procedure` to 
describe functions that are conceptually 
singular but implementationally multiple, in 
this sense.   
`p`


`p.
Application-level capabilities often require 
certain details to be specified %-- continuing 
the superpixel example, such an operation 
would need to know which image to target, and 
potentially would require certain threshold 
parameters (superpixels 
are relatively small image-regions of 
similar color, but `q.how` small and similar 
depends on input values that might vary 
from one run to another).  The `q.output` 
of such a segmentation would be a 
data structure (typically a `q.labeling` 
image, which adds an extra number to each 
pixel %-- alongside its existing 
color channels, such as red, green, and blue %-- 
identifying the superpixel to which it belongs); 
for visualization, sometimes superpixel 
segmentations will also be displayed by outlining 
the superpixels with a colored boundary 
(visually distinct from the underlying image).  
Thus a superpixel algorithm has inputs and 
outputs analogous to a single procedure.  
However, insofar as such an algorithm is not 
one procedure but rather (in my terminology) 
a `q.meta` procedure, these inputs and outputs 
are not delivered simply as binary 
data values (memory addresses or `CPU; registers) 
but rather need to be marshaled and 
encoded between the software component 
which invokes the metaprocedure and 
the application which exposes it.
`p`

`p.  
Unlike the execution-sequence `i.within` 
one component, where one procedure 
can call other procedures directly, 
for such `q.meta` procedures there 
is a series of steps 
wherein input parameters are 
built up via a common protocol
which both components can understand, 
and output results likewise 
encoded in reverse.  The procedure/metaprocedure 
contrast is analogous to the difference 
between a face-to-face conversation between 
two parties and a long-distance negotiation 
where diplomats (or lawyers, etc.) exchange 
offers and proposals and counter-offers 
according to a fixed set of rules.  Unlike 
in-person dialog, such indirect 
communication is less open-ended, and 
relies on trustworthy intermediaries 
to convey messages and information without 
distorting their meaning. 
`p`


`p.
What diplomats, lawyers, and mediators 
may be to human communication, their 
analogues in Information Technology 
would be data-sharing protocols 
and `API;s.  Usually, software components 
which interact via these indirect, 
carefully mediated pathways can potentially 
have divergent implementations %-- they may 
be written in different languages, adhere 
to different coding paradigms, and so 
forth.  Therefore, data has to be transferred 
from one component to another via neutral 
formats which are amenable to both 
sides: data in one context should be 
encapsulated in a neutral package and 
decoded (without distortion) on the 
other environment, and vice-versa.  
External-programming interfaces therefore 
depend on data representation systems 
which allow for information to be restructured 
according to the needs of different 
computing environments, while maintaining 
structural integrity (i.e., decoding the 
data back to its original form should 
produce an exact replica of the data prior 
to its originally being encoded). 
`p`


`p.
Ensuring data integrity across divergent 
programming environments is a complex 
task.  When being passed between 
components data is necessarily `i.serialized`/, 
or convert from its innate binary/in-memory 
form to a textual or numeric encoding, and 
then reconstituted into a new binary 
package which should be `q.equivalent to` 
the original (we can define `q.equivalence` here 
in terms of bi-directionality: re-encoding the data 
and sending back from target to source, then 
re-decoding, should yield a copy of the original; or, 
more rigorously, modifying a smaller part 
of the received data and then back-sending 
should yield a duplicate of the original 
`i.except for` the specific change).
`p`

`p.
Data-serialization formats should be designed to ensure 
that information is not lost in the encoding/decoding 
process.  For example, a quantity which 
originally has floating-point type (so it `i.could` 
have non-integer values) might get encoded as an integer 
if, in fact, on a specific occasion it has no 
fractional part; then, on decoding, the 
value might be interpreted `i.as` an integer 
(rather than a `b.float` which happens to 
have an integer value), which in turn could 
corrupt future calculations (in some contexts 
arithmetic operations performed on an integer 
paired with a `b.float` will cause the 
floating-point value to be truncated to an 
integer, yielding incorrect results in 
an algorithm which expects both 
values to be `b.float`/s) and/or 
corrupt data sent back to its source.  
Similar incompatibilities may exist 
between expected units of measurement 
(metric versus Imperial, for instance) 
or value-ranges (it is not obvious from a
single number what are the sensible range 
of values which the number could take on, if 
modified: magnitudes for individual pixel-colors, 
for instance, in most image-formats are 
restricted to the range 0-255; percentages 
are often limited to 0-100; angles 
in degree to 0-359, and so forth).  
Failure to anticipate the full set 
of metadata conventions and standards 
which are prerequisite for multiple 
software components to act on shared 
data is a common source of data-corruption.  
`p`


`p.
For these reasons, data-sharing protocols 
have to be designed around detailed and 
`q.expressive` representations, meaning 
that it is possible to systematically 
describe all facets of data profiles 
(types, ranges, units, scales, metadata) 
that could potentially be sources of 
ambiguity and encoding/decoding errors.  
To be sure, a lot of data-sharing happens 
through relatively simple formats 
(such as `JSON; %-- JavaScript Object Notation 
%-- or `CSV;, comma-separated values) 
which lack these higher-level guidelines, 
but such formats typically work in 
contexts where networking conventions 
are clearly defined %-- effectively 
relying on programmers' discipline and 
informal documentation to take the place 
of stipulations formally encoded in data 
protocols.  In more `q.critical` contexts 
which should be less susceptible to programming errors 
%-- or for more open-ended protocols 
where networking between disparate end-points 
should not depend on programmers  
honoring informal specifications %-- developers 
would tend to prefer more rigorous 
representations, such as `XML; (with 
validators and predefined tag and attribute sets) 
or `RDF; (Resource Description Framework), 
associated with the Semantic Web (in conjunction 
with explicit Ontologies).    
`p`


`p.
In this chapter I will discuss options for `i.hypergraph` 
representations (which in some sense generalize 
`RDF;, itself considered to be `q.graph-oriented`/, but, 
unlike hypergraphs, not internally multi-scale).  
I claim that hypergraphs are more effective 
at permitting rigorous data profiles to 
be described and confirmed (i.e., we can 
formulate hypergraph data paradigms that are 
more expressive still than, for instance, 
`XML; and `RDF;) while retaining the necessary 
attributes of formal verifiability and 
transparency.  Indeed, hypegraph models 
in various forms have been proposed as 
extensions to both `RDF; (for example, in 
the transition from graph database engines 
to hypergraph engines such as HyperGraphDB, 
Grakn, or AtomSpace) and `XML; 
(in the context of `q.concurrent` markup 
and related supersets of `XML; trees, 
which tend to focus on allowing tags 
to overlap one another instead of 
being strictly nested, as they are 
in `XML; and its variants, notably `HTML;).   
`p`

`p.
Although data (meta-) models are an important 
theoretical topic (particularly when 
the discussion turns to optimization, so that 
we are considering not only `i.whether` 
queries or validations can be completed, but 
how to do so efficiently), here I am 
equally concerned with practical software concerns: 
in particular, the coupling between `i.representing` 
information and `i.exposing` application 
functionality.  The most common reason we seek 
to (digitally) encode data is to send it between 
separate components (including ones designed 
with little or no explicit inter-connections, 
except for joint participation in data-sharing 
capabilities which have to be implemented 
`i.ex post facto`/).  Metamodels are most 
valuable when they permit inter-operative 
capabilities in an additive manner: applications 
should be able to expand the scope of 
other applications with which they might network.  
Representational systems that engender 
computational artifacts (parsers, for syntax, and 
validators, for semantics, let's say) which can be 
implemented, embedded, and leveraged by applications 
%-- as their capabilities are refined to support new 
protocols %-- are most effective when the amount of 
effort consumed by the requisite programming 
is minimized.  Data-sharing protocols should thus be 
engineered to pass through application-integration 
phases as quickly as possible, but likewise 
applications should be architected to accommodate 
new data-sharing initiatives without 
substantial re-coding.  
`p`


`p.
Generically, we can describe a metamodel as 
relatively `q.expressive` to the degree that 
data structures and their concomitant semantic 
paradigms can be transparently encoded 
according to the modeling system's rules.  
Expressivity might imply a level of 
redundancy, or at least superficial 
redundancy when viewed from the 
sole perspective of data encoding.  
For example, so-called `q.Industry 
Foundation Classes` %-- widely used 
in `AEC; (Architecture, Engineering, 
and Construction) technology and 
`BIM; (Building Information 
Management) %-- employ two different 
sorts of data annotations (called 
`q.attributes` and `q.properties`/) which 
both are roughly analogous to `XML; 
`q.attributes`/.  The `IFC; attribute/property 
distinction might seem locally superfluous 
in that asserting a given data-point via a 
property rather than an attribute 
(or vice-versa) does not appear to 
convey greater information %-- the 
information is borne by the 
field's value, not by its property-or-attribute 
classification.  However, this distinction  
is not semantically vacuous; 
its significance emerges in the larger 
scale of schema-standardization and validation.  
Data fields asserting properties 
of objects (in the sense of real-world 
physical materials incorporated into building designs) 
are classified as either `i.attributes` or `i.properties`/, 
with the distinction being that attributes are fixed within 
schemata for objects of any given kind, whereas property-sets 
can be introduced in objects' context more flexibly.  
In type-theoretic terms, attributes are immutable parameters 
in types' schema, such that objects of the same type have 
the same attributes, whereas properties are not bound to 
types with equal force (usage patterns might dictate, 
for example, that common type-instances share property-sets 
`i.in some context`/, but these are not essential maxims 
of the type itself).
`p`

`p.
In the realm of bioinformatics, 
a similar distinction is made in the 
context of `DICOM; (Digital Imaging and Commmunications 
in Medicine) wherein `q.tags` (which have descriptive labels 
but also two-part numeric codes) are encoded using a system 
that distinguishes `q.Standard Data Elements` from 
`q.Private Data Elements`/: the latter are 
assigned a numeric pair whose `q.Group` 
(first) Number is odd.  `lPACS; (Picture Archiving System) viewers 
(software that recognizes the `DICOM; format) will ignore 
Private Data Elements by default, so the Standad/Private 
distinction is intrinsic to protocols through which 
`DICOM; data is processed.  This distinction is not 
functionally identical to `IFC; attributes/properties, 
but reveals similar motivations insofar as data specifications 
are also guidelines for implementing software which 
manages data structures conformant to a given standard.  
Standardization projects' tasks include defining requirements 
on software as well as data representation, and it is 
logistically significant to distinguish standardized 
parameters that software `i.should` recognize as a 
compliance-criterion from non-standard kinds of values 
that particular users or groups of users working 
within a given protocol may want to introduce internally.   
`p`


`p.
The `BIM; attribute/property and `DICOM; Private/Standard 
distinctions are suggestive illustrations of challenges 
encountered when applying generic data-modeling principles 
to specific domains.  Most general-purpose representation 
frameworks (consider `XML; and `RDF;, for instance) lack a 
mechanism to directly express disjunction in parameters' 
level of standardization, as concretely found in these 
examples (not to imply that such details could 
not be represented indirectly, e.g. via meta-attributes 
or `DTD; stipulations, but the point is that 
one is thereby leveraging the affordances of a 
given representation scheme to accommodate semantic 
distinctions not specifically anticipated by that 
scheme, rather than organically encoding the 
semantics to begin with).  Also, note that %-- for 
fields indexed by character-strings %-- descriptive labels 
are designed to be meaningful for human users/readers, but 
in most modeling approaches labels do not have an 
`q.internal structure` recognized by the framework 
(at least if we consider, say, `XML; namespaces as separate 
labels from element names).  The `DICOM; pattern wherein 
two-part numeric codes are employed for something 
akin to namespace/element separation, and then the 
use of an odd/even to signal Private/Standard tag rules, 
is also an idiosyncratic formulation which does 
not have a natural correlate in multi-domain modeling protocols.   
`p`


`p.
Data-representation theories which are grounded 
solely in `q.logical` reasoning, or mathematical 
representation, can miss such real-world 
complications.  If we start from symbolic 
logic (or from a mathematical picture of 
graphs or trees as formal systems) we might 
be inclined to recognize `i.properties` 
(essentially metadata on graph sites, which 
is how properties work in conventional 
Property Graph database engines), or `i.attributes` 
in the `XML; sense (metadata on tree-nodes), 
but our theoretical framework could neglect to 
consider the possibility that a data-sharing 
protocol might need two `i.different` 
property/attribute mechanisms.  The semantics 
of a property/attribute distinction (in contexts 
such as `IFC;, or `DICOM; public/private) derives 
not from logical models but from 
real-world implementational concerns.    
A metamodel which supports this larger semantic 
context is therefore sufficiently expressive 
to properly encode `IFC; or `DICOM; data; a less expressive model 
(one which collapses attributes and properties 
into a generic notion of `q.fields`/, say), would 
fall short, at least without compensating 
by leveraging its own formal resources 
(e.g., classifying fields as attributes or 
properties via `q.meta-data` fields).  
Here we can appeal again to application-level 
concerns: the paucity of less-expressive 
systems would become evident insofar 
as reifications, indirections, and other ad-hoc 
solutions to representational blind-spots 
can render application-integration more complex 
and time-consuming.      
`p`


`p.
Metamodels are more expressive to the degree that 
they offer a greater range of representational parameters 
with which structural and semantic conventions 
might be communicated.  For example, `JSON; 
can be deemed more expressive than primitive `CSV;-style records because 
`JSON; distinguishes arrays (which are structurally akin 
to lists) from `q.objects` (i.e., associative arrays, 
lists indexed by field-names rather than numbers).  
Likewise, `XML; is more expressive than 
`JSON; insofar as elements' data can be asserted 
either via attributes or via nested 
elements (for sake of argument, treating `XML; 
as a `i.de facto` superset of `JSON;, albeit with 
different syntax, wherein arrays 
correspond to sequences of similarly-tagged child 
nodes).  Associative arrays in `XML; might 
be coded `i.either` as attribute key-value 
pairs on `i.one` node `i.or` as sibling nodes 
with unique tag-names.  This `q.redundancy` 
allows for conventions to cohere 
(in a given data-sharing protocol) stipulating 
when one or another of these alternatives should be 
adopted, thereby supplying an extra layer of 
semantic detail which is not present in `JSON;.  
`p`


`p.
In the case of property graphs and/or hypergraphs, 
data fields can be associated with an `q.object` 
(in the sense of an integral data structure) 
via properties (attributes on a node) or via 
node-to-hypernode relations (sometimes called 
`q.projections`/), a duality reminiscent 
of property/attribute in `IFC;.  Graphs, 
of course, have the further stipulation 
that any two nodes (or hypernodes) may be 
linked by edges (themselves equipped 
with labels, label-namespaces, controlled 
vocabularies, and potentially constraints/axioms 
enforced by `q.Ontologies`/).  Formats such as 
`XML; and `JSON; which are more `q.syntactically` 
oriented tend to be hierarchical, in that 
any specific value in a `JSON; object or array may 
itself be another object/array (rather than atomic 
value) and likewise `XML; elements may have other 
elements as children.  By contrast, formats 
such as `RDF; and property graphs which are 
more `q.semantically` focused tend to be 
graph-like and encode semantic relations via 
edges across nodes (rather than hierarchical nesting).  
Hypergraphs potentially combine both styles of 
representation, with hypernodes containing 
nodes hierarchically `i.and also` edges 
between two (or two-or-more) nodes and/or hypernodes 
(the precise rules as to which constructions are 
possible will vary from one system to another, but 
these are reasonable approximations). 
`p`


`p.
Any representational system, as these examples point out, 
provides a range of parameters that 
could be pressed into service when formalizing a protocol 
for encoding specific kinds of data via structures 
conformant to the specific system.  More expressive 
systems have a wider arsenal of parameters; for 
instance, along the lines of the above gloss, property 
hypergraphs (models which either add hyperedges to property graphs or, 
equivalently, add properties to hypergraphs) 
are more expressive than either property 
graphs or document-style hierarchical trees 
alone, because properties-on-nodes (as in `XML; attributes), 
nested hierarchies (hypernodes containing child nodes akin 
to child `XML; elements), and inter-node connections 
(as in Semantic Web labeled edges) are all potential 
representational devices in the property-hypergraph context.  
One conclusion to be made here is that property-hypergraphs 
form a flexible general-purpose 
metamodel, but the relevant point for the 
moment is that expressivity can be `q.measured` via 
the range of distinct representational parameters afforded by 
the system, at least intuitively.
`p`


`p.
This intuition might be made more precise, insofar as 
I have deferred any rigorous definition for 
`q.representational parameters`/.  That is to say, for a 
reasonably systematic analysis of metamodels we should 
specify building-blocks of constructions 
recognized through any metamodel.  The overall 
concept may be clear enough %-- in general, 
the parameters of a modeling system are the full set 
of structural elements that may potentially 
be employed in fully describing any given 
structure covered by the system %-- but one would 
like a still tighter definition.  For this chapter, 
I approach this problem from the 
perspective of Virtual Machines.
`p`


`subsection.Virtual Machines in the Context of Data Metamodels 
and Database Engineering` 
`p.
The correlation between Virtual Machines and 
representational paradigms should be clear: suppose 
we take any structure instantiating a particular 
metamodel.  Presumably, such a structure 
can be assembled over multiple stages.  Insofar 
as node-hypernode inclusion is a constructional parameter, 
for instance, then a structure can be modified 
by including a node within the scope of a hypernode.  
Similarly, insofar as inter-node relations (via directed 
edges or hyperedges) are significant constructions, 
then a structure may be modified by adding an edge 
between existing nodes.  In short, any structure 
can be derived from the modification of precursor 
structures.  The full set of structure-modifying 
operations available for a given representational 
paradigm could then be enumerated as (at least on part of) 
the opset of a hypothetical (or realized) 
Virtual Machine.  As such, Virtual Machines 
provide a potential formalizing environment 
for analyzing data metamodels.  
`p`

`p.
Conversely, data-models can serve as a prompt for 
motivating the proper scope of a Virtual Machine.  
That is, `VM;s 
may be designed subject to requirements that 
they permit the accumulation of any data structure 
conformant to a given metamodel.  Similarly, `VM;s 
might be characterized in terms of how they support 
various `q.calling conventions`/, in the sense of 
protocols through which computational procedures 
delegate to other procedures (supplying inputs, 
reading outputs, spawning concurrent 
execution paths, and so forth).  This section's chapters
will focus on `VM;s based on hypergraph 
data models, employing such structures 
both from the perspective of data-representations 
and interlocking procedures (i.e., describing 
procedures in terms of sequences of 
calls to other procedures).   
`p`

`p.
Virtual Machines are useful tools for studying software-interoperability 
insofar as both data-representations and communications protocols 
could be modeled in terms of `VM;s (at least as abstract summaries 
of working code; or, more ambitiously, application-networking 
frameworks can be provide actual `VM;s through which applications 
can route their networking logic, analogous to query languages 
as host-language-agnostic conduits for database access).  
As emphasized earlier, applications typically 
seek to `q.expose` functionality to external 
components.  In order to do so rigorously, 
it is necessary to stipulate preconditions 
on how functionality being externally invoked 
should be designated %-- for instance, if a Computer 
Vision application has multiple algorithms available 
for many processing tasks, external code needs to 
specify which implementation is being requested %-- 
and how input parameters (and then output results) 
should be encoded.  Applications can use 
`VM;s as a kind of neutral environment where 
third-party code build up representations 
of exposed-functionality invocations 
incrementally.  Indeed, many programs support 
scripting via languages such as Python; it 
is plausible to generalize this idea to 
`VM; platforms amenable to multiple scripting 
languages, so long as they can be compiled 
to the relevant `q.byte` code.  Alternatively, 
even if applications do not expose `VM;s 
to third parties directly, they could 
use `VM;-backed processes to process 
`API; requests: since `VM;-targeted code may be 
updated without recompiling the application, 
the `API; could then adapt to new networking 
situations.        
`p`


`p.
In this context `VM; models overlap with constructions pertaining to 
interoperability between applications and database engines.  
Consider the general setup of database systems: applications 
store data structures for future reuse by passing some 
(suitably encoded) serialization of the relevant information 
to a database, which arranges the data into a layout 
optimized for storage and retrieval/querying.  Typically the 
database will attempt not merely to preserve the presented 
data structure for future reconstruction, but will 
index or destructure it in such a manner that such specific 
data can be selected as matching a future query.     
`p`


`p.
At any moment in time, an application will be working with 
one or more data structure that might be called `q.live`/; they 
are directly implicated in the state of the application at 
that moment.  The current user (or a different user) might, 
accordingly, be interested in reconstructing that 
application-state (or some relevant subset thereof) at a 
future point in time %-- at which point database queries are 
typically necessary, because the relevant information is 
no longer in live memory.  To be sure, a database does 
not necessarily store `q.application state` as such 
%-- although developers can potentially 
create `q.application state objects` and persist 
those so that users can resume prior sessions %-- but 
a typical rationale for persistent data storage 
in the first place is functionality supporting 
users' desire to resume prior work, return to 
previously-viewed files, and so forth.   
`p`


`p.
As a concrete example, suppose we are considering an application 
which (at least as one of its features, and in the context 
of particular `GUI; windows or components accessed through 
the software) supports photo tagging and annotation.  
Imagine an image database allocated for documenting 
ecological and/or infrastructure damage 
from natural or man-made events, such as 
the 2022 Russian invasion of Ukraine, and perhaps 
setting the stage for reconstruction plans.  
A typical image for such an application might be a 
photograph of damaged building or facilities, 
or plots 
of land on which real estate will be rebuilt.  
Potentially,  
images would depict remnants of prior buildings that 
were destroyed, and/or could be annotated with 
data relevant to reconstruction designs 
(e.g., in recreating damaged neighborhoods 
a certain number of residential units might 
be targeted for each parcel of land or each block 
subject to redevelopment, perhaps optimized by 
models measuring the ideal urban density for 
that specific neighborhood based on environmental 
criteria, utility grids, public transit, and so forth).  
A typical session for such an software component 
could involve users viewing individual images, 
previewing image-series depicted via thumbnails, 
searching for images (based on criteria such as 
street address, city/district name, or perhaps 
`GIS; coordinates), switching between `TwoD; image 
and 3D street views, and %-- once in the context 
of a specific picture %-- viewing annotations 
and other associated data in tabular or 
otherwise structured forms (e.g., tables or key-value 
pairs displayed through independent `GUI; windows 
floating above the graphics viewport).  
`p`


`p.
Assuming such an application works with relatively 
large image-collections (enough to be impractical 
for users simply to browse images in a filesystem 
folder, say), functionality for finding and tracking 
images would need to be based on some 
systematical query capabilities, i.e., some 
form of image database: image file paths, feature sets 
for retrieval/similarity searches, metadata 
and formatting details, etc., can be hosted 
in a database so that images can be selected 
inside large series by variegated query strategies.`footnote.
Images themselves (i.e., their pixel-data) might also 
stored as `q.blobs` %-- binary large objects %-- but 
the term `q.image database` generically covers cases 
as well where images files are hosted on a filesystem 
with only their metadata stored in the database directly.
`footnote`
`p`

`p.
Suppose an image depicts a city block where 
damaged buildings have to be replaced.  
Data associated with that image could include 
estimates of the number of people who lived in 
that location prior to (for example) 
the Russia/Ukraine war; the number of 
residential units slated for redevelopment; 
cost estimates; links to public transit info, 
street views, data concerning utilities grid, 
etc.  Plausibly, such data would be held 
in a database and loaded alongside the image, 
or in response to user actions signaling an 
interest in the relevant data-points.  The 
database, in effect, is a means to an ends, 
whereas from a user's perspective the 
important detail is that the application 
can enter a state where multiple important 
data-points are visible side-by-side: users 
might view a photograph in one window juxtaposed 
with a window or windows showing civic/residential 
data.     
`p`


`p.
Continuing this specific (hypothetical) example, the 
envisaged scenario has image-viewport components 
serving as entry-points for a diversity of 
civil/architectural information; presumably 
the specific kinds of data available will vary 
from one image to another, and users will 
signal through interactive `GUI; features 
their interest in accessing certain branches of 
the available data over others.  That is, assume 
there is not a fixed metadata/associated information 
package that automatically accompanies each image, 
but instead that data and images are linked on a 
dynamically changing case-by-case basis.  That setup 
would call for a coding strategy which works to 
organize the available information and 
user-interaction pragmatics coherently.  
The resulting software-design choices would, 
moreover, propagate to `GUI; and database designs 
as well.  Once some aggregate of data is identified 
as a coherent unit, this structural decision 
must be accounted for at the `GUI; level 
(insofar as users request `i.that specific` 
data from application-states where they are 
viewing an image carrying the appropriate 
information) and the database-integration level 
(`q.that specific` data has to be 
queried from the larger database context when needed).  
In other words, the interaction between `GUI;, database, and 
application logic is more complex than if each image were 
given a fixed set of data fields isolated from user pragmatics.
`p`

`subsection.Database Engineering and Type Theory`
`p.
Suppose again that one information-bundle that 
could be associated with (some) photographs outlines 
redevelopment plans: data points such as the number 
of residential units targeted, renderings of building 
designs, contractors, contact information, and so forth.  
Presumably, such data would only be applicable 
to images showing blocks or plots where such plans 
are in the works; and moreover such images would link 
to other forms of data as well (about, say, utilities 
grids).  As part of the empirical background, in 
effect, one might conclude that various facts 
pertaining to individual reconstruction projects/contracts 
can both be aggregated (as interconnected data-points) 
and isolated from other information potentially 
relevant to an viewed image.  In general, software 
design depends on sensitivity to how information spaces, 
practically speaking, can be carved and organized.  
The decision to isolate (say) construction-project 
data as integral units would be made against 
that kind of design/decision context.  Having 
this topic reified as a specific `q.category` of data, 
say, affects `GUI; programming and event-handling 
(because user-visible components must be implemented 
enabling users to access information in that 
category, which in turn yields signals like 
context-menu activations and the need for appropriate  
event-handlers) as well as database interop (insofar 
as such data has to be packaged in persistable forms). 
Subsequently, representations of such integral construction-project 
data (in the form of, e.g., a cluster of interrelated 
datatypes) would be manifest as software artifacts 
threaded through a code base.
`p`


`p.
The specific patterns of data organization and programming-language 
types engineered for an application reflect practical 
concerns and aspirations to optimize User Experience; these patterns 
do not necessarily map neatly to native database 
architectures.  Neither relational databases (built up from 
tables with fixed tuples of single-value columns) nor 
conventional graph databases (whose representations are confined to 
one layer of labeled edges) generically match the 
multivariate and multi-level structure of real-world information 
typically managed at the application level.  This is why 
application-level data types generally need 
to be restructured and transformed when routed between applications 
and database back-ends.  
`p`


`p.
Software engineers are responsible for ensuring a proper 
synchronicity between application and database state, although 
(as intimated above) this does not (by and large) entail 
application-state being directly persisted in a back-end.  
Instead, back-end updates are a property of application-state at certain 
moments; insofar as users have performed edits or in general 
made changes resulting a mismatch between the data as 
currently seen by the user and what is stored in the database, 
the latter has to commit such changes for perpetuity. 
Update-worthy application-state then needs to be 
distributed over (potentially) multiple database 
`q.sites` which are collectively implicated in an update.     
`p`


`p.
For sake of argument, consider an update (or part thereof) consolidated 
into a single (application-level) datatype instance; e.g., 
one object of a given `Cpp; class.  Quite possibly, there is 
no one-to-one correspondence between application objects and 
database values or records, particularly if the classes 
in questions contain many data fields and/or multiple 
one-to-many relationships and values which are more 
involved than simple strings or numbers (e.g., pointers 
to other objects).  The structural mismatch between 
application data and database records is evident in 
technologies such as Object-Relation Mapping (`ORM;) 
%-- or `q.Object Triple Mapping` for the Semantic Web %-- 
marshaling data for relational databases or triplestores, 
respectively.  Engines with more flexible 
representation paradigms, needing less reconstruction 
of application data exported to a back-end, can be 
advantageous precisely because data-persistence 
capabilities end up consuming less `q.boilerplate` code.  
Hypergraph databases are a case-in-point: the kind of 
complexity in datatypes' internal orgnanization 
(multiple multivariate fields for a single object, 
for instance) which give rise to `ORM;/`OTM;-style 
transforms map organically to hypernode or hyperedge 
constructions. 
`p`


`p.
Modifying a database entails conveying a package 
of database between applications and the database 
engine; the structure of this `communique; 
in turn reflecting database architecture.  
The `SQL; `INSERT; 
statement, for example, derives its form 
from the layout of table-base data models:  
adding a record entails naming the table to which it 
will belong, which brings on board the specific 
list of fields (columns) whose values have 
to be accounted for in the query.  Our impression that 
relational algebra is a relatively crude or inflexible 
meta-model derives, it would seem, in large part 
from the quantity of bridge code needed to implement 
database updates through query commands such as 
`INSERT; that are restricted to the relational 
architecture.  To the extent that hypergraph 
engines (for example) are more flexible in principle, 
this advantage only becomes concretely 
evident to the degree that hypergraph databases 
support a query system such that updates (and 
analogous modifications to a database instance) 
are initiated with relatively less 
boilerplate code.    
`p`


`p.
As I alluded to earlier, hypergraph data models 
have comparatively greater parameters available 
for representing information; in particular, 
this implies that we have greater 
flexibility in formulating queries 
to modify database instances.  It is worth mentioning 
at this point that `q.queries` need not involve 
instructions passed to a database engine 
in the form of character strings (e.g., 
`SQL; code); indeed, forcing applications 
to build query code on the fly is a often 
an antipattern (spurring boilerplate code-bloat); 
better solutions involve query `q.factories` that 
assemble queries via procedure/method calls 
in a host programming language (perhaps 
through an embedded domain-specific language, 
as one finds with `LINQ; `visavis; `CSharp; and 
its emulations in other languages).  On the 
other hand, in the best case scenario a database 
would `i.also` support a query language that 
could be executed as text strings outside of an 
application context (and without relying on a 
host programming language), for examining 
the contents of a database in situations removed 
from application-based access (debugging, analytics, 
general admin functionality, and so forth).   
In other words, ideally engines will encompass 
a query engine that works with query-structured 
compiled `i.either` from application-code factories 
`i.or` standalone query code, which is one rationale 
for embracing a query-evaluation Virtual Machine.    
`p`


`p.
The specifics of database updates %-- sticking again for 
sake of exposition to single type-instances %-- depends 
of course on types' internal organization.  
For typically atomic types 
(like 1, 2, 4, or 8-byte integers) updates might 
only entail replacing one value with another, but 
more complex values (`q.objects`/, in effect`footnote.Taking 
the perspective that in some systems objects are formally 
defined as aggregate structures (rather than atomic data-points) 
and, even if not precisely stipulated, object/atomic value 
distinctions tend to align `i.de facto` with object-classes 
against, say, built-in types (cf. `Cpp;)`/) with multiple 
and/or multi-value data-fields updates can include 
adding or removing a value from a collections type-instance 
as well as altering such a value, and so on.   
For many compound types the collection of fields 
includes more than one which are in turn `q.collections` 
(e.g., vectors, stacks, queues, deques, and unordered sets/multi-sets, 
plus map-arrays that are pair-lists with possible nonduplication 
restrictions on the first element) subject to add/remove 
operations, in contrast to full-on value-to-value 
replacement.  Some collections have modification-restrictions 
(e.g., values can only be added or removed from one or 
both ends of a list, or, as in typical `q.sets`/, all added 
values must be unique) or enforce constraints such as 
monotone increase and decrease.`footnote.
Consider a collections type 
which has only one insertion operation, 
but will automatically place new values either at the beginning or 
end of the list to preserve increase-direction; here, 
new values have to be either greater or less than all 
prior values.  Or, automatically sorted lists need only 
one insertion operation because the insert procedure would 
deduce the proper insertion-point.
`footnote`  All of these are potential paths toward 
legal mappings of type-instances between states 
which initialized typed value can take on, given their 
internal organization.  Nor is this discussion complete; 
we could also mention reclassifying `q.union`/-type values 
(whose instances can be one of several types) or 
various types depending on binary arithmetic 
(cf. tagged/`q.decorated` pointers, or 
enumerations whose value can be members of 
nominal-value lists `i.or` bitwise combinations 
thereof, or unions where multiple type-tags 
are valid by virtue of shared binary encoding, 
in effect using one tagged value to update another 
member of the union %-- a simple case being 
integer/bitset unions where setting the 
integer automatically sets or clears corresponding 
fields in the bitset).  
`p`


`p.
The mechanisms for aggregating multiple values 
into individual type-instances tend to be 
much more complex for general-purpose programming 
languages such as `Cpp; (see unions, pointers, arrays, 
multiple inheritance, enumerations and their 
base types, etc.) compared to databases 
(see `SQL; or `RDF; types); this is a proximate 
cause of complications in persisting application-level 
data.  Conversely, databases with more refined 
type systems can (at least potentially) absorb 
application data more conveniently.  For this 
to work in practice, however, the 
engine needs a query system which can 
duly leverage type-system expressivity; the issues 
involved here are well-demonstrated by 
update-queries as I've mentioned.  Properly recognizing 
application-level intra-type organization 
depends on representing (and then 
exposing to a query interface) the full spectrum 
of morphisms through which a single type-instance 
might be updated.  
`p`


`p.
For complex types, an effective query-system would have 
update operations that are more targeted than 
just replacing one value with another 
`i.tout court`/; instead, updates may only 
involve one or some subset of all data-fields, 
and (for multi-value fields) could involve 
insertions/deletes in a collections context 
rather than a direct value-change.  The degree to 
which a database engine seamlessly interoperates 
with applications depends on the latter 
constructing data-packages (without undo effort) 
that signal the `i.kinds` of updates 
requested alongside the relevant new values 
(notating, e.g., collections-context changes as 
distinct from single-value morphisms).  
Intuitively, flexible architectures 
(e.g., hypergraphs) accelerate the requisite 
implementations, although actually 
supporting a query-interface satisfying 
such ambitions depends on a confluence 
of factors (e.g., underlying database 
architecture but also query-representation 
and query-evaluation protocols and the 
tools to compile code/text or procedurally-generated 
queries to internally-represented structures 
suitable for evaluation).
`p`


`p.
This section's discussion has focused on 
updating a single type-instance centering on 
the point that complex intra-type layout implies 
a diverse set of query-based update options.  
An `UPDATE; statement in `SQL;, say, 
only (directly) supports one 
particular `q.genre` of updates (value-to-value 
edits in one or more discrete columns).  Given application 
state which can be expressed in terms of modifications 
to existing persisted values, keeping the 
back-end in sync entails accounting for the changes 
embodies in the new application-state by 
presenting the back-end engine with (in general) a 
series of updates; the more flexibly updates 
can be encoded, the less development time 
need be expended figuring out how to 
translate application-state changes to updates 
the engine can process.  This is one reason why a 
diversity of data-modeling parameters can 
streamline application integration: a flexibly 
tableau of recognized constructions implies that 
applications can describe updates with 
relatively less boilerplate destructuring.  
Consider the multitude of `q.sites` where 
data associated with a hypernode can 
be asserted, in the context of property-hypergraphs; at least 
(and some systems may have more elaborate 
constructions as well) properties `i.on` a hypernode, 
nodes `i.in` the hypernode, and edges `i.from` a 
hypernode to its peers.  This articulation 
of site-varieties is, self-evidently, also a 
list of update-forms.  Having multiple protocols 
for describing updates allows different forms of 
updates to be recognized with distinct semantics.  
For example %-- consider again the attribute/property 
distinction in `IFC; %-- updates to `i.properties` 
(which would in general be ad-hoc annotations 
on a hypernode less strictly regulated than 
nodes encompassed `i.in` hypernodes) could be 
subject to different validators than updates 
performed via node-insertion or (potentially 
Ontology-constrained) edge-insertion.      
`p`


`p.
In effect, multi-parametric modeling 
tableaus in the database `i.architecture` 
propagate to multi-dimensional 
options for encoding updates, which in 
turn allows for coexisting update 
protocols each with their own semantics.  
Applications can then choose which 
protocol most efficiently describes 
any particular change in application-state.  
`p`


`p.
Of course, these points `visavis; changing 
`i.existing` database value have analogs 
in the context of inserting `i.new` values.  
Ideally, applications will have flexibility 
in how encode data structure for insertion 
into a back-end via database queries.  
This is not only a matter of notating 
all information which should be persisted, 
but also providing cues to the engine 
about how the new data should interact 
with other records (e.g., using 
primary keys or globally-unique 
identifiers to secure inter-record 
links analogous to %-- perhaps 
translating %-- live-memory pointers) 
and subsequent find/select queries.  
What are the criteria through which 
a database record, once deposited, should 
be retrieved again in the future?  Most 
database systems will give nodes/records 
unique id's, but the whole point of 
search queries may be that records 
should be located based on known data 
in contexts where an application does 
`i.not` have the requisite `gid;. 
`p`


`p.
Consider again the case-study of image-curation 
software that could be used in a redevelopment/urban 
planning context, such that photograph resources 
include depictions of future building sites.  
As suggested earlier, data associated with 
each picture could reference construction-project 
data (e.g., the number of units slated for 
construction, or the identity of the firm contracted 
to oversee the project), as well as, perhaps, 
`q.generic` information (image format, dimensions, 
color-depth, plus, say, `GIS; coordinates).  
One consideration when designing such an application 
would be how users would find images 
that they hadn't seen before, or had not 
revisited for an extended period of time 
(so that the presumptive image `gid; is not 
cached in recent history).  In would make sense 
to track images by longitude and latitude, 
for example, assuming that users would know such 
data.  Perhaps street address (accounting for 
the possibility that large-scale redevelopment 
might alter the street grid so that pre-war 
addresses, say, become obsolete; one might 
still maintain a mapping from such addresses 
to `GIS; coordinates so they remain useful 
for queries); or (less granularly) by district 
or city.  We can similarly envisage scenarios 
where architectural details are mentioned 
(`q.find images of sites within a 
10-kilometer radius featuring planned 
buildings over 4 stories tall`/).   
`p`


`p.
For queries along these lines to work, 
back-ends need to structure type-instances 
such that (potentially large collections of) 
values can be filtered into subsets 
meeting specific criteria: `GIS; coordinates 
restricted to a given locale, housing 
matching a given contractor-name, and so forth.  
When exporting info to a back-end, the 
relevant details are not only the specific values 
comprised by the new data but also 
which fields may serve as eventual query-parameters, 
and how they should be indexed.  
Such `q.selectability` criteria have to be 
encoded alongside persisted data structures 
themselves, and convenient application-integration 
entails supporting protocols for 
noting pathways for query-retrieval 
and doing so with minimal boilerplate code 
(for analogous reasons as with update queries).  
`p`


`p.
Of course, selection/retrieval and update protocols 
tend to be defined on types rather than the type-instance 
level.  To the degree that certain data-fields are 
indexed and queryable (for retrieving instances 
when their global id's are not at hand `i.a priori`/) 
decisions as to which fields to thereby expose 
tend to be made for each type %-- as part of the 
type's design and contract %-- rather than 
negotiated on a case-by-case basis per 
instance (though note that properties in property-graphs 
are possible exceptions; indeed this is one 
rationale for property-graphs in the first place).  
Similarly, type-level modeling tends to 
define which update protocols to invoke for 
different state-changes.  Accordingly, an expressive 
query interface should allow stipulations regarding 
updates and searches to be described as attributes 
of `i.types` as well as single instances (e.g., 
records and/or hypernodes) and to be deferred 
from instances to type-contracts (analogously, 
inferred in instance-contexts by virtue of type 
attributions).  An obvious corollary here 
is that query systems have to recognize type 
descriptions as well as encodings of 
type-instances.`footnote.
By implication, query languages would then 
require, as a subset,  
`q.type-expression` languages where types' 
attributes, internal organization, and 
back-end protocols are duly notated (so that 
type-information can be `q.loaded into` the 
system and consulted as the engine resolves 
how to accommodate insertions, updates, and 
filtering for specific instances).  
`footnote`  
`p`



`p.

`p`




